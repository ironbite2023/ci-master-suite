# Foundation Belt Case Studies

## Case Study 1: Transforming Culture at MidState Manufacturing

**Course:** Course 1 - CI Foundations & Culture  
**Length:** 1,150 words  
**Difficulty:** Beginner  
**Estimated Analysis Time:** 45-60 minutes

### Background

MidState Manufacturing is a 250-employee automotive parts supplier based in Ohio. Founded in 1985, the company has been profitable but faces increasing pressure from lower-cost overseas competitors. The CEO, Jennifer Martinez, attended a Lean Six Sigma conference and returned excited about implementing continuous improvement across the organization.

Jennifer's first initiative was to mandate that each department submit five improvement ideas per month. She announced this at the quarterly all-hands meeting: "Starting next month, every department must submit five improvement suggestions. Departments failing to meet this quota will have it noted in their performance reviews. We need to innovate or die!"

### Three Months Later

The results were disappointing. The Quality Department dutifully submitted reports with suggestions like "improve quality" and "reduce defects" - vague and not actionable. Production supervisors complained that the mandate was "just more paperwork" that distracted from their real work of meeting production targets. The Engineering team submitted highly technical suggestions that would require million-dollar investments and years to implement.

Human Resources received 47 total suggestions in three months, but when reviewed:
- 23 were duplicates or near-duplicates
- 12 were complaints disguised as suggestions ("suggestion: fix the terrible cafeteria food")
- 8 were outside the suggester's area (operators suggesting accounting changes)
- 4 were actually viable ideas

Of the 4 viable ideas, none were implemented. When employees asked about their suggestions, managers said they were "under review" or "being evaluated by the committee." One production operator, Maria Santos, submitted a suggestion to rearrange her workstation to eliminate unnecessary reaching, which was causing her shoulder pain. Three months later, she had received no response. When asked, she said: "I won't bother suggesting anything again. Nobody cares what we think."

### Current State Assessment

Jennifer commissioned an employee survey (72% response rate) with these results:

**"I feel comfortable pointing out problems or suggesting improvements."**
- Strongly Agree: 8%
- Agree: 22%
- Neutral: 31%
- Disagree: 28%
- Strongly Disagree: 11%

**"When I make a mistake, I worry about being blamed or punished."**
- Strongly Agree: 34%
- Agree: 31%
- Neutral: 18%
- Disagree: 12%
- Strongly Disagree: 5%

**"Leadership actively participates in improvement efforts."**
- Strongly Agree: 5%
- Agree: 14%
- Neutral: 27%
- Disagree: 38%
- Strongly Disagree: 16%

**"I have the authority and resources to implement improvements in my work area."**
- Strongly Agree: 3%
- Agree: 9%
- Neutral: 21%
- Disagree: 42%
- Strongly Disagree: 25%

### Additional Context

Jennifer spoke with several employees confidentially to understand the problem:

**Production Supervisor (15 years tenure):** "Look, I'm evaluated on meeting my production numbers. When I spend time on 'improvement ideas,' my boss asks why production is behind. The message is clear - hit your numbers, don't rock the boat."

**Quality Engineer (3 years tenure):** "I suggested we implement mistake-proofing on Line 3, which would prevent 80% of our most common defect. I put together a proposal showing $50K investment with $200K annual savings. It's been sitting with the plant manager for six months. I've stopped trying."

**Machine Operator (8 years tenure):** "Three years ago, I suggested a different tool arrangement that would save 30 seconds per cycle. My supervisor said 'that's not how we do it.' Last year, a consultant suggested the exact same thing, and now it's being implemented as a 'best practice.' That told me everything I need to know about whose ideas matter here."

**Engineering Manager (12 years tenure):** "The problem is nobody here understands what real continuous improvement looks like. They think it's about suggestion boxes and 'trying harder.' We need a complete culture change, but that takes years and significant investment. Honestly, I'm not sure senior leadership has the patience for it."

### Financial Context

MidState's profit margins have declined from 18% (five years ago) to 11% (current year). Two major customers are pressuring for 5% price reductions or they'll source from overseas suppliers. The company needs to reduce costs by approximately $2.5 million annually to maintain competitiveness.

Jennifer estimates that the three-month "improvement initiative" consumed:
- 200 hours of manager time reviewing suggestions
- 150 hours of employee time generating suggestions
- $15,000 in consulting fees for the suggestion review system
- Unmeasurable damage to employee morale and trust

### The Consultant's Assessment

Jennifer hired a continuous improvement consultant, Dr. Robert Chen, who spent two weeks observing and interviewing. His key findings:

1. **Top-down mandate culture:** "Improvement" is something done TO employees, not BY employees
2. **Zero psychological safety:** Employees fear blame for mistakes and speaking up
3. **No visible leadership commitment:** Executives talk about improvement but don't visibly participate
4. **Process over people:** Focus on documentation (5 suggestions) rather than actual improvement
5. **No empowerment:** Employees can't implement even small changes without multiple approvals
6. **Idea graveyard:** Suggestions disappear into bureaucracy with no feedback loop
7. **Conflicting priorities:** Production metrics trump improvement efforts
8. **No learning system:** Mistakes trigger blame, not learning

Dr. Chen told Jennifer: "You tried to install a continuous improvement program in a culture of compliance and fear. It's like planting seeds in concrete and wondering why nothing grows. You need to change the soil before you can grow anything."

### Questions for Analysis

1. **Culture Assessment (15 points)**
   - Using the 8 Principles of Continuous Improvement, assess MidState's current culture. Which principles are weakest? Which are strongest (if any)? Support your assessment with specific evidence from the case.

2. **Root Cause Analysis (20 points)**
   - Apply the 5 Whys technique to answer: "Why did the improvement initiative fail?" What is the root cause (or causes) of the failure?

3. **Leadership Actions (20 points)**
   - What specific mistakes did Jennifer make in how she launched and managed the initiative? What should she have done differently? Identify at least 5 specific errors.

4. **Improvement Plan (25 points)**
   - Develop a 6-month action plan to begin transforming MidState's culture. What are the first 3-5 initiatives Jennifer should launch? Prioritize them and explain your reasoning. What quick wins should she pursue?

5. **Measurement Strategy (10 points)**
   - How should Jennifer measure progress in culture change over the next 6-12 months? Suggest 3-5 specific metrics that would indicate improvement is happening.

6. **Risk Management (10 points)**
   - What are the biggest risks or obstacles to culture change at MidState? How should Jennifer address them?

---

### Answer Key & Teaching Notes

#### Question 1: Culture Assessment (15 points)

**Strong Answer Should Include:**

Assessment against 8 Principles:

**❌ Customer Focus (Very Weak - 1/5):**
- No mention of how improvements would benefit customers
- Focus is internal compliance, not customer value
- Evidence: No customer requirements in suggestion criteria

**❌ Respect for People (Very Weak - 1/5):**
- Ideas from employees ignored; consultant ideas valued
- Top-down mandates without input
- Maria's suggestion ignored for 3 months
- Blame culture evident (65% fear punishment for mistakes)
- Evidence: "Nobody cares what we think" quote

**❌ Process Thinking (Weak - 2/5):**
- Treating symptom (lack of ideas) not root cause (culture)
- No systematic approach to improvement
- Evidence: Random suggestion system, no process improvement methodology

**❌ Data-Driven Decisions (Weak - 2/5):**
- Quality Engineer's data-driven proposal ($50K → $200K savings) ignored
- Decisions based on hierarchy not data
- Evidence: Consultant idea accepted, employee idea for same thing rejected

**❌ Continuous Learning (Very Weak - 1/5):**
- Mistakes trigger blame, not learning
- 65% fear punishment for mistakes
- Zero feedback on suggestions (learning opportunity lost)
- Evidence: Survey results show fear of blame

**❌ Leadership Commitment (Very Weak - 1/5):**
- Leaders talk but don't participate
- 54% disagree that leadership actively participates
- Jennifer announced initiative but didn't model behavior
- Evidence: Survey results, supervisor quote about conflicting priorities

**❌ Teamwork & Collaboration (Weak - 2/5):**
- Suggestions submitted to committee, not team problem-solving
- Departments competing for quota, not collaborating
- Evidence: Departmental quotas create competition

**❌ Persistence & Patience (Very Weak - 1/5):**
- Three months then frustration
- Engineering Manager says leadership lacks patience
- Quick mandate without foundation-building
- Evidence: 3-month timeline, consultant quote

**Overall Culture Maturity: 11/40 (28%) - Pre-Continuous Improvement Stage**

**Teaching Point:** True CI culture requires years to develop. Quick programs fail without cultural foundation. MidState has compliance culture trying to implement improvement culture—they're incompatible.

#### Question 2: Root Cause Analysis (20 points)

**Strong Answer Should Include:**

**5 Whys Application:**

**Problem:** Improvement initiative failed (4 viable ideas, 0 implemented, low morale)

**Why #1:** Employees didn't submit good ideas and submitted ideas weren't implemented
- Why? → Because employees didn't believe their ideas mattered and no system existed to evaluate/implement

**Why #2:** Employees didn't believe their ideas mattered
- Why? → Because past suggestions were ignored (Maria's example, consultant vs. employee suggestion)

**Why #3:** Past suggestions were ignored
- Why? → Because no accountability system existed for responding to suggestions and leaders didn't prioritize improvement over production

**Why #4:** Leaders didn't prioritize improvement over production
- Why? → Because performance evaluation system rewards production numbers, not improvement participation

**Why #5:** Performance system doesn't reward improvement
- Why? → Because leadership hasn't fundamentally changed what they value and measure (still measuring old metrics)

**ROOT CAUSE:** Leadership attempted to change employee behavior (submit ideas) without changing the underlying systems, incentives, and leadership behaviors that drive culture. Surface-level program imposed on unchanged foundation.

**Alternative/Parallel Root Causes:**
- No psychological safety (fear-based culture incompatible with improvement)
- Top-down mandate approach vs. grassroots engagement
- Lack of leadership understanding of what CI culture requires
- No empowerment or authority given to employees

**Teaching Point:** Culture change must start with leadership changing themselves and systems, not mandating employee behavior change. You can't mandate culture.

#### Question 3: Leadership Actions (20 points)

**Strong Answer Should Identify These Mistakes:**

**1. Top-Down Mandate Without Engagement**
- **Mistake:** Announced quota requirement without employee input or buy-in
- **Should Have:** Started with pilot team of volunteers, built excitement through early wins, let demand for participation grow organically
- **Principle Violated:** Respect for People

**2. No Leadership Modeling**
- **Mistake:** Jennifer didn't visibly participate in improvements herself
- **Should Have:** Led by example - implemented improvements in executive area, went to Gemba, worked on improvement team
- **Principle Violated:** Leadership Commitment

**3. Quantity Over Quality Metrics**
- **Mistake:** Measured # of suggestions not quality or implementation
- **Should Have:** Measured # implemented and results achieved, not # submitted
- **Principle Violated:** Data-Driven Decisions (wrong data)

**4. No Response System**
- **Mistake:** Suggestions went into black hole with no feedback
- **Should Have:** 48-hour acknowledgment, 2-week decision, clear escalation path, regular feedback to submitter
- **Principle Violated:** Respect for People, Process Thinking

**5. Conflicting Priorities Not Addressed**
- **Mistake:** Added improvement mandate while keeping production-only performance reviews
- **Should Have:** Changed performance metrics to include improvement participation, gave protected time for improvement
- **Principle Violated:** Leadership Commitment

**6. No Training or Support**
- **Mistake:** Expected employees to know how to identify and document improvements without training
- **Should Have:** Provided training on problem-solving, waste identification, how to write good suggestions
- **Principle Violated:** Continuous Learning

**7. No Empowerment**
- **Mistake:** Required approvals even for small changes (Maria's workstation)
- **Should Have:** Given employees authority to implement small changes in their areas immediately (Just Do It zone)
- **Principle Violated:** Respect for People

**8. Impatient Timeline**
- **Mistake:** Expected results in 3 months, culture change takes years
- **Should Have:** Set realistic 2-3 year vision with 6-month milestones
- **Principle Violated:** Persistence & Patience

**Teaching Point:** Common CEO mistake - attending conference, getting excited, mandating program without understanding what it requires. Improvement is cultural transformation, not program implementation.

#### Question 4: Improvement Plan (25 points)

**Strong Answer Should Include:**

**6-Month Transformation Roadmap:**

**Phase 1: Foundation (Month 1-2) - BUILD TRUST**

**Initiative 1: Leadership Alignment & Training (Week 1-4)**
- **What:** Executive team attends 2-day CI culture workshop, reads key books, aligns on 3-year vision
- **Why:** Leaders must understand and commit before asking employees to change
- **Quick Win:** Executives publicly admit suggestion system failed, apologize, commit to different approach
- **Metric:** 100% executive participation, documented commitment

**Initiative 2: Pilot Team Selection (Week 3-4)**
- **What:** Select 5-8 volunteers (not mandated) from different areas for pilot improvement team
- **Why:** Volunteers are motivated; early success builds momentum
- **Quick Win:** Address Maria's workstation immediately (within 1 week), publicize that old suggestion was implemented
- **Metric:** 8 volunteers recruited, Maria's suggestion implemented

**Phase 2: Quick Wins & Capability Building (Month 2-3) - DEMONSTRATE COMMITMENT**

**Initiative 3: Executive Gemba Walks (Weekly ongoing)**
- **What:** Each executive spends 2 hours/week at Gemba, asks "What makes your job difficult?" and "What would you improve?"
- **Why:** Visible leadership engagement, builds relationships, identifies problems
- **Quick Win:** Each executive implements 1 employee-suggested improvement within their first 3 walks
- **Metric:** # of walks completed, # of improvements implemented

**Initiative 4: Just Do It Zone (Month 2)**
- **What:** Empower employees to make changes in their workspace without approval if: costs <$50, affects only their area, doesn't impact safety/quality
- **Why:** Immediate empowerment, removes bureaucracy, fast results
- **Quick Win:** 20+ Just Do It improvements in Month 2
- **Metric:** # of Just Do It improvements, time saved, $ saved

**Initiative 5: Pilot Team DMAIC Project (Month 2-4)**
- **What:** Train pilot team in DMAIC, select meaningful problem ($50K+ impact), complete project with coaching
- **Why:** Builds CI capability, demonstrates methodology works, creates advocates
- **Quick Win:** Project shows measurable results (30%+ improvement)
- **Metric:** Project completion, $ saved, team satisfaction

**Phase 3: Expand & Embed (Month 4-6) - BUILD MOMENTUM**

**Initiative 6: Suggestion System 2.0 (Month 4)**
- **What:** New system: 48-hour acknowledgment, 2-week decision, clear criteria, suggester involved in implementation
- **Why:** Fixes old system failures, shows ideas matter
- **Quick Win:** 80%+ of suggestions get response within SLA
- **Metric:** Response time, implementation rate, employee satisfaction

**Initiative 7: Performance System Alignment (Month 5)**
- **What:** Add CI participation to performance reviews (20% weight): # of improvements participated in, Gemba engagement, idea generation/implementation
- **Why:** "What gets measured gets done" - aligns incentives with desired behavior
- **Quick Win:** Supervisors evaluated on team improvement participation, not just production
- **Metric:** % of reviews including CI metrics

**Initiative 8: Celebrate & Communicate (Ongoing)**
- **What:** Monthly CI newsletter, quarterly celebration event, visible before/after displays, storytelling
- **Why:** Recognition reinforces behavior, communication spreads success, builds excitement
- **Quick Win:** Month 1 - celebrate Maria's suggestion, apologize for delay, publicize change
- **Metric:** Newsletter reach, event attendance, stories shared

**Prioritization Rationale:**

1. **Trust First:** Can't improve without trust. Maria story + apology + quick implementation = trust-building
2. **Leadership Commitment:** Executives must change first (Gemba walks, training)
3. **Volunteers:** Start with willing, avoid forcing reluctant
4. **Quick Wins:** Fast visible results build momentum and credibility
5. **Systems Change:** Align incentives (performance reviews) to sustain culture

**Teaching Point:** Culture change sequence: Leadership changes → Small group success → Expand based on demand → System alignment → Sustained culture. Can't skip steps or reverse order.

#### Question 5: Measurement Strategy (10 points)

**Strong Answer Should Include:**

**Culture Change Metrics (Leading Indicators):**

**1. Psychological Safety Index (Monthly Survey)**
- "I feel comfortable pointing out problems" - Target: 30% → 60% positive in 12 months
- "I fear blame for mistakes" - Target: 65% → 30% agree in 12 months
- **Why:** Core foundation for CI culture

**2. Leadership Engagement (Weekly)**
- # of Gemba walks completed by executives - Target: 100% complete weekly walks
- # of improvements implemented from executive Gemba walks - Target: 1 per walk
- **Why:** Visible leadership commitment

**3. Employee Empowerment (Monthly)**
- # of Just Do It improvements - Target: 50+ per month by Month 6
- # of employee-led improvement team participants - Target: 20% of workforce by Month 12
- **Why:** Measures actual authority and participation

**4. Idea Flow & Response (Weekly)**
- Suggestion response time - Target: 95% within 48 hours
- Implementation rate - Target: 40%+ of suggestions implemented or tested
- **Why:** System effectiveness and respect for people

**5. Improvement Results (Monthly)**
- $ saved from implemented improvements - Target: $100K in 6 months, $500K in 12 months
- # of improvement projects completed - Target: 8 projects in 12 months
- **Why:** Business impact and methodology adoption

**Lagging Indicators (Quarterly):**
- Employee engagement survey scores (continuous improvement items)
- Voluntary turnover rate (should decrease as culture improves)
- Customer complaints (should decrease as improvements take effect)
- Profit margin (ultimate goal: maintain competitiveness)

**Teaching Point:** Measure culture inputs (behaviors) not just outputs (results). Leading indicators predict success before financial results appear. Early focus on psychological safety and leadership engagement.

#### Question 6: Risk Management (10 points)

**Strong Answer Should Include:**

**Major Risks & Mitigation Strategies:**

**Risk 1: Middle Management Resistance**
- **Why:** Supervisors evaluated on production may sabotage CI efforts they see as distraction
- **Mitigation:** 
  - Change performance metrics to include CI (20% weight)
  - Train managers on CI leadership
  - Involve managers in pilot team selection
  - Celebrate managers who support team improvements
- **Early Warning Sign:** Managers not releasing team members for improvement work

**Risk 2: Employee Cynicism ("Here We Go Again")**
- **Why:** Burned by previous failed initiative, won't trust new approach
- **Mitigation:**
  - Public acknowledgment that previous approach failed
  - Apology and explanation of what will be different
  - Fast visible action (Maria's suggestion) proves sincerity
  - Volunteer-only initially (no mandate)
- **Early Warning Sign:** Low volunteer signup, sarcastic comments

**Risk 3: Executive Patience/Consistency**
- **Why:** Culture change takes 2-3 years, quarterly pressures may derail effort
- **Mitigation:**
  - Set realistic timeline expectations upfront
  - Show early financial results (Quick Wins)
  - Quarterly culture change dashboard for board
  - External coach/consultant to maintain focus
- **Early Warning Sign:** Executives skipping Gemba walks, introducing competing initiatives

**Risk 4: Insufficient Capability Development**
- **Why:** Enthusiasm without skill leads to poor projects and frustration
- **Mitigation:**
  - Invest in training (DMAIC, problem-solving, facilitation)
  - Hire/develop internal CI coaches
  - Start with coached projects, not independent teams
  - Use external expertise for first 12 months
- **Early Warning Sign:** Projects stalling, teams frustrated, poor results

**Risk 5: Scaling Too Fast**
- **Why:** Expanding before pilot proves successful dilutes resources and risks failure
- **Mitigation:**
  - Resist pressure to "roll out" quickly
  - Let demand build ("I want to join improvement team")
  - Expand only after pilot succeeds (measurable results + team satisfaction)
  - Build capacity (coaches) before expanding
- **Early Warning Sign:** Multiple half-trained teams, declining quality of projects

**Risk 6: Customer Price Pressure Derails Effort**
- **Why:** $2.5M cost reduction needed, executives may panic and cut CI investment
- **Mitigation:**
  - Show CI is path to cost reduction (not expense)
  - Target improvement projects at high-cost areas
  - Calculate and publicize savings monthly
  - Tie CI to business survival narrative
- **Early Warning Sign:** CI budget cuts, executives pull back from Gemba

**Teaching Point:** Culture change is fragile, especially early. Anticipate resistance, plan mitigation, monitor for warning signs. Leadership consistency and patience are most critical success factors.

---

## Case Study 2: StatPro's Quality Crisis

**Course:** Course 2 - Basic Statistics for CI  
**Length:** 1,100 words  
**Difficulty:** Beginner  
**Estimated Analysis Time:** 60-75 minutes

### Background

StatPro Industries manufactures precision injection-molded plastic components for medical devices. Their flagship product, the FlowValve housing, must meet extremely tight dimensional specifications to ensure proper function and patient safety. The critical dimension is the inner diameter (ID), which must be 10.00mm ± 0.15mm (specification limits: 9.85mm to 10.15mm). The target is exactly 10.00mm.

The FlowValve represents 40% of StatPro's revenue ($12 million annually). Each valve that passes inspection is sold for $8.50. Defective valves must be scrapped at a cost of $3.20 each (material + labor). StatPro manufactures approximately 1.5 million valves per year.

### The Problem Emerges

In March, StatPro's largest customer, MedTech Corp, reported an increase in field failures. Valves that passed StatPro's inspection were failing in assembly or, worse, during testing at MedTech's facility. MedTech issued a warning: "Improve quality to acceptable levels within 90 days, or we'll be forced to find an alternative supplier."

The Quality Manager, David Park, was shocked. "Our inspection shows we're meeting specifications!" he insisted. He pulled the most recent inspection data from the past 60 days (150 valves measured, one per production lot):

### Inspection Data Summary (60 Days, n=150)

**Descriptive Statistics:**
- Mean: 9.99mm
- Median: 9.98mm
- Mode: 9.95mm
- Standard Deviation: 0.18mm
- Minimum: 9.42mm
- Maximum: 10.44mm
- Range: 1.02mm

**Specification Conformance:**
- Parts below LSL (9.85mm): 22 valves (14.7%)
- Parts above USL (10.15mm): 15 valves (10.0%)
- Parts within specification: 113 valves (75.3%)
- **Defect Rate: 24.7%**

David's reaction: "We're at 75% pass rate. That's a C grade—not great, but acceptable. The mean is 9.99mm, which is almost exactly on target! I don't understand why MedTech is complaining."

### The Investigation

The Plant Manager, Sarah Chen, brought in a continuous improvement specialist, Dr. Emily Rodriguez, to investigate. Dr. Rodriguez requested more detailed data and spent a week analyzing the process.

**Finding 1: Measurement System Analysis**

Dr. Rodriguez had three inspectors measure the same 10 valves five times each:

**Measurement System Gage R&R Results:**
- Total Variation: 0.18mm standard deviation
- Measurement Variation: 0.06mm standard deviation
- Part-to-Part Variation: 0.17mm standard deviation
- Gage R&R as % of Total Variation: 33%

**Dr. Rodriguez's Assessment:** "Your measurement system contributes 33% of the variation you're seeing. Industry standard says measurement should be <10% of total variation. You're measuring noise, not just signal. Some 'good' parts are being measured as 'bad,' and some 'bad' parts are being measured as 'good.'"

**Finding 2: Process Capability Analysis**

Dr. Rodriguez collected 100 consecutive parts from a stable production run and measured each with a properly calibrated instrument:

**Capability Study Results:**
- Process Mean: 9.92mm (off-target by 0.08mm)
- Process Standard Deviation: 0.12mm
- Specification Range: 0.30mm (10.15 - 9.85)

**Calculations:**
- Cp = (USL - LSL) / (6σ) = (10.15 - 9.85) / (6 × 0.12) = 0.30 / 0.72 = **0.42**
- Cpk = min[(USL - Mean)/(3σ), (Mean - LSL)/(3σ)]
  - CPU = (10.15 - 9.92) / (3 × 0.12) = 0.23 / 0.36 = 0.64
  - CPL = (9.92 - 9.85) / (3 × 0.12) = 0.07 / 0.36 = 0.19
  - Cpk = min(0.64, 0.19) = **0.19**

**Industry Benchmarks:**
- Cpk < 1.0: Process is incapable
- Cpk = 1.0: Marginal (2,700 PPM defects)
- Cpk = 1.33: Capable (63 PPM defects)
- Cpk ≥ 2.0: Six Sigma quality (0.002 PPM defects)

**Finding 3: Distribution Analysis**

Dr. Rodriguez created a histogram of the 100-part sample. The distribution showed:
- Right-skewed (tail extending toward larger values)
- Peak around 9.88mm
- Several outliers above 10.20mm
- Distribution clearly non-normal

She also stratified the data by production shift:

**Shift Performance:**
- **Day Shift (6am-2pm):** Mean = 9.98mm, SD = 0.08mm, Defect rate = 8%
- **Swing Shift (2pm-10pm):** Mean = 9.88mm, SD = 0.14mm, Defect rate = 35%
- **Night Shift (10pm-6am):** Mean = 9.90mm, SD = 0.16mm, Defect rate = 28%

**Finding 4: Time-Series Analysis**

Dr. Rodriguez plotted measurements over time and discovered:
- Gradual upward drift within each shift (parts getting larger over time)
- Sharp reset at beginning of each shift
- Drift rate: approximately 0.04mm per 4 hours

Investigation revealed: molding machine temperature increases during operation, causing dimensional drift. Operators reset temperature at shift start but don't adjust during shift.

### Additional Context

**Cost Analysis:**

Current state (based on 150-part sample projected to annual volume):
- Annual production: 1,500,000 valves
- Current defect rate: 24.7%
- Annual defects: 370,500 valves
- Scrap cost: 370,500 × $3.20 = **$1,185,600 annual scrap cost**
- Inspection cost: $0.50 per valve × 1,500,000 = $750,000
- Customer returns/complaints (estimated): $200,000
- **Total quality cost: $2,135,600 (17.8% of revenue)**

Dr. Rodriguez's estimate if process improved to Cpk = 1.33:
- Projected defect rate: 0.42% (from 24.7%)
- Annual defects: 6,300 valves
- Scrap cost: 6,300 × $3.20 = $20,160
- **Annual savings opportunity: $1,165,440**

**Root Cause Hypotheses:**

Dr. Rodriguez identified potential causes:
1. Machine temperature drift during shifts
2. Poor measurement system (33% of variation)
3. Process mean off-center (9.92mm vs. 10.00mm target)
4. High process variation (σ = 0.12mm)
5. Operator differences between shifts
6. Material variation (not yet investigated)

### Questions for Analysis

**Question 1: Data Interpretation (20 points)**
- Calculate and interpret the Coefficient of Variation (CV) for the inspection data. What does it tell you about process consistency?
- David said "the mean is 9.99mm, almost exactly on target" - why is this reasoning flawed? What other statistics should he consider?
- Interpret the difference between Cp (0.42) and Cpk (0.19). What does this tell you about the process?

**Question 2: Process Capability Assessment (25 points)**
- Given Cpk = 0.19, approximately what defect rate would you expect? (Use the relationship between Cpk and sigma level)
- The measured defect rate is 24.7%. Is this consistent with Cpk = 0.19? Explain any discrepancies.
- If StatPro achieves Cpk = 1.33, what defect rate would they expect? Calculate the ROI of achieving this improvement (use cost data provided).

**Question 3: Variation Analysis (20 points)**
- The Gage R&R is 33%. Explain what this means in practical terms. How does measurement error affect David's understanding of the process?
- Why does the difference in standard deviation between shifts matter? (Day = 0.08mm, Swing = 0.14mm, Night = 0.16mm)
- What does the time-series drift pattern suggest about the root cause?

**Question 4: Problem Prioritization (20 points)**
- Dr. Rodriguez identified three major issues: (1) measurement system (33% error), (2) process off-center (9.92mm vs. 10.00mm target), (3) high variation (σ = 0.12mm). Which should be addressed first? Why? What's the logical sequence?
- Using the shift stratification data, what quick wins could StatPro achieve?

**Question 5: Improvement Recommendations (15 points)**
- Develop specific recommendations to address: (a) the measurement system problem, (b) the off-center process mean, (c) the temperature drift during shifts.
- What ongoing monitoring would you recommend after improvements are implemented?

---

### Answer Key & Teaching Notes (Available for Instructors)

*[Answer key follows same comprehensive format as Case Study 1, with detailed solutions, teaching points, and common student errors to address]*

---

## Case Study 3: WestPac Distribution's Warehouse Crisis

**Course:** Course 3 - Lean Fundamentals  
**Length:** 1,050 words  
**Difficulty:** Beginner  
**Estimated Analysis Time:** 60-75 minutes

### Background

WestPac Distribution operates a 200,000 square foot warehouse in Seattle, fulfilling orders for a national office supply retailer. The facility processes approximately 5,000 order lines per day with a team of 45 warehouse associates and 8 supervisors.

Over the past six months, WestPac has experienced deteriorating performance:
- On-time shipment rate: 94% → 82%
- Order accuracy: 97% → 91%
- Orders per labor hour: 18 → 14 (22% productivity decline)
- Worker's compensation claims: 3 → 11 (injuries/strain)

Their largest customer, OfficeMax Regional, has issued a warning: "Improve on-time delivery to 95%+ and accuracy to 98%+ within 60 days, or we'll move the contract to another distributor." This contract represents $18 million (40% of WestPac's revenue).

### The Waste Walk

Operations Manager Marcus Thompson conducted a waste walk with his team, spending 8 hours observing operations and timing activities. Here's what they discovered:

**Observation 1: The Picking Process**

Following picker Janet Rodriguez for two hours while she filled a typical 12-item order:

| Activity | Time | Value-Added? | Notes |
|----------|------|--------------|-------|
| Print pick ticket | 2 min | No (NVA-N) | Printer down, walked to backup printer |
| Walk to first item | 4 min | No (Waste) | Item in far corner, Zone G |
| Search for item | 3 min | No (Waste) | Location label faded, checked 3 shelves |
| Pick item, scan | 0.5 min | YES | Actual value-added work |
| Walk to second item | 6 min | No (Waste) | Items not sequenced by location |
| Search for item | 1 min | No (Waste) | Found on second shelf |
| Pick item, scan | 0.5 min | YES | Actual value-added work |
| Walk to third item | 5 min | No (Waste) | Back to Zone B (opposite direction) |
| Search for item | 4 min | No (Waste) | Item not in designated location |
| Call supervisor | 2 min | No (Waste) | Supervisor found item in wrong spot |
| Pick item, scan | 0.5 min | YES | Actual value-added work |
| *[Pattern continues for remaining 9 items]* | | | |
| **Total for 12 items** | **78 min** | | |
| **Value-added time** | **6 min** | | Actual picking/scanning |
| **Process efficiency** | **7.7%** | | VA time / Total time |

**Time breakdown for full order:**
- Value-added (picking/scanning): 6 minutes (7.7%)
- Walking: 47 minutes (60%)
- Searching: 15 minutes (19%)
- Waiting (printer, supervisor, etc.): 10 minutes (13%)

Marcus was stunned: "Janet walked for almost an hour to fulfill one order? And only 6 minutes was actual productive work?"

**Observation 2: The 8 Wastes (DOWNTIME) Identified**

**D - Defects:**
- Picking errors: 9% of orders (1 in 11 orders has wrong item or quantity)
- Root causes: Faded labels, items in wrong locations, similar-looking items adjacent
- Rework: 30 minutes average to correct each error

**O - Overproduction:**
- Replenishment team brings full pallets to pick locations even when only 2 boxes needed
- Result: Congested aisles, items stored in temporary locations, lost inventory

**W - Waiting:**
- Pickers wait average 12 minutes per shift for equipment (scanners, pallet jacks)
- Charger stations in only 2 locations (long walk to charge scanners)
- Average 3 equipment breakdowns per day (10-15 minute waits)

**N - Non-Utilized Talent:**
- Experienced pickers (5+ years) doing same work as new hires
- Pickers have suggested improvements but "that's not your job"
- No formal improvement suggestion process

**T - Transportation:**
- Average pick route: 0.8 miles walked per order
- Items not organized by picking frequency
- No route optimization (pick tickets list items by customer order, not warehouse location)

**I - Inventory:**
- 15% of locations contain slow-moving items (picked <1x/month)
- These slow-movers occupy prime locations (waist-height, near shipping)
- Fast-movers often in hard-to-reach locations (high shelves, far corners)
- $2.3M inventory, but only $1.8M actively picked (22% is "dead stock")

**M - Motion:**
- Pickers constantly bend/reach (items stored floor to 8-foot height)
- No consideration for ergonomics or pick frequency
- 11 injury claims in 6 months (up from 3)

**E - Excess Processing:**
- Every item scanned 3 times: receiving, putaway, picking (third scan redundant)
- Paper pick tickets printed, then data re-entered into shipping system
- Double-checking of already-scanned picks

**Observation 3: The 5S Assessment**

Marcus scored each zone on 5S:

**Sort (Seiri):**
- Empty boxes, pallets, and packing materials scattered throughout
- Old equipment and broken pallet jacks parked in aisles (not discarded)
- 30% of items in each zone are obsolete or slow-moving
- **Score: 2/10**

**Set in Order (Seiton):**
- No visual management or location organization system
- Items arranged by arrival date, not by pick frequency or product family
- Scanners, tools, and equipment have no designated homes
- **Score: 2/10**

**Shine (Seiso):**
- Dust and debris on shelves (difficult to read labels)
- Broken equipment not removed or repaired
- No cleaning schedule or standards
- **Score: 3/10**

**Standardize (Seiketsu):**
- Each shift has different methods (no standard work)
- No visual standards or guidelines posted
- Labeling inconsistent (some zones color-coded, others not)
- **Score: 2/10**

**Sustain (Shitsuke):**
- Previous improvement attempts abandoned
- No accountability for maintaining standards
- "We're too busy to organize" mentality
- **Score: 1/10**

**Overall 5S Maturity: 10/50 (20%)**

### Root Cause Analysis

Marcus conducted 5 Whys on the low productivity:

**Problem:** Orders per labor hour dropped from 18 to 14 (22% decline)

- **Why?** → Each order takes longer to complete
  - **Why?** → Pickers spend more time walking and searching
    - **Why?** → Items are disorganized and not arranged by pick frequency
      - **Why?** → Inventory placement is based on available space, not optimization
        - **Why?** → No systematic warehouse layout design; items placed wherever space available as they arrive
          - **ROOT CAUSE:** Reactive inventory placement without strategic layout design, resulting in high-runners in poor locations and slow-movers in prime locations

### The Business Impact

**Current State Performance:**
- Orders per labor hour: 14
- Labor cost: $22/hour (loaded rate with benefits)
- Cost per order: $22 / 14 = **$1.57 per order**
- Daily orders: 5,000
- Daily labor cost: $1.57 × 5,000 = **$7,850**
- Annual labor cost: $7,850 × 250 days = **$1,962,500**

**Competitor Benchmark:**
- Industry best practice: 24 orders per labor hour
- Best-in-class: 28 orders per labor hour

**Improvement Opportunity:**
If WestPac improved to 20 orders per labor hour (conservative target):
- Cost per order: $22 / 20 = $1.10 per order
- Annual labor cost: $1.10 × 5,000 × 250 = $1,375,000
- **Annual savings: $587,500**

**Additional Costs of Current State:**
- Error rework: 9% error rate × 5,000 orders × $30 rework = $13,500/day = $3,375,000/year
- Injury claims: $45,000 average × 11 claims = $495,000
- Customer penalties (late delivery): Estimated $250,000/year
- **Total annual waste: $4,707,500 (26% of revenue)**

### Questions for Analysis

**Question 1: Waste Identification & Quantification (25 points)**
- For each of the 8 wastes (DOWNTIME), identify specific examples from the case and estimate the time/cost impact. Which waste has the biggest impact?
- Calculate the total distance pickers walk daily (5,000 orders × 0.8 miles/order). If walking speed is 3 mph, how many hours per day are spent just walking? What's the annual cost?
- Janet's process efficiency is 7.7%. What would "world-class" look like? (Hint: Lean facilities target 40-60% value-added time)

**Question 2: 5S Implementation Plan (25 points)**
- Design a specific 5S implementation plan for one zone of the warehouse (choose Zone G - far corner where first item was located). For each S, provide 3-5 specific actions.
- What obstacles or resistance might Marcus encounter? How should he address them?
- How should Marcus measure 5S improvement over time?

**Question 3: Layout Optimization (20 points)**
- Explain the concept of "slotting" in warehouse operations. How should fast-moving items be located differently from slow-moving items?
- If WestPac reorganizes so the top 20% of SKUs (by pick frequency) are in optimal locations (closest to shipping, waist height), what impact would you expect on walking time and productivity?
- Design a simple ABC analysis approach: A items (top 20% of picks), B items (next 30%), C items (remaining 50%). Where should each category be located and why?

**Question 4: Quick Wins vs. Long-Term Improvements (15 points)**
- Identify 3 "quick wins" Marcus could implement within 2 weeks with minimal investment. What impact would each have?
- Identify 2-3 longer-term improvements (1-3 months) that require planning and investment. Justify the ROI for each.

**Question 5: Sustaining Improvement (15 points)**
- Why did previous improvement attempts fail at WestPac? (Hint: "Sustain" scored 1/10)
- Design a sustainability plan including daily/weekly/monthly activities to maintain improvements.
- How should Marcus involve the warehouse associates in the improvement process?

---

### Answer Key & Teaching Notes (Available for Instructors)

*[Answer key follows same comprehensive format, with detailed solutions, teaching points, and common student errors to address]*

---

## Case Study 4: HealthFirst's Emergency Department Wait Time Problem

**Course:** Course 4 - Six Sigma DMAIC Overview  
**Length:** 1,200 words  
**Difficulty:** Beginner  
**Estimated Analysis Time:** 75-90 minutes

### Background

HealthFirst Regional Medical Center is a 400-bed hospital serving a community of 250,000 in suburban Indiana. Their Emergency Department (ED) sees approximately 45,000 patients annually (125 patients per day average).

Over the past year, patient satisfaction scores for the ED have dropped dramatically:
- "Overall ED experience" rating: 4.2/5.0 → 3.1/5.0
- "Wait time to see provider" rating: 3.8/5.0 → 2.4/5.0
- "Likelihood to recommend" rating: 85% → 58%

Online reviews are brutal. Common themes:
- "Waited 4 hours in waiting room for a sprained ankle"
- "Nurses were great, but the wait was unacceptable"
- "Left after 3 hours without being seen - went to CompeteCare Urgent Care instead"

Two key financial impacts:
1. **Patient volume declining:** Down 12% year-over-year as patients choose competitors
2. **Press Ganey penalties:** Hospital losing $340,000 in quality incentive payments due to low patient satisfaction scores

The CEO, Dr. Patricia Morrison, mandated a Six Sigma DMAIC project: "Fix wait times within 90 days. Our ED is losing patients and money."

### DEFINE Phase

Project Champion: Dr. Ellen Martinez, ED Medical Director  
Project Leader: James Chen, Quality Improvement Manager  
Team: 2 ED nurses, 1 registration clerk, 1 physician, 1 data analyst

**Problem Statement:**
"Emergency Department wait times at HealthFirst Regional have increased from an average of 45 minutes (12 months ago) to 118 minutes (current), resulting in declining patient satisfaction (4.2 → 3.1), 12% volume reduction, and $340K annual revenue loss from quality penalties."

**Goal Statement:**
"Reduce average ED wait time from 118 minutes to 60 minutes or less within 90 days, improving patient satisfaction to 4.0+ and regaining lost patient volume."

**Scope:**
- **In Scope:** Wait time from patient arrival to provider assessment (door-to-provider time)
- **In Scope:** Emergency Severity Index (ESI) levels 3-5 (non-urgent to urgent)
- **Out of Scope:** ESI levels 1-2 (immediate/emergent - different process)
- **Out of Scope:** Treatment time after provider assessment

**SIPOC Analysis:**

| Suppliers | Inputs | Process | Outputs | Customers |
|-----------|--------|---------|---------|-----------|
| Patients | Patient arrival | Registration | Registered patient | ED clinical staff |
| EMS | Medical history | Triage | Triage assessment | Patient |
| Referring physicians | Insurance info | Waiting | Patient in room | Hospital admin |
| | Symptoms | Room placement | Provider assessment | Payers |
| | | Provider exam | | |

### MEASURE Phase

The team collected 30 days of data on 3,750 ED visits (ESI 3-5 only):

**Overall Wait Time Statistics:**
- Mean: 118 minutes
- Median: 97 minutes
- Mode: 85 minutes
- Standard Deviation: 67 minutes
- Minimum: 22 minutes
- Maximum: 387 minutes
- 95th percentile: 245 minutes

**Target:** 60 minutes average (internal goal based on patient satisfaction correlation)

**Specification Limits:**
- Hospital's published commitment: "Average wait time <90 minutes"
- Current mean (118 min) exceeds this by 31%

**Process Capability:**
Using USL = 90 minutes, no LSL (faster is always better):
- Cpk = (90 - 118) / (3 × 67) = -28 / 201 = **-0.14**
- **Interpretation:** Negative Cpk means process mean is beyond specification limit. Process is severely incapable.

**Data Stratification:**

**By Time of Day:**
- Morning (6am-2pm): Mean = 78 min, SD = 42 min
- Afternoon (2pm-10pm): Mean = 135 min, SD = 71 min
- Night (10pm-6am): Mean = 89 min, SD = 58 min

**By Day of Week:**
- Weekday: Mean = 108 min, SD = 63 min
- Weekend: Mean = 142 min, SD = 74 min

**By ESI Level:**
- ESI 3 (Urgent): Mean = 95 min, SD = 52 min
- ESI 4 (Less Urgent): Mean = 129 min, SD = 68 min
- ESI 5 (Non-urgent): Mean = 158 min, SD = 79 min

**Process Steps Timed (n=50 observations):**

| Step | Average Time | Std Dev | % of Total Wait |
|------|--------------|---------|-----------------|
| Arrival to registration start | 8 min | 12 min | 7% |
| Registration process | 12 min | 5 min | 10% |
| Registration to triage | 25 min | 28 min | 21% |
| Triage assessment | 7 min | 3 min | 6% |
| Triage to room assignment | 48 min | 42 min | 41% |
| Room assignment to room ready | 6 min | 8 min | 5% |
| Room ready to provider | 12 min | 11 min | 10% |
| **TOTAL** | **118 min** | **67 min** | **100%** |

**Key Finding:** "Triage to room assignment" represents 41% of total wait time (48 minutes average).

**Operational Data:**
- ED capacity: 24 beds
- Average length of stay (LOS) after provider assessment: 187 minutes
- Bed turnover time: 32 minutes (room clean/prep)
- Daily arrivals: 125 patients (average)
- Peak arrival time: 2pm-6pm (35% of daily volume in 4 hours)

**Bed Occupancy Analysis:**

Average occupancy by time of day:
- Morning: 68% (16 of 24 beds occupied)
- Afternoon/Evening: 96% (23 of 24 beds occupied)
- Night: 75% (18 of 24 beds occupied)

**Occupancy >95% creates bottleneck - no beds available for new patients in triage.**

### ANALYZE Phase

**Root Cause Hypothesis Testing:**

**Hypothesis 1: Not Enough Beds**
- Test: Calculate theoretical capacity vs. actual demand
- Calculation: 
  - Beds: 24
  - Hours per day: 24
  - Bed capacity: 24 beds × 24 hours = 576 bed-hours/day
  - Average LOS: 187 min (3.1 hours) per patient
  - Daily demand: 125 patients × 3.1 hours = 388 bed-hours/day
  - Utilization: 388 / 576 = 67% average
- **Conclusion: Capacity IS adequate on average. Not a capacity problem, but a flow/timing problem.**

**Hypothesis 2: Afternoon Arrival Surge Creates Bottleneck**
- Test: Plot arrivals by hour vs. bed occupancy
- Finding: 35% of patients arrive 2pm-6pm (44 patients), but bed capacity same as other times
- During peak:
  - Arrivals: 11 patients/hour
  - Discharges: 7 patients/hour
  - Net: +4 patients/hour for 4 hours = 16 patient backlog
  - Backlog creates 2-3 hour wait for bed
- **Conclusion: CONFIRMED - Arrival surge creates temporary capacity shortage even though daily capacity is adequate.**

**Hypothesis 3: Long Length of Stay (LOS) Ties Up Beds**
- Test: Compare HealthFirst LOS to national benchmarks
- HealthFirst average LOS: 187 minutes
- National average: 151 minutes (per CDC data)
- HealthFirst is 24% higher than benchmark
- **Conclusion: CONFIRMED - Long LOS reduces throughput, contributing to afternoon bottleneck.**

**Hypothesis 4: Slow Bed Turnover**
- Test: Measure time from patient discharge to bed ready for next patient
- HealthFirst: 32 minutes average
- National benchmark: 18 minutes
- HealthFirst is 78% slower than benchmark
- **Conclusion: CONFIRMED - Slow turnover reduces effective capacity by ~30 minutes per cycle.**

**Hypothesis 5: Triage Delays**
- Test: Time from registration to triage completion
- Data: 25 minutes average (waiting) + 7 minutes (triage) = 32 minutes total
- Benchmark: 15 minutes (ACEP recommended)
- **Conclusion: CONFIRMED - Triage wait contributes 15+ minutes of unnecessary wait.**

**Fishbone Diagram Key Causes Identified:**

**Man (People):**
- Insufficient staffing during 2pm-6pm peak
- Triage nurse also handling other duties (not dedicated)

**Machine (Equipment/Facilities):**
- Only 24 beds, cannot flex capacity during surge

**Method (Process):**
- No fast-track for ESI 4-5 (less urgent patients)
- All patients go through same process regardless of acuity
- Bed assignment waits until bed clean (sequential, not parallel)

**Material:**
- Bed turnover delayed waiting for cleaning supplies
- Linen delays

**Measurement:**
- No real-time visibility to bed status (staff check manually)
- No predictive model for arrival surges

**Environment:**
- Arrival pattern has shifted (more afternoon arrivals) but staffing hasn't adapted

**Root Cause Summary:**
1. **Primary:** Arrival surge 2pm-6pm exceeds momentary capacity, creating backlog
2. **Contributing:** Long LOS (187 min vs. 151 benchmark) reduces throughput
3. **Contributing:** Slow bed turnover (32 min vs. 18 benchmark) reduces effective capacity
4. **Contributing:** No fast-track for less-urgent patients (all compete for same beds)
5. **Contributing:** Triage bottleneck (25 min wait for triage)

**Expected Defect Rate:**
- % of patients waiting >90 minutes: 68%
- % of patients waiting >120 minutes: 42%
- % leaving without being seen (LWBS): 7% (875 patients/year)

### IMPROVE Phase

**Solution Brainstorming (Impact vs. Effort Matrix):**

**Quick Wins (High Impact, Low Effort):**

**Solution 1: Dedicated Triage Nurse (2pm-10pm)**
- **Action:** Assign one nurse exclusively to triage during peak hours
- **Impact:** Reduce triage wait from 25 min to 5 min (20-minute savings)
- **Cost:** $90,000/year (1 FTE nurse)
- **Timeline:** 2 weeks to implement

**Solution 2: Parallel Bed Turnover**
- **Action:** Notify Environmental Services (EVS) when discharge process starts (not after patient leaves)
- **Impact:** Reduce bed turnover from 32 min to 20 min (12-minute savings)
- **Cost:** Process change only (no cost)
- **Timeline:** Immediate

**Solution 3: Fast-Track Area**
- **Action:** Designate 6 beds for ESI 4-5 (less urgent) with dedicated NP/PA
- **Impact:** Remove 40% of patients from main ED flow (reduce competition for beds)
- **Cost:** $120,000/year (1 FTE Nurse Practitioner)
- **Timeline:** 4 weeks to implement

**Major Projects (High Impact, High Effort):**

**Solution 4: Dynamic Staffing Model**
- **Action:** Shift nursing hours to match arrival patterns (more staff 2pm-10pm)
- **Impact:** Better match capacity to demand during surge
- **Cost:** Reallocation (potentially neutral), some overtime initially
- **Timeline:** 2 months (schedule changes, union negotiation)

**Solution 5: Reduce Length of Stay**
- **Action:** Standardize discharge process, order labs/tests concurrently (not sequentially)
- **Impact:** Reduce LOS from 187 min to 160 min (15% improvement)
- **Cost:** Process redesign (no cost)
- **Timeline:** 6 weeks (requires physician buy-in, protocol development)

**Selected Implementation Plan (90-day project):**

**Week 1-2: Quick Wins**
- Implement parallel bed turnover (immediate)
- Hire dedicated triage nurse (2pm-10pm shift)

**Week 3-6: Fast-Track**
- Hire Nurse Practitioner
- Designate and equip fast-track area
- Train staff on fast-track protocols

**Week 7-12: Staffing & LOS**
- Implement dynamic staffing model
- Launch LOS reduction initiative

**Expected Results:**

| Improvement | Time Savings | Cumulative Wait Time |
|-------------|--------------|---------------------|
| Baseline | - | 118 min |
| Dedicated triage nurse | -20 min | 98 min |
| Parallel bed turnover | -12 min | 86 min |
| Fast-track (for ESI 4-5) | -25 min | 61 min |
| Dynamic staffing | -10 min | 51 min |
| **Total Improvement** | **-67 min** | **51 minutes** |

**Goal: 60 minutes. Projected: 51 minutes. GOAL EXCEEDED.**

### CONTROL Phase

**Control Plan:**

**What to Monitor:**
1. Average wait time (door-to-provider) - Target: <60 min
2. 95th percentile wait time - Target: <120 min
3. % of patients waiting >90 min - Target: <20%
4. Left without being seen (LWBS) rate - Target: <3%
5. Patient satisfaction (wait time question) - Target: >4.0/5.0

**How to Monitor:**
- Real-time dashboard (updated hourly)
- Daily review by ED charge nurse
- Weekly review in ED leadership meeting
- Monthly control charts

**Control Charts:**
- X-bar and R chart for average wait times (by day)
- P-chart for % exceeding 90 minutes
- Run chart for LWBS rate

**Response Plan:**

If average wait time >70 minutes for 3 consecutive days:
1. Review staffing schedules (is dedicated triage nurse working?)
2. Check fast-track utilization (are ESI 4-5 being routed correctly?)
3. Audit bed turnover process (is parallel process being followed?)
4. Escalate to ED Director if issue persists >1 week

**Standard Work:**
- Triage nurse role and responsibilities documented
- Fast-track protocols and ESI routing criteria documented
- Bed turnover parallel process documented with visual guide
- Staff training completed and competency verified

**Audit Schedule:**
- Weekly: Spot-check compliance with new processes
- Monthly: Full audit of all control plan elements
- Quarterly: Review and update control plan based on data

### Questions for Analysis

**Question 1: Problem Definition (10 points)**
- Evaluate the problem statement and goal statement. Are they SMART? What strengths/weaknesses do you see?
- The goal is 60 minutes, but the hospital's published commitment is <90 minutes. Should the goal be 60 or 90? Justify your answer.

**Question 2: Measurement & Capability (20 points)**
- The current Cpk is -0.14 (using USL = 90 minutes). What does a negative Cpk mean? Interpret this result.
- After improvements, the projected mean is 51 minutes with SD = 35 minutes. Calculate the new Cpk. Is the process capable?
- The data shows high standard deviation (67 minutes). What does this tell you about process consistency? Why is reducing variation (SD) as important as reducing the mean?

**Question 3: Root Cause Analysis (25 points)**
- The team tested 5 hypotheses. Evaluate their hypothesis testing approach. Was it data-driven? Were the tests valid?
- Hypothesis 1 concluded "not a capacity problem" because average utilization is 67%. Do you agree? Why does the afternoon still have capacity problems if overall capacity is adequate? (Hint: Consider queuing theory and variation)
- Which of the 5 root causes is most critical? If you could only address one, which would you choose and why?

**Question 4: Solution Selection & Implementation (25 points)**
- Evaluate the solutions using Impact vs. Effort matrix. Do you agree with the "quick wins" selections? Would you prioritize differently?
- The fast-track solution costs $120,000/year. Calculate the ROI:
  - Current revenue loss: $340K (quality penalties) + volume loss
  - Assume 12% volume decline = 5,400 patients/year lost
  - Average revenue per ED visit: $850
  - Total revenue loss from volume: 5,400 × $850 = $4,590,000
  - If fast-track helps recover volume, what's the ROI?
- What risks or challenges might the team encounter during implementation? How should they mitigate these risks?

**Question 5: Control & Sustainability (20 points)**
- The control plan includes 5 metrics to monitor. Are these the right metrics? Would you add or change any?
- Why are control charts necessary? What would an X-bar and R chart show that a simple average wouldn't?
- What are the biggest risks to sustainability after the project ends? Design a sustainability plan to address these risks.

---

### Answer Key & Teaching Notes (Available for Instructors)

*[Answer key follows same comprehensive format with detailed solutions, teaching points, common student errors, and instructor guidance]*

---

## Case Study 5: TechServ's Customer Support Process Breakdown

**Course:** Course 5 - Process Mapping & Problem-Solving Essentials  
**Length:** 1,100 words  
**Difficulty:** Beginner  
**Estimated Analysis Time:** 60-75 minutes

### Background

TechServ Solutions provides IT helpdesk support for 150 small-business clients across the Midwest. Their 25-person support team handles approximately 400 support tickets per day via phone, email, and chat.

Six months ago, TechServ lost their two largest clients (20% of revenue) who cited "unacceptable response times and inconsistent service quality." The company's Net Promoter Score (NPS) has plummeted from +45 to +12.

The new VP of Operations, Rachel Kim, suspects process problems. She hears complaints:
- **From customers:** "I submitted a ticket 3 days ago and haven't heard anything."
- **From support team:** "We're drowning in tickets. No time to actually solve problems."
- **From team leads:** "Every tech has their own system. Total chaos."

Rachel decides to map the current support process to identify problems.

### Current State Process Mapping

Rachel and her team followed 10 support tickets from submission to resolution, documenting every step. Here's the **current state process map** they created:

**Process: Customer Support Ticket Resolution**

| Step # | Activity | Owner | Time (avg) | Handoffs | Value-Added? | Notes |
|--------|----------|-------|-----------|----------|--------------|-------|
| 1 | Customer submits ticket via email/phone/chat | Customer | 5 min | → | No (NVA-N) | 3 different intake channels |
| 2 | Ticket sits in general inbox | - | 2.5 hours | → | No (Waste) | No one monitors continuously |
| 3 | Tier 1 tech checks inbox, reads ticket | Tier 1 | 3 min | → | No (NVA-N) | Manual checking every few hours |
| 4 | Tech manually logs ticket in system | Tier 1 | 8 min | → | No (Waste) | Re-typing email content |
| 5 | Tech determines priority (guess) | Tier 1 | 2 min | → | No (Waste) | No clear priority criteria |
| 6 | If complex: Tech escalates to Tier 2 | Tier 1 | 5 min | → Tier 2 | No (NVA-N) | 60% of tickets escalated |
| 7 | Escalated ticket sits in Tier 2 queue | - | 4.5 hours | → | No (Waste) | Tier 2 queue not prioritized |
| 8 | Tier 2 tech reads ticket | Tier 2 | 3 min | → | No (NVA-N) | Re-reading what Tier 1 already read |
| 9 | Tier 2 researches solution | Tier 2 | 25 min | → | YES | Actual problem-solving |
| 10 | If needs more info: Email customer | Tier 2 | 5 min | → Customer | No (Waste) | Info should have been gathered initially |
| 11 | Wait for customer response | - | 18 hours | ← Customer | No (Waste) | Customer delays |
| 12 | Read customer response | Tier 2 | 2 min | → | No (NVA-N) | |
| 13 | Continue research/testing | Tier 2 | 30 min | → | YES | Actual problem-solving |
| 14 | Document solution in system | Tier 2 | 10 min | → | No (NVA-N) | Documentation |
| 15 | Email solution to customer | Tier 2 | 5 min | → Customer | YES | Delivering solution |
| 16 | Update ticket status to "resolved" | Tier 2 | 2 min | → | No (NVA-N) | Administrative |
| 17 | Customer tests solution | Customer | varies | | | |
| 18 | If doesn't work: Ticket reopened | Customer | - | → Tier 1 | No (Waste) | 30% reopen rate |
| 19 | Repeat steps 2-16 | | | | | Rework loop |

**Current State Metrics:**

**Total Cycle Time:**
- Simple tickets (40%): Average 8 hours
- Complex tickets (60%): Average 32 hours
- **Weighted Average: 22 hours** (3 business days)

**Time Breakdown:**
- Value-added time (actual problem-solving): 60 minutes (4.5%)
- Non-value-added but necessary: 40 minutes (3%)
- Waiting time: 25 hours (93%)
- Transportation/handoffs: 5 minutes (0.5%)

**Process Efficiency: 4.5%** (VA time / Total time)

**Other Metrics:**
- First Response Time: 4.8 hours average (target: <1 hour)
- Resolution Time: 22 hours average (target: <8 hours)
- Reopen Rate: 30% (target: <10%)
- Customer Satisfaction (CSAT): 3.1/5.0 (target: 4.5/5.0)
- Cost per Ticket: $45 (labor + overhead)
- Technician Utilization: 62% (spending 38% on non-value-added work)

**Handoff Analysis:**
- Number of handoffs: 6 (Customer → Tier 1 → System → Tier 2 → Customer → Tier 2 → Customer)
- Average delay per handoff: 4.2 hours
- Total handoff delay: 25 hours

### Problem Identification

Using the DOWNTIME framework, Rachel identified wastes:

**Defects:**
- 30% of tickets reopened (solution didn't work)
- 15% of tickets have wrong priority assigned
- Information gathering incomplete (40% require follow-up questions)

**Overproduction:**
- Detailed documentation created for simple tickets (only needed for complex)
- Tier 1 techs gathering information that Tier 2 never uses

**Waiting:**
- 2.5 hours in initial queue
- 4.5 hours in escalation queue
- 18 hours waiting for customer responses (many could be avoided)

**Non-Utilized Talent:**
- Tier 1 techs (capable of solving 60% of tickets) only solving 40%
- Tier 2 techs spending 40% of time on admin work (could be automated)

**Transportation:**
- Manual ticket transfer between systems
- Email-based communication requires copying/pasting

**Inventory:**
- 320 tickets in backlog (0.8 days of work queued)
- 80% of backlog is waiting, not being actively worked

**Motion:**
- Techs checking 3 different inboxes (email, phone log, chat)
- Switching between 4 different systems to resolve one ticket

**Excess Processing:**
- Re-reading tickets multiple times (Tier 1 reads, Tier 2 reads again)
- Re-typing ticket information from email into system
- Creating detailed documentation for simple password resets

### Root Cause Analysis

Rachel facilitated a fishbone diagram session:

**Problem:** Long resolution time (22 hours vs. 8-hour target)

**Key Causes Identified:**

**Method (Process):**
- No triage system to prioritize tickets
- 60% unnecessary escalation (Tier 1 could solve with training)
- No standard workflow or template

**Man (People):**
- Tier 1 technicians under-trained (60% escalation rate)
- No clear escalation criteria
- Knowledge not shared (each tech has own solutions)

**Machine (Systems/Technology):**
- 3 separate intake channels (email, phone, chat) not integrated
- Manual ticket entry (no automation)
- No knowledge base system
- 4 different systems to check (ticket system, knowledge base, client database, asset management)

**Measurement:**
- No real-time visibility into queue depth
- No SLA tracking or alerts
- Ticket priority based on "gut feel"

**Material (Information):**
- Initial ticket often missing key information
- No standard information collection template
- Customer responses delayed (no urgency communicated)

**5 Whys on High Escalation Rate:**

**Problem:** 60% of tickets escalated to Tier 2 (should be ~30%)

- **Why?** → Tier 1 techs escalate tickets they could solve
  - **Why?** → Tier 1 techs lack knowledge/confidence
    - **Why?** → Limited training and no knowledge base to reference
      - **Why?** → No systematic knowledge capture from Tier 2 solutions
        - **Why?** → No knowledge management system or process
          - **ROOT CAUSE:** Knowledge exists in individual Tier 2 tech heads, not documented/shared, so Tier 1 can't access

### The Proposed Future State

Rachel's team brainstormed improvements and designed a **future state process map**:

**Proposed Improvements:**

**1. Unified Ticketing System**
- Integrate email/phone/chat into one system
- Auto-create tickets (eliminate manual entry)
- **Impact:** Eliminate Steps 4 (8 min), reduce Step 2 (real-time intake)

**2. Automated Triage & Priority**
- Algorithm assigns priority based on keywords, client SLA, issue type
- Color-coded queue (red = urgent, yellow = medium, green = routine)
- **Impact:** Eliminate Step 5 (2 min), improve prioritization accuracy

**3. Tier 1 Empowerment**
- Knowledge base with solutions for top 50 issues (covers 70% of tickets)
- Expanded Tier 1 training
- Clear escalation criteria checklist
- **Impact:** Reduce escalation from 60% → 30%, eliminate Steps 6-8 for 30% of tickets

**4. Structured Information Gathering**
- Template for ticket submission (required fields)
- Chatbot pre-screens and gathers info before human involvement
- **Impact:** Eliminate Step 10-11 (23 hours) for 40% of tickets

**5. Parallel Processing**
- Tier 2 can pull from queue while Tier 1 still working (don't wait for complete handoff)
- Customer can provide additional info while tech researches (don't wait sequentially)
- **Impact:** Reduce waiting time by 40%

**Future State Projected Metrics:**

| Metric | Current | Future State | Improvement |
|--------|---------|--------------|-------------|
| Average Cycle Time | 22 hours | 6 hours | -73% |
| First Response Time | 4.8 hours | 0.5 hours | -90% |
| Escalation Rate | 60% | 30% | -50% |
| Reopen Rate | 30% | 12% | -60% |
| Process Efficiency | 4.5% | 25% | +450% |
| Cost per Ticket | $45 | $25 | -44% |
| Technician Utilization | 62% | 85% | +37% |

**Annual Savings Calculation:**

Current state:
- 400 tickets/day × 250 days = 100,000 tickets/year
- Current cost: $45/ticket × 100,000 = $4,500,000/year

Future state:
- Same volume: 100,000 tickets/year
- Future cost: $25/ticket × 100,000 = $2,500,000/year
- **Annual savings: $2,000,000**

**Plus:**
- Improved customer satisfaction → reduce customer churn
- Higher tech utilization → can handle growth without adding headcount
- Faster resolution → higher NPS → customer referrals

**ROI:**
- Implementation cost: $250,000 (system integration, training, knowledge base development)
- Annual savings: $2,000,000
- Payback period: 1.5 months
- 5-year NPV: $9.75 million

### Questions for Analysis

**Question 1: Current State Process Analysis (25 points)**
- Calculate the value-added percentage for both simple and complex tickets separately. Why is it so low?
- Identify all handoffs in the current process. Which handoffs add value? Which could be eliminated? Which cause the longest delays?
- The current process efficiency is 4.5%. What does this mean in practical terms? What is being done during the other 95.5% of time?

**Question 2: Waste Identification (20 points)**
- Using the 8 wastes (DOWNTIME), identify and quantify the biggest waste in this process. Support your answer with data from the case.
- The reopen rate is 30%. What waste does this represent? What's the cost? (Calculate annual rework cost)
- How much time is spent waiting vs. actual work? What does this tell you about the process design?

**Question 3: Root Cause Analysis & Prioritization (25 points)**
- Evaluate the 5 Whys analysis. Did the team reach an actionable root cause? Would you continue further?
- Create a Pareto analysis of the time spent in each process step. Which steps represent 80% of the total time? These are your improvement priorities.
- Of all the root causes identified (in the fishbone), which 2-3 should be addressed first? Justify your prioritization using impact and feasibility.

**Question 4: Future State Design & Impact (20 points)**
- Evaluate the 5 proposed improvements. Are they addressing root causes or symptoms?
- The projected cycle time reduction is from 22 hours to 6 hours (73%). Is this realistic? Justify your answer by calculating time savings from each improvement.
- Design a swimlane diagram for the future state process (show different roles/systems). How many handoffs are eliminated?

**Question 5: Implementation & ROI (10 points)**
- The ROI shows $250K investment for $2M annual savings. Calculate the 5-year ROI percentage.
- What risks might prevent achieving the projected savings? How would you mitigate these risks?
- Create a 90-day implementation plan. What should be implemented first? What are the dependencies?

---

### Answer Key & Teaching Notes (Available for Instructors)

*[Answer key follows same comprehensive format with detailed solutions, calculations, teaching points, common errors, and instructor guidance]*

---

## Case Study Usage Guidelines

### For Instructors

**Timing:**
- Case studies work best after students complete course content and exercises
- Assign 3-5 days before class discussion
- Allow 60-90 minutes for written analysis
- Budget 45-60 minutes for in-class discussion

**Facilitation Approach:**
- Start with open-ended question: "What's the real problem here?"
- Use Socratic method to guide students to discover insights
- Encourage debate and multiple perspectives
- Connect case to course principles explicitly
- Share real-world similar scenarios

**Assessment Options:**
- Individual written analysis (comprehensive)
- Group presentation (collaborative learning)
- In-class discussion participation (engagement)
- Case exam (test understanding under time pressure)

**Learning Objectives:**
- Apply course concepts to realistic business scenarios
- Develop analytical and problem-solving skills
- Practice data-driven decision making
- Understand complexity and interconnectedness of real problems
- Build confidence in recommendation development

### For Students

**How to Analyze:**
1. Read case thoroughly (twice)
2. Highlight key data and facts
3. Identify the core problem (not symptoms)
4. Apply relevant course frameworks/tools
5. Support recommendations with data
6. Consider implementation challenges
7. Write clearly and professionally

**Common Mistakes to Avoid:**
- Jumping to solutions without analysis
- Ignoring data provided in case
- Solving symptoms instead of root causes
- Making unsupported assumptions
- Forgetting implementation feasibility
- Neglecting cost/benefit analysis

---

**End of Foundation Belt Case Studies**

*These case studies are designed to develop critical thinking, analytical reasoning, and practical application skills. Students should demonstrate mastery of course concepts while grappling with the complexity and ambiguity of real-world business problems.*
