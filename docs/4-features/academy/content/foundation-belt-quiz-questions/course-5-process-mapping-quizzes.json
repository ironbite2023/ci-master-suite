[
  {
    "module": "5.1",
    "module_name": "Process Mapping Basics",
    "question": "What is the primary purpose of creating a process map in continuous improvement?",
    "type": "multiple_choice",
    "options": [
      "To visualize the current state of a process, making waste, bottlenecks, and improvement opportunities visible to everyone on the team",
      "To create documentation for ISO certification only",
      "To assign blame for process problems",
      "To replace the need for data collection"
    ],
    "correct_answer": 0,
    "explanation": "Process mapping is about making the invisible visible. Most processes exist in people's heads with different assumptions about how things work. A process map creates a shared visual representation that reveals: (1) Waste—non-value-added steps consuming time/resources (transportation, waiting, inspection), (2) Bottlenecks—where work piles up causing delays (one person handling 5 different tasks), (3) Redundancy—duplicate steps or rework loops, (4) Variation—different people doing the same work differently, (5) Disconnects—handoffs where information gets lost. Real example: Healthcare team maps patient discharge process. Discovers: 7 handoffs between departments, 4 hours average waiting time for pharmacy, 3 forms with duplicate information, discharge coordinator unaware of delays upstream. The map makes problems obvious that were previously hidden in complexity. Without the map, everyone blamed 'busy-ness'—with the map, they see systemic issues to fix. Why process mapping works: Visual communication transcends language barriers, shows relationships between steps, reveals cumulative impact of small delays, identifies improvement priorities, creates baseline for measuring improvement. The map is not the goal—improvement is. The map is the tool that makes improvement possible. Option B is wrong—documentation is a side benefit, not primary purpose. Option C is wrong—focus is on process, not people/blame. Option D is wrong—maps complement data, don't replace it.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.1.1",
    "estimated_time_seconds": 85
  },
  {
    "module": "5.1",
    "module_name": "Process Mapping Basics",
    "question": "What is the difference between a high-level process map (SIPOC) and a detailed process map?",
    "type": "multiple_choice",
    "options": [
      "High-level maps show 5-7 major steps to understand scope and boundaries; detailed maps show 30-50 steps to identify specific improvement opportunities",
      "High-level maps are for executives, detailed maps are for workers—they serve different audiences only",
      "There is no difference—all process maps should show every detail",
      "Detailed maps are always better and should be created first"
    ],
    "correct_answer": 0,
    "explanation": "Different levels of detail serve different purposes—use the right tool for the right stage. High-level maps (SIPOC, value stream maps at 30,000 feet): Show 5-7 major process steps (Receive Order → Process Payment → Pick Items → Pack → Ship). Purpose: understand scope, identify boundaries, see big picture, align stakeholders on what's included. When to use: Define phase, new team starting project, senior leadership review, identifying which area to investigate deeper. Detailed maps (flowcharts, swimlane diagrams): Show 30-50+ detailed steps (Receive order → Validate customer info → Check inventory system → If available then reserve items, else notify customer → Generate pick list → Print pick list → Walk to warehouse → Locate first item → etc.). Purpose: identify specific waste, find root causes, design improvements, create standard work. When to use: Analyze phase, process deep-dive, training new employees, implementing improvements. The sequence matters: Start high-level (big picture, scope) → Then go detailed (specific problems). Going detailed first risks: Getting lost in weeds before understanding context, mapping wrong process or wrong boundaries, overwhelming stakeholders with complexity, wasting time on irrelevant details. Example: Manufacturing improvement. High-level map shows: Raw Materials → Cutting → Assembly → Inspection → Packaging → Shipping (6 steps). Identifies bottleneck: Assembly. Now create detailed map of just Assembly (30 steps) to find root cause. Don't detail everything—focus where problems are. Option B is wrong—audience is one factor, but purpose and timing matter more. Option C is wrong—different levels serve different needs. Option D is wrong—always start high-level to understand context.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.1.2",
    "estimated_time_seconds": 85
  },
  {
    "module": "5.1",
    "module_name": "Process Mapping Basics",
    "question": "When creating a process map, why is it important to map what actually happens rather than what should happen?",
    "type": "multiple_choice",
    "options": [
      "Because the gap between actual and ideal reveals where problems occur—mapping 'should' hides waste, workarounds, and root causes that need addressing",
      "Because 'should' maps are always more accurate",
      "Because procedures are never followed correctly",
      "Because documenting the ideal state is a waste of time"
    ],
    "correct_answer": 0,
    "explanation": "The most valuable insights come from understanding reality, not theory. 'Should' maps show the procedure manual. 'Actual' maps show what really happens. The gap between them reveals opportunities. Example: Order processing. Should map (from procedure manual): Receive order → Enter in system → Print pick list → Pick items → Pack → Ship (6 steps, 30 minutes). Actual map (by observing Gemba): Receive order → Wait for supervisor approval → System crashes, write on paper → Wait for system backup → Re-enter in system → Print pick list → Walk to warehouse → Pick list printer jammed, return to office → Print again → Walk back to warehouse → First item out of stock, call purchasing → Wait for answer → Substitute different item → Pick remaining items → Return to packing area → Packing materials low, search stockroom → Pack → Generate shipping label → Label printer offline → Troubleshoot printer → Ship (20+ steps, 2 hours). The 'actual' map reveals: 6 waiting steps (1 hour cumulative), 3 system/equipment failures causing rework, 2 workarounds not in procedure, stockout and material shortage issues. The 'should' map hides all of this! If you only map 'should,' you'll design improvements for a process that doesn't exist. Why actual ≠ should: Equipment breaks down, materials run short, systems have quirks, workarounds develop over time, procedures outdated, variation between operators, unwritten tribal knowledge, exception handling not documented. How to map actual: Go to Gemba (where work happens), observe don't interview (people describe 'should' when asked), follow multiple cycles (catch variation), watch different operators, ask 'what happens when...' for exceptions, capture workarounds (they reveal problems). Then compare actual vs should to find improvement opportunities. Option B is wrong—'should' maps miss reality. Option C is wrong—not about blame, about understanding gaps. Option D is wrong—ideal state is eventual goal, but start with actual state.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.1.3",
    "estimated_time_seconds": 90
  },
  {
    "module": "5.1",
    "module_name": "Process Mapping Basics",
    "question": "What does a swimlane diagram add to a standard process flowchart?",
    "type": "multiple_choice",
    "options": [
      "Swimlanes show which person or department is responsible for each step, revealing handoffs, delays, and accountability gaps in cross-functional processes",
      "Swimlanes make the diagram prettier but add no useful information",
      "Swimlanes are only used for swimming pool maintenance processes",
      "Swimlanes replace the need to show decision points"
    ],
    "correct_answer": 0,
    "explanation": "Swimlane diagrams (also called cross-functional flowcharts) add a critical dimension: WHO does each step. This reveals problems invisible in standard flowcharts. Standard flowchart shows: Step A → Step B → Step C → Step D (shows sequence). Swimlane diagram shows: Customer lane: Step A → wait. Sales lane: wait → Step B → wait. Operations lane: wait → wait → Step C → wait. Shipping lane: wait → wait → wait → Step D. Now you see the problems: handoffs between every step (information loss risk), waiting time between handoffs (2 hours total), work piles up in Sales lane (bottleneck), no one responsible for end-to-end process (accountability gap). Why swimlanes reveal waste: Handoffs are major waste sources—every time work moves between people/departments: Information gets lost or misinterpreted (90% of errors occur at handoffs), delays occur while waiting for next person, work sits in queues between steps, no one owns the complete process, finger-pointing when problems occur. Swimlanes make handoffs visible—each lane change = a handoff. Count the handoffs, measure delay at each, investigate why handoffs needed. Example: Expense reimbursement process. 7 swimlanes (Employee, Manager, Accounting, AP Clerk, AP Manager, Finance Director, Accounts Payable System). 15 handoffs. Average 2 days waiting per handoff = 30 days total cycle time for a 10-minute task! Swimlane diagram makes this absurdity visible and obvious. Improvement opportunity: Reduce handoffs (empower employees, eliminate non-value approvals), automate handoffs (system-to-system), co-locate teams (reduce delay), create process owner role (end-to-end accountability). Use swimlanes when: Process crosses multiple departments, responsibilities unclear, handoff delays suspected, finger-pointing occurs, 'not my job' excuses common. Option B is wrong—swimlanes add critical WHO information. Option C is wrong—'swimlane' is a metaphor (like swimming lanes). Option D is wrong—swimlanes complement decision points, don't replace them.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.1.4",
    "estimated_time_seconds": 90
  },
  {
    "module": "5.1",
    "module_name": "Process Mapping Basics",
    "question": "A team creates a process map and discovers 15 non-value-added steps. What should they do next?",
    "type": "multiple_choice",
    "options": [
      "Prioritize which non-value-added steps to eliminate first based on impact and ease—focus on quick wins and major bottlenecks before tackling everything",
      "Eliminate all 15 steps immediately without further analysis",
      "Keep all 15 steps because they're part of the current process",
      "Create more detailed maps of each step before taking any action"
    ],
    "correct_answer": 0,
    "explanation": "Discovery is just the beginning—smart prioritization drives results. Finding 15 non-value-added steps is great, but trying to fix everything at once leads to: Analysis paralysis, overwhelmed team, scattered resources, limited results, lost momentum. Better approach—prioritize using impact vs. effort matrix: High Impact, Low Effort (Quick Wins—do first): Step 3: Duplicate data entry (15 min/order, easy to eliminate). Step 7: Unnecessary approval by manager who always approves (5 min, just remove). Step 12: Walking documents to another building (30 min, use email instead). Action: Implement these within 1-2 weeks. High Impact, High Effort (Major Projects—plan carefully): Step 5: Manual inventory check (1 hour, requires system integration). Step 10: Waiting for batch processing (4 hours, need real-time system). Action: Create project plan, pilot, then scale. Low Impact, Low Effort (Fill-ins—do when time permits): Step 8: Extra copy of form (1 min, minor savings). Step 14: Redundant email notification (30 sec). Action: Bundle with other improvements. Low Impact, High Effort (Avoid—not worth it): Step 15: Reformatting data for legacy system (5 min, requires replacing entire system). Action: Live with it or include in larger system upgrade. Prioritization factors: Impact: Time saved per cycle × frequency = total savings. 15 min/order × 100 orders/day = 1,500 min/day (25 hours). Ease: Can we do it ourselves? Need approval? Requires budget? How long? Risk: Will it cause other problems? Need to test first? Dependencies: Must we fix Step A before Step B? Data availability: Do we know root cause or need more analysis? Example: Team prioritizes: Week 1: Eliminate 3 quick wins (save 50 min/order). Month 1: Pilot automated inventory check on one product line. Quarter 1: Scale inventory automation, implement real-time processing. Result: 70% of savings achieved in Week 1, momentum built, team energized, stakeholders see results, harder projects now have support. Option B is wrong—hasty elimination causes unintended consequences. Option C is wrong—non-value-added steps should be eliminated/minimized. Option D is wrong—more analysis delays action on obvious improvements.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.1.5",
    "estimated_time_seconds": 95
  },
  {
    "module": "5.2",
    "module_name": "Problem-Solving Tools",
    "question": "What is the primary advantage of using a structured problem-solving approach (like A3 thinking or 8D) instead of jumping to solutions?",
    "type": "multiple_choice",
    "options": [
      "Structured approaches ensure root causes are identified and verified before implementing solutions, preventing wasted effort on fixes that don't address the real problem",
      "Structured approaches are slower, making them less effective than quick solutions",
      "Structured approaches are only useful for complex problems, not simple ones",
      "Structured approaches eliminate the need for team involvement"
    ],
    "correct_answer": 0,
    "explanation": "Structured problem-solving prevents the most common mistake: solving the wrong problem. Unstructured approach (jump to solution): See high defect rate → assume equipment old → buy new equipment ($500K) → defects persist → wasted money, problem unsolved. Structured approach (A3 thinking example): Step 1 Define problem: Defect rate increased from 3% to 15% in Product Line A over 3 months, costing $500K annually. Step 2 Analyze current state: Collect data, stratify by shift/operator/material, measure capability. Find: 80% of defects on night shift. Step 3 Identify root causes: Investigate night shift. Discover: Temperature variation (no monitoring at night). Step 4 Verify root cause: Test temperature control vs defects. Confirm correlation (r = 0.89). Step 5 Develop solutions: Install automated temperature control, train operators. Step 6 Implement: Pilot on one line, validate results, scale. Step 7 Monitor: Control plan, track defect rate over time. Result: Defects drop to 2%, $50K investment vs $500K, problem solved. Why structure matters: Forces you to understand before acting (no jumping to conclusions), requires data verification (no guessing), identifies root causes not symptoms (prevents recurrence), tests solutions before full implementation (reduces risk), establishes monitoring (ensures sustainability), documents learning (helps future problem-solvers). Common structured methods: A3 Thinking (one-page story: background, current state, root cause, solutions, plan), 8D (8 disciplines: team, problem, containment, root cause, solutions, implementation, prevention, recognition), DMAIC (Define-Measure-Analyze-Improve-Control for Six Sigma projects), PDCA (Plan-Do-Check-Act for iterative improvement). When to use structured approach: Problem is recurring (quick fixes haven't worked), stakes are high (expensive to get wrong), causes unclear (need investigation), multiple stakeholders involved, solution requires investment, need to prove results. When simple approach okay: Problem is obvious and simple, fix is cheap and reversible, impact is minor, urgency overrides analysis. But even then, verify the fix worked! Option B is wrong—structure saves time by preventing rework. Option C is wrong—structure helps all problems, though depth varies. Option D is wrong—structure enhances team collaboration.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.2.1",
    "estimated_time_seconds": 90
  },
  {
    "module": "5.2",
    "module_name": "Problem-Solving Tools",
    "question": "A team is brainstorming potential solutions. They generate 20 ideas. What should they do before implementing?",
    "type": "multiple_choice",
    "options": [
      "Evaluate solutions against criteria (impact, feasibility, cost, risk) and select the best 2-3 to pilot test—not all ideas are equally valuable",
      "Implement all 20 ideas immediately to maximize improvement",
      "Pick the first idea suggested and implement it",
      "Discard all ideas and start brainstorming again"
    ],
    "correct_answer": 0,
    "explanation": "Brainstorming generates quantity—evaluation generates quality. Having 20 ideas is great, but they're not all equally good. Some will be high-impact and easy, others low-impact and difficult. Evaluation process: Step 1 Define evaluation criteria. Impact: How much will this reduce defects/time/cost? Feasibility: Can we actually do this with current resources? Cost: What's the investment required? Risk: What could go wrong? Will it cause other problems? Time: How long to implement? Alignment: Does it fit our strategy and culture? Step 2 Score each solution. Use simple 1-5 scale for each criterion. Solution A: Impact=5, Feasibility=4, Cost=2 (low cost=good), Risk=2 (low risk=good), Time=1 (fast=good). Total score = High. Solution B: Impact=2, Feasibility=1, Cost=5, Risk=5, Time=5. Total score = Low (don't pursue). Step 3 Plot on Impact vs Effort matrix. High Impact, Low Effort = Quick Wins (do first). High Impact, High Effort = Major Projects (plan carefully). Low Impact, Low Effort = Fill-ins (do when time permits). Low Impact, High Effort = Avoid (not worth it). Step 4 Select top 2-3 solutions to pilot. Why not all 20? Limited resources (can't do everything), need to learn (pilot reveals issues), some ideas won't work (better to discover on small scale), some ideas conflict (can't do both). Why not just 1? Multiple pilots hedges risk (if one fails, others might work), comparison shows which works best, team engagement (more people involved). Example evaluation: Problem: Order processing takes 2 hours. 20 solutions generated. Evaluate: Solution 3 (eliminate duplicate data entry): Impact HIGH (saves 15 min/order), Feasibility HIGH (just change procedure), Cost LOW ($0), Risk LOW (easy to reverse), Time FAST (1 week). Score: 9/10. Solution 11 (buy new ERP system): Impact HIGH (saves 30 min/order), Feasibility LOW (requires approval, IT, training), Cost HIGH ($2M), Risk HIGH (implementation risk), Time SLOW (2 years). Score: 3/10. Select: Solution 3 for immediate pilot. Solution 11 for long-term consideration if Solution 3 insufficient. After pilot testing top solutions: Measure results, refine based on learning, scale what works, abandon what doesn't, document lessons learned. Option B is wrong—implementing everything wastes resources on low-value ideas. Option C is wrong—first idea often not best, evaluation needed. Option D is wrong—20 ideas is great, now evaluate them.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.2.2",
    "estimated_time_seconds": 90
  },
  {
    "module": "5.2",
    "module_name": "Problem-Solving Tools",
    "question": "What is the purpose of a Pareto chart in problem-solving?",
    "type": "multiple_choice",
    "options": [
      "To identify the vital few causes that create most of the problems (80/20 rule), helping teams focus improvement efforts on the highest-impact issues",
      "To show the timeline of when problems occurred",
      "To replace the need for root cause analysis",
      "To prove that all causes are equally important"
    ],
    "correct_answer": 0,
    "explanation": "Pareto charts operationalize the 80/20 rule: 80% of problems come from 20% of causes. Focus on the vital few, ignore the trivial many. What a Pareto chart shows: X-axis: Categories of problems/causes (Defect Type A, Type B, Type C, etc.). Y-axis (left): Frequency or cost of each category (bar chart). Y-axis (right): Cumulative percentage (line graph). The insight: First 2-3 bars often represent 70-80% of total problems. Example—Customer complaints analysis: Created Pareto chart of complaint types. Results: Billing errors (40%), Late delivery (30%), Product defects (15%), Poor communication (10%), Other (5%). Cumulative: First two categories = 70% of complaints. Decision: Focus improvement efforts on billing and delivery. Ignore 'other' categories for now. Result: Reduced complaints 60% by fixing just 2 of 5 categories. Without Pareto thinking, team might have: Spent equal time on all categories (diluted effort), tackled easiest not most important (felt busy, minimal impact), addressed newest not biggest problems (recency bias), listened to loudest voices not data (politics). How to create Pareto chart: Step 1 Collect data on problem categories (defect types, complaint reasons, delay causes). Step 2 Count frequency or sum cost for each category. Step 3 Sort categories from highest to lowest. Step 4 Calculate cumulative percentage. Step 5 Create bar chart (descending) with cumulative line. Step 6 Identify vital few (usually where cumulative reaches 80%). When Pareto analysis powerful: Multiple problem types, limited resources, unclear priorities, need to convince stakeholders where to focus, establishing baseline for before/after comparison. Limitations to be aware of: Pareto shows 'what' not 'why'—still need root cause analysis of vital few, categories must be meaningful (bad categories = bad insights), frequency ≠ always importance (one rare catastrophic failure might be more critical than many minor annoyances), Pareto can change over time (recheck after improvements). The process: Use Pareto to identify vital few → Apply root cause analysis to those few → Implement improvements → Create new Pareto to verify shift → Focus on new vital few. Example: Manufacturing defects. Initial Pareto: Scratches (50%), Dents (30%), Cracks (15%), Other (5%). Fix scratches → New Pareto: Dents (50%), Cracks (30%), Discoloration (15%), Scratches (5%). Continuous focus on vital few drives continuous improvement. Option B is wrong—Pareto shows categories, not timeline (use run chart for timeline). Option C is wrong—Pareto identifies what to analyze, doesn't replace root cause analysis. Option D is wrong—Pareto explicitly shows causes are NOT equally important.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.2.3",
    "estimated_time_seconds": 95
  },
  {
    "module": "5.2",
    "module_name": "Problem-Solving Tools",
    "question": "Why is it important to define the problem clearly before trying to solve it?",
    "type": "multiple_choice",
    "options": [
      "Because a well-defined problem is half-solved—clear problem definition focuses the team, prevents scope creep, and ensures everyone is solving the same problem",
      "Because problem definition is required by ISO standards",
      "Because defining problems takes longer, giving more time to think",
      "Because vague problems are easier to solve"
    ],
    "correct_answer": 0,
    "explanation": "Problem definition is the foundation of successful problem-solving. A vague problem leads to: Scattered efforts (everyone solving different things), wrong solutions (addressing symptoms not root causes), scope creep (problem keeps expanding), inability to measure success (no clear target), wasted resources (solving non-problems). Example of vague vs. clear problem definition: Vague: 'Quality is bad.' Issues: Which quality? How bad? Where? When? Who's affected? How do we know when solved? Team might: Investigate all possible quality issues (scattered), assume they know which quality matters (wrong guess), improve wrong metrics (busy but ineffective). Clear: 'Injection molding defects in Product Line A increased from 3% to 15% over the last 3 months, resulting in $500K annual scrap costs and 20% customer returns. Target: Reduce to <5% within 6 months.' Benefits: Specific (injection molding defects, not general quality), Located (Product Line A, not whole factory), Quantified (15% current, 5% target), Timebound (3-month trend, 6-month goal), Impact (business case clear: $500K), Measurable success (will know when achieved). Elements of good problem definition: What is the problem? (specific, observable). Where does it occur? (scope boundaries). When did it start? (timing helps identify causes). Who is affected? (stakeholders). How big is the impact? (business case). What is the goal? (desired future state). The test: Could two different teams read your problem definition and work on the exact same problem? If yes, it's well-defined. If no, too vague. Why time spent on definition pays off: Prevents solving wrong problem (biggest waste in problem-solving), aligns team on shared understanding, focuses data collection on right metrics, enables effective root cause analysis, allows measurement of success, builds stakeholder support (clear business case). Common mistakes in problem definition: Solution in disguise: 'Problem is we need new equipment' (assumes solution). Too broad: 'Everything is broken' (no focus). Blame: 'Operators don't care' (focuses on people not process). Unmeasurable: 'Customers unhappy' (how much? How measure?). No impact: 'Defects are high' (so what? What's the cost?). Process for defining problem: Observe current state (what's actually happening), Quantify with data (how much, how often), Identify stakeholders (who's affected), Determine impact (why it matters), Set target (what success looks like), Get alignment (everyone agrees this is THE problem). Option B is wrong—definition important for success, not just compliance. Option C is wrong—definition focuses effort, not delays it. Option D is wrong—vague problems are harder to solve (no clear target).",
    "difficulty": "beginner",
    "learning_objective": "LO-5.2.4",
    "estimated_time_seconds": 95
  },
  {
    "module": "5.2",
    "module_name": "Problem-Solving Tools",
    "question": "A team implements a solution that worked in another department. The solution fails. What mistake did they likely make?",
    "type": "multiple_choice",
    "options": [
      "They didn't verify that the root cause was the same in both situations—solutions must match the specific root cause, not just similar symptoms",
      "They should have implemented it faster",
      "The first department lied about their results",
      "All solutions work everywhere if implemented correctly"
    ],
    "correct_answer": 0,
    "explanation": "Solutions are not one-size-fits-all—they must address the specific root cause. Same symptom doesn't mean same cause. Example of this mistake: Department A: Problem = late deliveries. Root cause = outdated scheduling system. Solution = implement new scheduling software. Result = on-time delivery improved 95% → 99%. Department B: Problem = late deliveries (same symptom!). Department B thinks: 'Department A fixed late deliveries with new software. Let's do the same!' Department B implements same software ($200K investment). Result = no improvement. Late deliveries still 85%. Why? Department B investigates (should have done this first): Root cause = insufficient delivery trucks, not scheduling. Software can't fix a truck shortage! Correct solution for B = add 2 more trucks or optimize routes. The lesson: Same symptom, different root cause, needs different solution. Why root cause must guide solution: Symptoms are visible effects, causes create those effects, solutions must address causes not symptoms. Treating symptoms = temporary relief but problem returns. Treating root cause = permanent fix. Real examples: Symptom: High defect rate. Possible causes: Poor training → Solution: Comprehensive training program. Equipment malfunction → Solution: Preventive maintenance. Material quality variation → Solution: Incoming inspection/supplier quality. Insufficient lighting → Solution: Better lighting. Unclear specifications → Solution: Document standards. Each cause requires different solution! Implementing wrong solution = waste. How to avoid this mistake: Step 1 Understand the solution context: Why did it work there? What was their root cause? What conditions existed? Step 2 Verify your root cause: Is our cause the same? Do we have same conditions? Or just similar symptoms? Step 3 Adapt, don't copy: Modify solution to fit your cause. Pilot test before full implementation. Step 4 Verify results: Did it work for us? If not, wrong solution or wrong cause? The mindset shift: Don't ask: 'What solution did they use?' Do ask: 'What was their root cause, and is ours the same?' Best practices can be adapted, but blindly copying solutions is dangerous. Example of proper knowledge transfer: Department C has late deliveries. They: Study Department A's root cause (scheduling), study Department B's root cause (insufficient trucks), investigate their own process, discover their root cause is poor communication between sales and operations. Solution: Daily coordination meeting + shared dashboard. Result: Late deliveries reduced 75%. They learned from others but solved their specific problem. Option B is wrong—speed doesn't fix wrong solution. Option C is wrong—results were real but not transferable. Option D is wrong—context matters, solutions must match causes.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.2.5",
    "estimated_time_seconds": 95
  },
  {
    "module": "5.3",
    "module_name": "Root Cause Analysis Techniques",
    "question": "What is the main difference between asking 'Who caused this problem?' and 'What caused this problem?'",
    "type": "multiple_choice",
    "options": [
      "'Who' leads to blame and defensiveness; 'What' leads to system improvement—focus on process failures, not personal failures, to find true root causes",
      "There is no difference—both questions accomplish the same thing",
      "'Who' questions are more effective because people cause all problems",
      "'What' questions avoid accountability"
    ],
    "correct_answer": 0,
    "explanation": "The language of problem-solving shapes the culture and effectiveness of improvement. 'Who' creates a blame culture: People hide mistakes (fear of punishment), problems go unreported (don't want to be scapegoat), defensive behavior (protect yourself, not solve problem), focus on personalities (John is careless), missed opportunities (people ARE the problem, can't fix people), high turnover (people leave blame cultures). Result: Problems persist because real causes hidden. 'What' creates a learning culture: People report problems (learn from mistakes), open discussion (safe to surface issues), collaborative investigation (team effort), focus on systems (process allowed mistake), improvement opportunities (fix process so anyone succeeds), innovation (try new ideas without fear). Result: Problems get solved because root causes addressed. Example—medication error in hospital: 'Who' approach: Investigation: 'Nurse Smith gave wrong medication.' Conclusion: 'Nurse Smith was careless.' Action: 'Discipline Nurse Smith.' Result: Next month, Nurse Jones makes same error. Problem persists (system unchanged). 'What' approach: Investigation: 'What system failures enabled this error?' Findings: Similar medication names stored side-by-side, poor lighting in medication room, no double-check procedure, 12-hour shifts with fatigue, interruptions during medication prep. Conclusion: System made error inevitable. Action: Separate similar medications, improve lighting, implement double-check, reduce shift hours, create interruption-free zone. Result: Medication errors drop 90% across all nurses. Problem solved (system fixed). W. Edwards Deming's principle: '85% of problems are system problems, only 15% are people problems.' When you ask 'Who,' you assume it's the 15%. When you ask 'What,' you find the 85% that you can actually fix. The mindset: Poor performer in a good system → will improve. Good performer in a poor system → will fail. Fix the system, not the person. When 'Who' is appropriate: Intentional sabotage, fraud, criminal behavior, pattern of willful negligence after training and support. But even then, ask: 'What system allowed this person to be hired/remain/not be caught sooner?' How to shift from 'Who' to 'What': Observe the person doing the work (Gemba), ask 'What made it difficult to do correctly?', investigate process, tools, training, environment, design system where error is impossible or obvious, assume people want to do good work (usually true). The cultural shift: From: 'Who messed up?' To: 'What can we learn?' From: 'Whose fault?' To: 'What failed?' From: 'Who's responsible?' To: 'What's the root cause?' Result: Psychological safety, continuous improvement, sustained results. Option B is wrong—'Who' and 'What' lead to very different outcomes. Option C is wrong—most problems are system/process issues. Option D is wrong—'What' creates better accountability (fix root causes vs. blame individuals).",
    "difficulty": "beginner",
    "learning_objective": "LO-5.3.1",
    "estimated_time_seconds": 100
  },
  {
    "module": "5.3",
    "module_name": "Root Cause Analysis Techniques",
    "question": "A fishbone diagram identifies 25 potential causes. How do you determine which are actual root causes?",
    "type": "multiple_choice",
    "options": [
      "Test each potential cause with data—collect evidence showing correlation between cause and effect; causes without data support are just guesses",
      "Pick the cause that seems most obvious",
      "Assume all 25 causes contribute equally",
      "The longest bone on the fishbone is always the root cause"
    ],
    "correct_answer": 0,
    "explanation": "Fishbone diagrams generate hypotheses—data analysis identifies truth. The fishbone (Ishikawa diagram) is a brainstorming tool organizing potential causes into categories (Man, Machine, Material, Method, Measurement, Environment). It's the starting point, not the conclusion. Example: Problem = high defect rate (15%). Fishbone brainstorm generates 25 potential causes across categories: Man: insufficient training, operator fatigue, high turnover. Machine: equipment age, poor maintenance, wrong settings. Material: supplier quality variation, wrong grade, storage conditions. Method: no standard procedures, unclear specs, complex process. Measurement: inconsistent inspection, subjective criteria, broken gauge. Environment: temperature variation, humidity, poor lighting. Now what? You have 25 hypotheses, not 25 root causes! The testing process: Step 1 Prioritize for testing. Quick screening: Plausibility (does it make sense?), Team knowledge (do we think this matters?), Test feasibility (can we get data?). Select top 5-10 to test. Step 2 Collect data for each hypothesis. Hypothesis: 'Operator training affects defect rate.' Data: Compare experienced operators (>2 years) vs. new (<6 months). Results: Experienced = 3% defects, New = 18% defects. Conclusion: Training confirmed as root cause. Hypothesis: 'Equipment age affects defect rate.' Data: Compare old machines (>10 years) vs. new (<2 years). Results: Old = 8% defects, New = 9% defects. Conclusion: Age NOT a root cause (no meaningful difference). Step 3 Statistical verification. Use t-tests, ANOVA, chi-square to determine if differences are statistically significant. Strong correlation (r > 0.7) suggests causation (verify further). Weak correlation (r < 0.3) suggests not a root cause. Step 4 Focus on verified causes. After testing all 25 hypotheses: 3 confirmed as true root causes (training, temperature variation, material quality). 22 rejected (correlation not found). Now: Focus 100% of improvement efforts on the 3 real causes. Ignore the 22 that don't actually matter. Result: Efficient, effective improvement. Why data validation critical: Opinions are often wrong (confirmation bias, recency bias, availability bias), obvious causes often not actual causes (correlation ≠ causation), multiple causes can exist (need to identify all contributors), non-causes waste resources if addressed (fixing things that don't matter). Methods for testing causes: Stratification: Separate data by suspected cause, compare subgroups. Scatter plots: Plot suspected cause vs. effect, look for pattern. Design of Experiments (DOE): Systematically vary factors, measure impact. Before/after: Change suspected cause, measure if effect changes. Correlation analysis: Quantify relationship strength. Real outcome after testing 25 hypotheses: Typically find: 2-4 are true root causes (focus here), 8-10 are minor contributors (address if time), 11-15 have no impact (ignore). This focused approach is why data-driven teams succeed where guessing teams fail. Option B is wrong—'obvious' causes are often wrong, test with data. Option C is wrong—most causes don't contribute, data shows which matter. Option D is wrong—diagram structure doesn't indicate causation, data does.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.3.2",
    "estimated_time_seconds": 100
  },
  {
    "module": "5.3",
    "module_name": "Root Cause Analysis Techniques",
    "question": "Why might a team need to use multiple root cause analysis tools rather than relying on just one?",
    "type": "multiple_choice",
    "options": [
      "Different tools reveal different aspects—5 Whys explores cause chains, Fishbone generates comprehensive possibilities, data analysis verifies which matter; combining tools provides complete understanding",
      "Using multiple tools wastes time—one tool is always sufficient",
      "All root cause analysis tools produce identical results",
      "More tools make the problem more complex"
    ],
    "correct_answer": 0,
    "explanation": "Root cause analysis tools are complementary, not redundant—each has unique strengths. Tool comparison and use cases: 5 Whys—Strengths: Simple, fast, explores cause chains (surface symptom → deeper cause → root cause), good for linear problems. Weaknesses: Can oversimplify complex problems, may miss parallel causes, requires discipline to avoid stopping too soon. Best for: Single clear problem path, quick investigation, training new problem-solvers. Fishbone Diagram—Strengths: Comprehensive, organizes thinking, engages team, considers multiple categories. Weaknesses: Generates hypotheses not proof, can become overwhelming, doesn't prioritize or verify. Best for: Complex problems, brainstorming with teams, when causes unclear. Pareto Analysis—Strengths: Prioritizes issues, shows vital few vs. trivial many, quantifies impact. Weaknesses: Shows 'what' not 'why,' requires categorized data, doesn't identify root causes. Best for: Multiple problem types, resource allocation, establishing priorities. Scatter Plots/Correlation—Strengths: Shows relationships visually, quantifies strength, tests hypotheses. Weaknesses: Correlation ≠ causation, requires numerical data, limited to two variables at a time. Best for: Testing suspected causes, quantifying relationships, verifying hypotheses. Design of Experiments (DOE)—Strengths: Tests multiple factors simultaneously, isolates interactions, rigorous proof. Weaknesses: Complex to design, time-intensive, requires statistical knowledge. Best for: Complex multi-factor problems, high-stakes decisions, proving causation. Example of combining tools: Problem: Customer complaints increased 200% over 3 months. Step 1 Use Pareto to identify vital few. Create Pareto of complaint types. Result: 40% billing errors, 30% late delivery, 20% product defects, 10% other. Decision: Focus on billing errors (vital few). Step 2 Use Fishbone to brainstorm causes of billing errors. Categories: People (training), Process (manual entry), Systems (software bugs), Data (customer info errors), Timing (month-end rush). Generate 20 potential causes. Step 3 Use 5 Whys to explore deep causes. Why billing errors? → Data entry mistakes. Why mistakes? → Rush during month-end. Why rush? → All invoices batched to month-end. Why batched? → Legacy procedure from manual era. Why still using legacy? → Nobody questioned it. Root cause: Outdated batch process creates time pressure. Step 4 Use data analysis to verify. Test hypothesis: Do errors spike at month-end? Data: 5% error rate week 1-3, 25% error rate week 4. Conclusion: Hypothesis confirmed! Month-end batching is root cause. Solution: Implement daily invoicing (eliminate batch), automate data entry (reduce manual errors). Result: Billing errors drop 80%. Without multiple tools: Pareto alone: Would identify billing as problem but not why. 5 Whys alone: Might miss other complaint types (tunnel vision). Fishbone alone: Would generate ideas but not prioritize or verify. Data alone: Shows correlation but not full system context. The power of combination: Each tool compensates for others' weaknesses, provides triangulation (multiple methods → same conclusion = confidence), engages different thinking styles (some people visual, some analytical), builds comprehensive understanding, increases solution success probability. When to use which combination: Simple problems: 5 Whys + data verification (quick and effective). Complex problems: Fishbone + Pareto + 5 Whys + data analysis (comprehensive). High-stakes decisions: Full suite + DOE (rigorous proof). Option B is wrong—multiple tools increase understanding, don't waste time. Option C is wrong—each tool reveals different insights. Option D is wrong—proper use of multiple tools clarifies, doesn't complicate.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.3.3",
    "estimated_time_seconds": 100
  },
  {
    "module": "5.3",
    "module_name": "Root Cause Analysis Techniques",
    "question": "A team identifies what they believe is the root cause. What should they do before implementing solutions?",
    "type": "multiple_choice",
    "options": [
      "Verify the root cause with data or experimentation—confirm that changing this cause actually affects the problem before investing in solutions",
      "Immediately implement solutions—verification wastes time",
      "Present to management and get approval without verification",
      "Assume the root cause is correct if the team agrees"
    ],
    "correct_answer": 0,
    "explanation": "Verification is the difference between solving the problem and wasting resources. Suspected root cause is a hypothesis until proven. The risk of skipping verification: Implement solution for wrong cause → waste time and money, problem persists → credibility damaged, team becomes discouraged → momentum lost, stakeholders lose confidence → future projects harder to approve. The cost can be enormous: $500K equipment purchase for wrong cause, 6 months implementing ineffective solution, opportunity cost of not fixing real cause. Verification methods: Method 1 Correlation analysis—Measure suspected cause and effect over time. Look for pattern. Example: Suspect temperature causes defects. Data: Plot temperature vs. defect rate daily for 30 days. Result: Strong correlation (r = 0.89)—when temperature high, defects high. Conclusion: Likely root cause (but verify further). Method 2 Controlled experiment—Change suspected cause deliberately, measure effect. Example: Day 1-5: Allow temperature to vary (current state), measure defects. Day 6-10: Control temperature tightly, measure defects. Results: Variable temp = 15% defects. Controlled temp = 3% defects. Conclusion: Temperature IS root cause (proven). Method 3 Before/after comparison—Find times when suspected cause was different. Example: Suspected cause = operator training. Data: Defect rates before training program (15%) vs. after (4%). Confounding factors checked: same equipment, same materials, same season. Conclusion: Training is likely root cause. Method 4 Absence test—Remove suspected cause temporarily, observe effect. Example: Suspected cause = specific supplier's material. Test: Switch to different supplier for one week. Result: Defects drop from 15% to 2%. Switch back: Defects return to 14%. Conclusion: Supplier material IS root cause (confirmed by reversal). What verification confirms: Change in suspected cause → change in effect (correlation). Change is statistically significant (not random variation). Change is consistent across multiple tests (repeatable). No other factors changed simultaneously (cause is isolated). Effect size matches prediction (magnitude makes sense). Example of verification catching wrong assumption: Suspected root cause: 'Equipment age causes defects' (old machines produce more defects). Verification test: Collect data from 10 old machines and 10 new machines. Results: Old machines = 8% defects. New machines = 9% defects. No significant difference! Conclusion: Equipment age is NOT root cause (hypothesis rejected). New investigation: What else could it be? Further analysis: Operators on old machines are more experienced! New hypothesis: Experience is the real root cause. New verification: Compare experienced vs new operators (controlled for machine age). Results: Experienced = 3% defects. New = 18% defects. Conclusion: Experience IS root cause (verified). Without verification, would have wasted money replacing equipment that wasn't the problem! When to verify: Before major investments ($10K+), before process changes affecting many people, when multiple hypotheses exist (verify each), when stakeholders skeptical, always for recurring problems (get it right this time). Quick verification shortcuts: If low-cost experiment possible (< $1K, < 1 week) → test before implementing. If reversible change possible → try it, measure, reverse if doesn't work. If natural variation exists → analyze historical data for pattern. The principle: Prove it before you bet on it. Option B is wrong—verification prevents wasted implementation of wrong solutions. Option C is wrong—management approval doesn't make wrong cause right. Option D is wrong—consensus doesn't equal truth, data verification essential.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.3.4",
    "estimated_time_seconds": 100
  },
  {
    "module": "5.3",
    "module_name": "Root Cause Analysis Techniques",
    "question": "Why is it often better to fix the system that allowed the problem rather than just fix the individual problem instance?",
    "type": "multiple_choice",
    "options": [
      "System fixes prevent recurrence across all situations; fixing individual instances addresses one occurrence while leaving the underlying vulnerability that will cause future problems",
      "System fixes are always easier than fixing individual problems",
      "Individual fixes are more expensive in all cases",
      "Systems can't be improved, only individual problems can be fixed"
    ],
    "correct_answer": 0,
    "explanation": "The difference between fixing one problem and preventing thousands. Individual fix (firefighting): Problem occurs → fix that instance → problem occurs again → fix again → repeat forever. Each fix: solves one occurrence, reactive approach, problem keeps recurring, constant firefighting, never-ending cycle, exhausting for team. System fix (prevention): Problem occurs → investigate why system allowed it → fix system vulnerability → problem can't recur. One fix: prevents all future occurrences, proactive approach, problem eliminated, sustainable results, team can focus on new challenges. Example—medication errors: Individual fix approach: Monday: Nurse gives wrong medication. Fix: Correct patient's treatment, counsel nurse. Tuesday: Different nurse, same error. Fix: Correct treatment, counsel nurse. Wednesday: First nurse makes error again. Fix: Correct treatment, stronger warning. This continues indefinitely. Result: Errors persist (15/month), constant firefighting, nurses demoralized, patients at risk. System fix approach: Investigate: Why does system allow this error? Findings: Similar medication names stored together, poor lighting makes labels hard to read, no double-check procedure, interruptions during medication prep common. System changes: Separate similar medications physically, improve lighting in medication room, implement double-check protocol, create interruption-free medication prep zone, add barcode scanning verification. Result: Errors drop to 1/month (93% reduction), sustainable (doesn't rely on perfect humans), applies to all current and future nurses, frees time from firefighting. The cost-benefit: Individual fix: 15 errors/month × 12 months = 180 fixes/year. Cost: 1 hour per fix = 180 hours/year ongoing forever. System fix: One-time investigation and implementation. Cost: 40 hours upfront + $5K for improvements. Benefit: 167 errors prevented/year (168 vs 1), 167 hours saved/year, reduced patient harm, improved nurse morale. Payback: Pays for itself in 3 months, then saves 167 hours every year forever. When to fix system vs. individual: Fix individual when: Problem is truly one-time (unique circumstances), system fix cost >> individual fix × recurrence rate, immediate correction needed (fix instance now, investigate system later). Fix system when: Problem recurs or likely to recur, affects multiple people/situations, root cause is systemic, prevention is possible, organization committed to improvement. Best approach—do both: Fix individual problem immediately (stop the bleeding), then fix system to prevent recurrence (cure the disease). Example: Customer receives wrong product. Immediate: Ship correct product overnight (fix individual instance). System: Investigate why error occurred (barcode scanning not used), implement barcode verification at picking stage (fix system), train all pickers on new procedure, monitor for 30 days (verify fix worked). Result: Customer satisfied (immediate fix) + error eliminated for all future customers (system fix). The mindset shift: From: 'We need to work harder to avoid mistakes.' To: 'We need to change the system so mistakes are impossible.' From: 'Train people to be more careful.' To: 'Design processes that don't require perfection.' From: 'Who made this error?' To: 'What system failure allowed this error?' Result: Fewer problems, less firefighting, more sustainable improvements, better team morale, continuous improvement culture. Option B is wrong—system fixes often harder initially but pay off long-term. Option C is wrong—individual fixes can be cheaper per instance but more expensive over time. Option D is wrong—systems can and should be improved for prevention.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.3.5",
    "estimated_time_seconds": 100
  },
  {
    "module": "5.4",
    "module_name": "Solution Implementation",
    "question": "Why is it important to pilot a solution on a small scale before full implementation?",
    "type": "multiple_choice",
    "options": [
      "Pilots reveal unforeseen issues and allow refinement on small scale before committing full resources—reducing risk and increasing success probability",
      "Pilots delay improvement and should be skipped",
      "Pilots are only needed when management requires them",
      "Full implementation is always more efficient than piloting"
    ],
    "correct_answer": 0,
    "explanation": "Piloting is controlled experimentation before full commitment—test, learn, refine, then scale. The risk of skipping pilots: Implement across all 10 production lines. Discover: Settings need adjustment, operators need different training, unintended side effects, conflicts with other systems. Result: Chaos across entire operation, expensive to fix everywhere, difficult to roll back, credibility damaged. The value of piloting: Implement on 1 production line (10% of operation). Discover same issues but: Limited impact if fails, easy to adjust and refine, learn lessons cheaply, prove concept before scaling, build confidence and support. Refine solution based on learnings. Scale to remaining 9 lines with proven approach. Result: Smooth deployment, budget on track, high success rate. Pilot methodology (PDSA): Plan: Define pilot scope (which line/area), set objectives (what success looks like), determine metrics (how to measure), establish timeline (how long to test). Do: Implement solution in pilot area, collect data throughout, document issues and surprises, gather feedback from users. Study: Analyze results vs. objectives, identify what worked/didn't work, understand why variations occurred, document lessons learned. Act: Refine solution based on learnings, if successful → scale to full implementation, if failed → revise and re-pilot or abandon. Example—office productivity solution: Problem: Email overload (50+ emails/day per person). Solution idea: Implement collaboration platform to reduce email. Without pilot: Roll out to all 500 employees immediately ($100K investment). Issues discovered: Platform confusing (adoption low), email still used (people didn't switch), notifications annoying (turned off), integration problems with existing systems. Result: Low adoption (20%), email unchanged, money wasted. With pilot: Roll out to one department (30 people) for 30 days. Issues discovered: Same as above but on small scale. Refinements: Simplified setup process, provided hands-on training, adjusted notification settings, fixed integration issues, created clear guidelines on when to use platform vs email. Re-pilot refined solution: Adoption 80%, email reduced 40%, high satisfaction. Scale to full organization with refined solution and proven training approach. Result: 500 employees, 70% adoption, email reduced 35%, ROI achieved. Pilot scope considerations: Large enough: Representative of full population/process, enough data to draw conclusions, real-world conditions. Small enough: Limited risk if fails, manageable to support closely, quick feedback cycles, easy to adjust. Typical pilot sizes: Manufacturing: 1 production line or shift. Office process: 1 department or team. IT system: 10-30 users. New procedure: 1 location or product line. Pilot success criteria (define upfront): Performance metrics: Does it achieve improvement goal? User acceptance: Do people use it willingly? Operational feasibility: Can we actually operate this way? Cost-benefit: Is ROI acceptable? Unintended consequences: Any negative side effects? Scalability: Can this work across full scope? Red flags to watch during pilot: Users find workarounds (not actually using solution), metrics don't improve (solution ineffective), significant resistance (change management issue), unforeseen problems (need to revise), resource requirements higher than expected (budget risk). Green lights to scale: Objectives met or exceeded, users satisfied and engaged, no major surprises, lessons learned incorporated, stakeholders supportive, ROI confirmed. The ROI of piloting: Pilot investment: 10% of full implementation cost, 10% of full implementation time. Risk mitigation: 90% of problems discovered on 10% scale (cheap to fix). Success rate increase: Piloted solutions 80% success rate vs 50% without pilots. Example: Pilot = $10K, 2 weeks. Discovers issues, refines solution. Full implementation = $90K, 8 weeks (total $100K, 10 weeks). No pilot = $100K, 10 weeks BUT 50% chance of failure requiring rework ($200K, 20 weeks total). Piloting reduces risk and total cost. Option B is wrong—pilots accelerate long-term success by preventing full-scale failures. Option C is wrong—pilots are good practice regardless of requirements. Option D is wrong—skipping pilots risks expensive full-scale failures.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.4.1",
    "estimated_time_seconds": 100
  },
  {
    "module": "5.4",
    "module_name": "Solution Implementation",
    "question": "What is the purpose of creating an implementation plan before executing a solution?",
    "type": "multiple_choice",
    "options": [
      "An implementation plan defines who does what by when with what resources—preventing confusion, ensuring coordination, and increasing execution success",
      "Implementation plans are bureaucratic obstacles that slow down improvement",
      "Implementation plans replace the need for team communication",
      "Good solutions don't need implementation plans"
    ],
    "correct_answer": 0,
    "explanation": "Implementation plans turn good ideas into reality—without a plan, even great solutions fail. The failure of 'ready, fire, aim' implementation: Team decides on solution: 'Implement barcode scanning for inventory.' Without plan: Start implementing, confusion about who does what, procurement doesn't know what to buy, IT wasn't informed (system not ready), training not scheduled, no budget approved, implementation half-done, results disappointing. With implementation plan: Clear roadmap, everyone knows responsibilities, resources lined up, coordinated execution, successful implementation. What an implementation plan includes: Objectives: What are we trying to achieve? Clear, measurable goals. Activities: What specific tasks must be done? (Procure scanners, install software, train users, update procedures). Sequence: What order must activities happen? (Can't train before scanners arrive). Responsibilities: Who is accountable for each activity? Names, not roles. Timeline: When will each activity happen? Start and end dates. Resources: What's needed? (Budget, people, equipment, time). Dependencies: What must happen before what? Critical path. Risks: What could go wrong? Mitigation plans. Communication: Who needs to know what when? Success metrics: How will we know if it worked? Example implementation plan—barcode scanning: Phase 1 Planning (Weeks 1-2): Finalize scanner specifications (John, by 3/15), get budget approval (Sarah, by 3/18), select vendor (Procurement, by 3/22). Phase 2 Preparation (Weeks 3-4): Order scanners (Procurement, by 3/25), configure software (IT, by 4/5), create training materials (Operations, by 4/8), update procedures (Quality, by 4/10). Phase 3 Pilot (Weeks 5-6): Install scanners in Warehouse A (IT, by 4/15), train pilot team (Operations, by 4/17), run pilot for 2 weeks (All, 4/18-4/29), collect feedback and refine (All, by 5/3). Phase 4 Scale (Weeks 7-10): Install in all warehouses (IT, by 5/24), train all staff (Operations, by 5/31), monitor and support (All, ongoing). Success metrics: Inventory accuracy from 85% → 98%, picking errors from 5% → 1%, time to locate items from 5 min → 1 min. Resources: $50K budget, 2 IT staff for 200 hours, 3 operations staff for 100 hours. Risks and mitigation: Risk: Scanner battery life insufficient. Mitigation: Test during pilot, have chargers strategically placed. Risk: Staff resistance to change. Mitigation: Involve operators in selection, emphasize benefits, provide good training. Risk: System integration issues. Mitigation: IT testing in week 3-4, contingency plan. Why implementation plans work: Clarity: Everyone knows what to do, ambiguity eliminated. Coordination: Activities synchronized, dependencies managed. Accountability: Clear ownership, can track progress. Resource alignment: Budget, people, time secured upfront. Risk management: Potential problems identified and mitigated. Communication: Stakeholders informed, expectations set. Measurement: Can track progress and results. Common implementation failures without plan: Key person not available (assumed they'd help but wasn't in their plan), resources not allocated (no budget approved), dependencies missed (tried to train before equipment arrived), scope creep (project expanded beyond original intent), timeline unrealistic (underestimated time needed), no one accountable (everyone thought someone else was doing it). Implementation plan formats: Simple projects: Spreadsheet with tasks, owners, dates. Medium projects: Gantt chart showing timeline and dependencies. Complex projects: Detailed project plan with phases, milestones, risks. When to skip detailed planning: Immediate emergency requiring action now (plan quickly later), very simple change (one person, one step, low risk), iterative experiment (plan-do-check-act cycles). But even then, have some plan! The principle: Plans aren't perfect and will need adjustment, but having a plan is 10× better than having no plan. Option B is wrong—good plans accelerate execution by preventing chaos. Option C is wrong—plans enhance communication, don't replace it. Option D is wrong—even great solutions need coordination to implement successfully.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.4.2",
    "estimated_time_seconds": 100
  },
  {
    "module": "5.4",
    "module_name": "Solution Implementation",
    "question": "After implementing a solution, defects decrease from 15% to 5% (goal was 3%). What should the team do?",
    "type": "multiple_choice",
    "options": [
      "Celebrate the improvement, understand why the goal wasn't fully met, determine if additional action is needed to close the gap or if 5% is acceptable",
      "Declare complete failure because the goal wasn't met exactly",
      "Immediately start over with a completely different solution",
      "Ignore the gap—any improvement is sufficient"
    ],
    "correct_answer": 0,
    "explanation": "Improvement is progress, but the response depends on context and analysis. The situation: Started at 15% defects, goal was 3%, achieved 5%. This is 67% reduction—significant improvement! But still 2% above goal. The balanced response: Acknowledge success: 67% reduction is substantial, celebrate the improvement, recognize team effort. Understand the gap: Why didn't we reach 3%? Was goal unrealistic? Did solution work less than expected? Is there a remaining root cause? Is 5% acceptable? Determine next steps based on analysis: Decision tree: Question 1: Is 5% acceptable to stakeholders/customers? If yes: Celebrate success, implement control plan, move to next problem. If no: Continue to Question 2. Question 2: What's causing the remaining 2%? Option A: Different root cause than original 15%. Action: Investigate remaining defects, identify new root cause, implement additional solution. Option B: Same root cause not fully addressed. Action: Refine current solution, increase effectiveness. Option C: Goal was unrealistic (3% not achievable with current resources). Action: Negotiate new goal, implement controls at 5%. Example analysis: Initial 15% defects had two root causes: Primary cause (80% of defects): Poor temperature control. Solution implemented: Automated temperature monitoring. Result: 12% reduction (15% → 3%). Secondary cause (20% of defects): Material quality variation. Not addressed yet. Result: Remaining 2% defects (3% → 5% actual vs 3% goal). Next steps: Phase 2 project: Implement incoming material inspection, work with supplier on quality. Expected result: Additional 2% reduction, reaching 3% goal (or better). Alternatively—is 5% acceptable? Cost-benefit analysis: To go from 5% → 3% requires $100K investment. Current 5% defects cost $50K/year. ROI: 2-year payback. Decision: Acceptable if strategic priority, otherwise maintain at 5% and focus resources on bigger opportunities. When to celebrate 'partial' success: Significant progress toward goal (67% of the way), lessons learned for future projects, stakeholder satisfaction with progress, cost-benefit supports stopping here. When to continue pushing: Customer requirements demand reaching goal, competitive pressure requires excellence, low-hanging fruit remains (easy to achieve), committed to original goal. The mindset: Improvement is not all-or-nothing, progress should be recognized, perfection is often not necessary, continuous improvement continues (can always revisit), celebrate wins while maintaining standards. Toxic responses to avoid: Don't punish team for 'failing' to hit exact target (kills motivation), don't ignore significant improvement achieved (demoralizes team), don't automatically assume more effort will close gap (might be other factors), don't keep pushing without analysis (could be diminishing returns). Healthy responses: Celebrate improvement publicly, analyze gap objectively, make informed decision about next steps, maintain momentum for continuous improvement, learn from both success and shortfall. Example communication: 'Great work team! We reduced defects by 67%, from 15% to 5%. This saves $300K annually and significantly improves customer satisfaction. Our stretch goal was 3%. Let's investigate the remaining 2% and determine if additional effort is warranted or if 5% meets our business needs. Either way, this is a major win for the organization!' Result: Team feels valued, momentum maintained, decision made rationally, culture of improvement reinforced. Option B is wrong—67% improvement is not failure, it's significant success. Option C is wrong—starting over wastes the improvement achieved, build on success. Option D is wrong—gap should be analyzed to determine if acceptable or needs addressing.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.4.3",
    "estimated_time_seconds": 105
  },
  {
    "module": "5.4",
    "module_name": "Solution Implementation",
    "question": "What is the purpose of standardizing a process after implementing an improvement?",
    "type": "multiple_choice",
    "options": [
      "Standardization ensures everyone follows the improved method consistently, preventing regression to old ways and maintaining improvement gains over time",
      "Standardization stifles creativity and should be avoided",
      "Standardization is only needed in manufacturing, not services or offices",
      "Standardization means never changing the process again"
    ],
    "correct_answer": 0,
    "explanation": "Standardization is how improvements become permanent rather than temporary. The problem without standardization: Implement improvement: Defects drop 15% → 3%. Week 1: Everyone follows new method. Week 4: Some people drift back to old way. Week 8: Half the team doing it the old way, half the new way. Week 12: Defects back to 12%. Improvement lost because it wasn't standardized! The solution with standardization: Implement improvement: Defects drop 15% → 3%. Document new method as standard: Create clear procedures, train everyone on standard, post visual guides at workstation. Monitor compliance: Regular audits, feedback and coaching. Sustain improvement: Defects stay at 3% long-term. What standardization includes: Standard Work documents: Step-by-step procedures (what to do, in what sequence), time for each step (takt time), quality checkpoints (where/how to verify), tools and materials needed (what to use). Visual management: Photos or diagrams at workstation, color-coding for clarity, mistake-proofing (poka-yoke), progress boards showing compliance. Training: Train all current employees on standard, include in new hire onboarding, refresher training periodically, certification/competency checks. Monitoring: Regular audits of compliance, metrics showing results, feedback to individuals and team, continuous improvement of the standard. Example—email response process improvement: Before improvement: Email response times varied 1 hour to 3 days (no standard method). Each person had own approach. Average response time: 18 hours. Improvement implemented: Triage system (urgent/routine), templates for common questions, dedicated response times. Result: Average response time: 4 hours (78% improvement). Standardization to sustain: Document standard: Triage criteria documented, templates created and shared, schedule defined (check email 9am, 1pm, 4pm), escalation procedures for complex issues. Train team: Workshop on new standard, practice with real examples, reference guide created, posted on shared drive. Monitor: Track response times weekly, audit template usage monthly, team review of standard quarterly, adjust standard as needed. Result 2 years later: Response time sustained at 4 hours (improvement maintained). New employees onboarded to standard method. Standard has been improved twice based on learning. Standardization enables further improvement: Current best method documented (baseline for next improvement), variation reduced (can see if changes actually work), data collection consistent (reliable measurement), knowledge preserved (doesn't leave with individuals), continuous improvement cycles (PDCA on the standard). The paradox: Standardization enables flexibility: Clear standard (baseline to deviate from with purpose), measure current performance (know if change is better or worse), experiment safely (can always return to standard), capture learning (update standard with improvements). Example: Manufacturing standard work includes 15 steps. Operator suggests improvement: Skip step 7 (redundant check). Test: Run 50 parts without step 7. Result: Quality same, time reduced 2 minutes. Decision: Update standard work to eliminate step 7. New standard: 14 steps. Without original standard, couldn't have tested improvement confidently! When standardization is appropriate: Process is stable and understood, best known method identified, multiple people perform same work, consistency important for quality/safety, training new employees needed. When to avoid rigid standardization: Process still unstable (standardize what?), rapid experimentation phase (too early), creative work where variation valuable, customer interaction requiring personalization. Standardization principles: Standardize the best current method (not the old way), involve people doing the work (not just managers), make it visual and simple (easy to follow), update regularly based on learning (living document), audit for compliance (what gets measured gets done). Option B is wrong—standardization of best known method enables rather than stifles improvement. Option C is wrong—standardization applies everywhere (offices, healthcare, services). Option D is wrong—standards should be continuously improved (standardize-improve-standardize cycle).",
    "difficulty": "beginner",
    "learning_objective": "LO-5.4.4",
    "estimated_time_seconds": 105
  },
  {
    "module": "5.4",
    "module_name": "Solution Implementation",
    "question": "A solution is implemented successfully in one department. What factors should be considered before replicating it in other departments?",
    "type": "multiple_choice",
    "options": [
      "Context differences—verify that other departments have similar root causes, resources, culture, and conditions before assuming the solution will work; adapt as needed",
      "All successful solutions transfer perfectly to any context",
      "Never replicate solutions—each department must start from scratch",
      "Only the cost of replication matters"
    ],
    "correct_answer": 0,
    "explanation": "Solutions are context-dependent—what works in Department A might not work in Department B without adaptation. Context assessment checklist: Root cause similarity: Question: Is the problem caused by the same root cause? Department A: Late deliveries caused by insufficient trucks. Department B: Late deliveries caused by poor route planning. Assessment: Different root causes! Same symptom (late deliveries) but A's solution (add trucks) won't fix B's cause (poor planning). Action: Adapt solution or develop different solution. Resource availability: Question: Does Department B have the resources Department A had? Department A solution: Automated system ($100K, 2 FTE to operate). Department B: Budget only $20K, 0.5 FTE available. Assessment: Resource mismatch! Can't directly replicate. Action: Scaled-down version or different solution. Process similarity: Question: Do departments have similar processes? Department A: Batch processing with 100 units/batch. Solution: Optimize batch sequencing. Department B: Continuous flow processing (no batches). Assessment: Process mismatch! Batch solution irrelevant to continuous flow. Action: Don't replicate; develop flow-specific solution. Cultural readiness: Question: Is Department B ready for this change? Department A: Improvement-oriented culture, embraced change. Department B: Change-resistant culture, skeptical of 'corporate initiatives.' Assessment: Cultural mismatch! Will face resistance. Action: Build readiness first (communicate benefits, involve team, pilot slowly). Environmental factors: Question: Do physical/operational environments differ? Department A: Climate-controlled facility. Solution: Process relies on stable temperature. Department B: Outdoor operation with temperature variations. Assessment: Environmental mismatch! Solution assumptions invalid. Action: Adapt solution for variable temperature. Stakeholder buy-in: Question: Do Department B stakeholders support this? Department A: Manager championed solution, team engaged. Department B: Manager skeptical, team unaware. Assessment: Stakeholder mismatch! Will fail without support. Action: Build support before implementing (present results from A, engage B's manager). The assessment matrix: Create scorecard: Root cause similarity (0-10): 8 = very similar. Resource availability (0-10): 6 = mostly available. Process similarity (0-10): 9 = nearly identical. Cultural readiness (0-10): 4 = resistant. Environmental match (0-10): 7 = similar enough. Stakeholder support (0-10): 5 = neutral. Total score: 39/60 (65%). Decision: Medium fit—replicate with modifications. Adaptations needed: Address cultural resistance (communication plan, involvement), build stakeholder support (share Dept A results, pilot in Dept B), minor process adjustments (document differences), provide extra support during implementation. Example replication success: Department A reduces order processing time 18 hours → 4 hours using: Triage system, email templates, dedicated response times. Assess Department C for replication: Root cause same? Yes (same email overload issue). Resources available? Yes (same staffing). Process similar? Mostly (some differences in order types). Culture ready? Unknown. Stakeholders support? Manager interested. Decision: Replicate with pilot approach. Pilot in Department C: Start with 5 volunteers (not whole team), run 30-day pilot, adjust templates for C's specific order types (adaptation), measure results, gather feedback, refine approach. Results: Response time 18 hours → 5 hours (similar to A), some templates needed modification (expected), discovered additional improvement (bonus), team positive (good change management). Scale: Roll out to full Department C with refined approach, share learnings back to Department A (continuous improvement), document adaptations for future replications. Replication strategy: Assess context fit (as above), adapt solution as needed (don't blindly copy), pilot before full deployment (test in new context), engage local stakeholders (build ownership), measure and verify results (confirm it works here), document learnings (for next replication). When to replicate: High context similarity (80%+ match), strong stakeholder support, clear value proposition, resources available. When not to replicate: Low context similarity (<50% match), fundamental differences in root cause, insufficient resources, strong resistance. Option B is wrong—context matters, solutions must be adapted. Option C is wrong—learning from others accelerates improvement, but adapt don't copy blindly. Option D is wrong—cost is one factor but context fit determines success.",
    "difficulty": "beginner",
    "learning_objective": "LO-5.4.5",
    "estimated_time_seconds": 105
  }
]
