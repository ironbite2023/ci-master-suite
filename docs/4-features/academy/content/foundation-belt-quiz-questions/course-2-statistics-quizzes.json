[
  {
    "module": "2.1",
    "module_name": "Types of Data",
    "question": "A manufacturing quality engineer needs to track both the number of defects per unit and the exact weight of each finished product. What types of data are these, and why does the distinction matter?",
    "type": "multiple_choice",
    "options": [
      "Defects are discrete data (counted in whole numbers), while weight is continuous data (measured on a scale)—this affects which statistical tests and charts are appropriate",
      "Both are continuous data because you can measure them with precision",
      "Both are discrete data because you record them in a quality database",
      "Defects are qualitative data while weight is quantitative data, but otherwise there's no practical difference"
    ],
    "correct_answer": 0,
    "explanation": "Understanding data types is fundamental to choosing appropriate analysis methods. Defects per unit are discrete data—you can have 0, 1, 2, or 3 defects, but never 2.5 defects. You count discrete data in whole numbers. Weight, however, is continuous data—a product can weigh 10.234 kg, 10.235 kg, or any value on a continuous scale limited only by measurement precision.\n\nWhy this distinction matters practically:\n\n**For Charts**: Discrete data typically uses bar charts or Pareto charts (showing counts of defects by type). Continuous data uses histograms (showing distribution of weights across ranges) or run charts (showing weight over time).\n\n**For Statistics**: Discrete data is often analyzed using counts and proportions (defect rate = 5%). Continuous data uses mean, standard deviation, and process capability indices (Cp, Cpk).\n\n**For Control Charts**: Discrete data uses p-charts (proportion defective), c-charts (count of defects), or u-charts (defects per unit). Continuous data uses X-bar and R charts or individuals charts.\n\n**For Sample Size**: Continuous data typically requires smaller sample sizes for valid statistical conclusions because each measurement carries more information. Discrete data (especially when defects are rare) requires larger samples.\n\nWhy other options are wrong:\n\nOption B is incorrect—defects cannot be continuous. You never have fractional defects.\n\nOption C confuses how data is stored with its inherent nature. Both types can be stored digitally, but their mathematical properties differ.\n\nOption D is partially correct (defects could be considered qualitative if just categorizing as 'defective/not defective,' and weight is quantitative), but misses the critical discrete/continuous distinction that determines which analytical tools to use. The practical difference is enormous—using the wrong chart or statistic for your data type leads to invalid conclusions.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.1.1",
    "estimated_time_seconds": 85
  },
  {
    "module": "2.1",
    "module_name": "Types of Data",
    "question": "A hospital is tracking two metrics: patient satisfaction ratings (1-5 scale) and patient wait time in minutes. What types of data are these?",
    "type": "multiple_choice",
    "options": [
      "Satisfaction ratings are discrete ordinal data (ranked categories with order), while wait time is continuous ratio data (measured with a true zero point)",
      "Both are continuous data since they involve numbers",
      "Satisfaction ratings are continuous because they can be averaged, while wait time is discrete because it's recorded in whole minutes",
      "Both are qualitative data because they describe patient experiences"
    ],
    "correct_answer": 0,
    "explanation": "This question reveals nuances in data classification that affect how you analyze and interpret results.\n\n**Patient Satisfaction Ratings (1-5 scale):**\nThese are discrete ordinal data. Discrete because you can only select 1, 2, 3, 4, or 5—no fractional ratings. Ordinal because the numbers represent ranked categories with meaningful order (5 is better than 4, which is better than 3), but the intervals between categories aren't necessarily equal. The difference between 'very satisfied' (5) and 'satisfied' (4) might not equal the difference between 'dissatisfied' (2) and 'very dissatisfied' (1) in patients' actual experiences.\n\n**Important consideration**: While you CAN calculate an average satisfaction rating (3.8), you must interpret it carefully. The average treats intervals as equal, which may not reflect reality. Median or mode are often better central tendency measures for ordinal data.\n\n**Patient Wait Time (minutes):**\nThis is continuous ratio data. Continuous because wait time can theoretically be any value (15.3 minutes, 15.32 minutes, etc.), limited only by measurement precision. Ratio data because it has a true zero point (zero minutes means no wait), and ratios are meaningful (40 minutes is exactly twice as long as 20 minutes).\n\n**Practical Implications:**\n- Satisfaction ratings: Use bar charts, median, mode, frequency distributions\n- Wait time: Use histograms, mean, standard deviation, control charts, capability analysis\n- Correlation analysis between them requires recognizing that one is ordinal (use Spearman correlation, not Pearson)\n\nWhy other options are wrong:\n\nOption B oversimplifies—not all numbers are continuous. Satisfaction ratings have discrete, fixed categories.\n\nOption C is backward. Satisfaction ratings are discrete despite being averaged. Wait time is continuous even if you record it in whole minutes—that's just measurement granularity, not the underlying nature of time.\n\nOption D is wrong. Both involve numerical measurement (quantitative), not just descriptive categories (qualitative). Qualitative would be 'satisfied/neutral/dissatisfied' without numerical values.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.1.2",
    "estimated_time_seconds": 90
  },
  {
    "module": "2.1",
    "module_name": "Types of Data",
    "question": "A quality inspector examines 100 products and records each as either 'Pass' or 'Fail.' What type of data is this, and what is its primary limitation for improvement efforts?",
    "type": "multiple_choice",
    "options": [
      "This is attribute data (pass/fail categories), which provides less information than variable data and makes it harder to detect small process shifts or understand root causes",
      "This is the most powerful type of data because it's simple and clear",
      "This is continuous data measured on a binary scale",
      "This is qualitative data that cannot be used for statistical analysis"
    ],
    "correct_answer": 0,
    "explanation": "Attribute data (also called discrete categorical data) classifies items into categories—in this case, 'Pass' or 'Fail,' 'Good' or 'Bad,' 'Accept' or 'Reject.' While attribute data is easy to collect and communicate, it has significant limitations for process improvement:\n\n**Primary Limitation—Loss of Information:**\nAttribute data throws away details. Imagine a specification requiring parts between 10.0mm and 10.2mm:\n- Part A: 10.10mm (dead center) → Pass\n- Part B: 10.19mm (near limit) → Pass\n- Part C: 10.21mm (just over limit) → Fail\n\nAttribute data treats A and B identically ('Pass') even though B is drifting toward trouble. It also treats B and C very differently ('Pass' vs 'Fail') even though they're only 0.02mm apart. This masks process drift and makes early warning impossible.\n\n**Secondary Limitations:**\n\n1. **Large sample sizes required**: With only 5% defects, you might need 200+ samples to detect a process shift statistically. Variable data (actual measurements) needs far fewer samples.\n\n2. **Root cause analysis is harder**: Knowing '5 units failed' doesn't tell you whether they failed by a little or a lot, or where in the specification they're clustering. Variable data (actual measurements) reveals patterns.\n\n3. **Less sensitive control charts**: Attribute control charts (p-charts, np-charts) detect shifts more slowly than variable control charts (X-bar and R charts).\n\n4. **Cannot calculate process capability**: You can calculate defect rate, but not Cp/Cpk which require variable data.\n\n**When attribute data makes sense:**\n- Measurement is truly binary (light bulb works or doesn't)\n- Variable measurement is destructive or expensive\n- Quick screening before detailed measurement\n- Customer cares only about pass/fail\n\nWhy other options are wrong:\n\nOption B is wrong—attribute data is simple but not powerful. It's often necessary, but variable data is always more informative when feasible.\n\nOption C is confused—'binary' suggests two categories, but that's still discrete/categorical, not continuous. Continuous means infinite possible values on a scale.\n\nOption D is incorrect—attribute data CAN be analyzed statistically (proportion tests, chi-square tests, p-charts), just with limitations compared to variable data.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.1.3",
    "estimated_time_seconds": 90
  },
  {
    "module": "2.1",
    "module_name": "Types of Data",
    "question": "A Six Sigma project team is deciding what data to collect to improve a manufacturing process. They can either measure the exact diameter of each part (variable data) or simply inspect whether each part is within specification (attribute data). The variable measurement costs $2 per part, while the attribute inspection costs $0.50 per part. What should they consider in this decision?",
    "type": "multiple_choice",
    "options": [
      "Variable data provides more information per measurement, so despite higher cost per part, you typically need far fewer measurements to achieve the same statistical confidence—often making variable data more cost-effective overall",
      "Always choose attribute data since it costs less per measurement",
      "Always choose variable data since it provides more information",
      "The cost difference doesn't matter for data collection decisions"
    ],
    "correct_answer": 0,
    "explanation": "This question addresses a critical real-world tradeoff in Six Sigma projects: cost versus information content. The answer requires understanding that total cost depends on both cost-per-measurement AND number of measurements needed.\n\n**The Statistical Reality:**\n\nVariable data (actual measurements) contains much more information than attribute data (pass/fail). Because of this:\n\n**Sample Size Comparison:**\n- To detect a process shift with 95% confidence:\n  - Variable data might need n=30 samples\n  - Attribute data might need n=200+ samples\n\nThe exact ratio depends on defect rates, but attribute data typically requires 3-10x more samples for equivalent statistical power.\n\n**Total Cost Calculation:**\n\nScenario A - Variable Data:\n- Cost per measurement: $2.00\n- Samples needed: 30\n- Total cost: $60\n- Information gained: High (see actual distribution, calculate Cp/Cpk, identify trends)\n\nScenario B - Attribute Data:\n- Cost per measurement: $0.50\n- Samples needed: 200\n- Total cost: $100\n- Information gained: Lower (only know % passing, no pattern visibility)\n\nVariable data costs more per measurement but less overall AND provides better insights for root cause analysis.\n\n**Additional Variable Data Advantages:**\n- Detects process shifts earlier (before defects occur)\n- Enables process capability analysis (Cp, Cpk)\n- Reveals distribution patterns\n- Better for control charts (more sensitive)\n- Supports correlation analysis\n\n**When Attribute Data Makes Sense Despite Limitations:**\n- Variable measurement is destructive (breaks the part)\n- Variable measurement is impractical (go/no-go gauge is simple, micrometer is complex)\n- Process is very stable with rare defects\n- Initial screening before detailed study\n- Customer requirement is truly binary\n\n**The Strategic Approach:**\nMany Six Sigma projects use a hybrid:\n1. Attribute data for initial screening (cheap, fast)\n2. Variable data for deeper analysis (expensive but informative)\n3. Attribute data for ongoing monitoring (if process is stable)\n\nWhy other options are wrong:\n\nOption B (always attribute) ignores total cost and information value. Cheaper per unit doesn't mean cheaper overall.\n\nOption C (always variable) ignores legitimate constraints. Sometimes attribute is all you can practically measure.\n\nOption D (cost doesn't matter) is unrealistic. Projects have budgets, and resource allocation matters.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.1.4",
    "estimated_time_seconds": 95
  },
  {
    "module": "2.1",
    "module_name": "Types of Data",
    "question": "A process improvement team collects the following data about customer complaints: (1) Number of complaints per month, (2) Type of complaint (billing, quality, delivery, service), (3) Complaint severity rating (1-10), and (4) Resolution time in hours. How would you classify each data type?",
    "type": "multiple_choice",
    "options": [
      "(1) Discrete quantitative, (2) Qualitative categorical, (3) Discrete ordinal, (4) Continuous quantitative",
      "All four are continuous quantitative data since they all involve measurements",
      "(1) and (4) are qualitative, (2) and (3) are quantitative",
      "All four are qualitative since they describe customer experiences"
    ],
    "correct_answer": 0,
    "explanation": "This comprehensive question tests ability to classify multiple data types correctly—a crucial skill for selecting appropriate analysis methods. Let's break down each one:\n\n**(1) Number of complaints per month - Discrete Quantitative:**\nYou count complaints in whole numbers: 15, 16, 17, never 16.5 complaints. It's quantitative (numerical) but discrete (countable integers only). Analysis: Use run charts, control charts (c-chart or u-chart), time series analysis.\n\n**(2) Type of complaint - Qualitative Categorical (Nominal):**\nThis classifies complaints into named categories with no inherent order. Billing isn't 'more' or 'less' than quality—they're just different. Analysis: Use Pareto charts (to identify most frequent types), bar charts, pie charts, cross-tabulations. Calculate frequencies and percentages.\n\n**(3) Complaint severity rating (1-10) - Discrete Ordinal:**\nDiscrete because only whole numbers 1-10 are possible. Ordinal because numbers represent ranked order (10 is more severe than 5), but intervals between ranks may not be equal. Is the difference between severity 9 and 10 the same as between 4 and 5 in customers' actual experience? Probably not. Analysis: Use median (better than mean for ordinal data), mode, frequency distributions, bar charts. Be cautious with averages.\n\n**(4) Resolution time in hours - Continuous Quantitative (Ratio):**\nTime can take any value (2.5 hours, 2.53 hours, etc.) limited only by measurement precision. It's ratio data because zero hours means 'instant resolution,' and ratios are meaningful (6 hours is exactly twice 3 hours). Analysis: Use histograms, calculate mean and standard deviation, control charts (X-bar and R), process capability, correlation with severity.\n\n**Why This Classification Matters:**\n\n**For Visualization:**\n- (1): Line chart over time\n- (2): Pareto chart\n- (3): Bar chart or box plot\n- (4): Histogram or run chart\n\n**For Statistics:**\n- (1): Count-based statistics\n- (2): Frequencies, chi-square tests\n- (3): Median, non-parametric tests\n- (4): Mean, standard deviation, t-tests, regression\n\n**For Relationships:**\nTo analyze if complaint type (2) affects resolution time (4), you'd use ANOVA. To see if severity (3) correlates with resolution time (4), use Spearman rank correlation (not Pearson, since severity is ordinal).\n\nWhy other options are wrong:\n\nOption B lumps everything as continuous, which is wrong. Complaint counts and severity ratings are discrete.\n\nOption C has it backward. Numbers don't automatically mean quantitative, and words don't automatically mean qualitative.\n\nOption D ignores that numbers 1, 3, and 4 are numerical measurements (quantitative), even though they relate to customer experience.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.1.5",
    "estimated_time_seconds": 95
  },
  {
    "module": "2.2",
    "module_name": "Descriptive Statistics",
    "question": "A production line's cycle times for 20 units are: 14, 15, 15, 16, 14, 15, 16, 15, 14, 43, 15, 16, 14, 15, 16, 15, 14, 15, 16, 15 minutes. The mean is 16.75 minutes and the median is 15 minutes. Which measure better represents typical performance, and why?",
    "type": "multiple_choice",
    "options": [
      "The median (15 minutes) better represents typical performance because the mean is inflated by the outlier of 43 minutes, which likely represents a special cause (machine jam, etc.) rather than normal operation",
      "The mean (16.75 minutes) is always more accurate than the median for any dataset",
      "Both measures are equally valid and will always give the same result",
      "Neither measure is useful; you should only look at individual data points"
    ],
    "correct_answer": 0,
    "explanation": "This question illustrates a critical principle: the mean is sensitive to outliers, while the median is robust against them. Understanding this difference helps you interpret process data correctly and avoid misleading conclusions.\n\n**Why the Median is Better Here:**\n\n**The Data Pattern:**\nLooking at the 20 values, 19 of them cluster tightly between 14-16 minutes. But one value (43 minutes) is wildly different—nearly 3x the typical time. This outlier represents a special cause: perhaps the machine jammed, an operator took a break, or a quality issue required rework.\n\n**Mean Calculation:**\nSum = 335 minutes ÷ 20 = 16.75 minutes\nThe single outlier (43) adds 27 extra minutes to the sum, pulling the average up significantly. The mean suggests typical cycle time is nearly 17 minutes, but 19 of 20 units took 14-16 minutes!\n\n**Median Calculation:**\nSort the data, find the middle value (average of 10th and 11th values when sorted). Median = 15 minutes.\nThis accurately reflects what happens in most cycles because it's not affected by the extreme outlier.\n\n**Process Improvement Implications:**\n\n**If you use the mean:**\n- You might incorrectly believe the process typically runs at 16.75 minutes\n- You might set wrong targets or capacity plans\n- You miss that the process is actually quite consistent (14-16) except for special causes\n\n**If you use the median:**\n- You correctly identify typical performance (15 minutes)\n- You can separate normal variation (14-16) from special causes (43)\n- You focus improvement efforts appropriately: stabilize the process by eliminating special causes\n\n**When to Use Each Measure:**\n\n**Use Median when:**\n- Data has outliers or is skewed\n- You want a typical value resistant to extremes\n- Distribution is not normal\n- Examples: Income data, real estate prices, cycle times with occasional delays\n\n**Use Mean when:**\n- Data is normally distributed without outliers\n- You care about total (sum matters, like total production time)\n- Data is symmetric\n- For statistical tests requiring normal distribution\n\n**Best Practice in Six Sigma:**\nCalculate BOTH mean and median. If they're very different (like here: 16.75 vs 15), investigate:\n- Are outliers special causes that should be addressed separately?\n- Is the distribution skewed?\n- Should you remove outliers before calculating capability?\n\nWhy other options are wrong:\n\nOption B is absolutely wrong. The mean is NOT always more accurate—it's more affected by outliers, which makes it misleading when outliers are present.\n\nOption C is false. Mean and median are often different, especially with skewed data or outliers. That's precisely why we have both measures!\n\nOption D throws out useful summary statistics. While individual data points matter, summary measures help communicate patterns and central tendency efficiently.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.2.4",
    "estimated_time_seconds": 90
  },
  {
    "module": "2.2",
    "module_name": "Descriptive Statistics",
    "question": "Two production lines make identical products. Line A has a mean cycle time of 10 minutes with standard deviation of 2 minutes. Line B has a mean of 12 minutes with standard deviation of 0.5 minutes. Which line is 'better' from a Six Sigma perspective?",
    "type": "multiple_choice",
    "options": [
      "It depends on customer requirements and priorities—Line A is faster on average but less predictable; Line B is slower but much more consistent, which might be more valuable",
      "Line A is always better because it has a lower mean (faster)",
      "Line B is always better because it has lower standard deviation (more consistent)",
      "Both lines are equally good because they both produce the same product"
    ],
    "correct_answer": 0,
    "explanation": "This question reveals a fundamental Six Sigma concept: the tension between average performance (mean) and consistency (standard deviation), and why the answer depends on context and customer requirements.\n\n**Line A Analysis:**\n- Mean: 10 minutes (faster average)\n- Std Dev: 2 minutes (high variation)\n- Range: Roughly 6-14 minutes (±2 sigma covers ~95% of data)\n- Character: Fast but unpredictable\n\n**Line B Analysis:**\n- Mean: 12 minutes (slower average)\n- Std Dev: 0.5 minutes (very consistent)\n- Range: Roughly 11-13 minutes (±2 sigma covers ~95% of data)\n- Character: Slower but very predictable\n\n**Why Context Matters:**\n\n**Scenario 1: Line A Might Be Better**\nIf customer requirement is 'deliver within 15 minutes' and average throughput matters most:\n- Line A averages 10 minutes (20% faster), meeting requirements with room to spare\n- Even with variation (6-14 minutes), nearly all units meet the 15-minute spec\n- Higher throughput = more revenue\n\n**Scenario 2: Line B Might Be Better**\nIf customer requirement is 'deliver within 11 minutes' or if predictability is critical:\n- Line A: Many cycles exceed 11 minutes (roughly 31% will be >12 minutes)\n- Line B: Nearly all cycles are 11-13 minutes, with very few exceeding 13\n- Line B's predictability enables better scheduling, reduced buffer inventory, happier downstream operations\n\n**Scenario 3: Just-In-Time Manufacturing**\nIf this line feeds a synchronized production system:\n- Line A's variability (6-14 minutes) would cause either waiting (downstream starved) or inventory buildup\n- Line B's consistency (11-13 minutes) enables smooth flow\n- Predictability might matter more than speed\n\n**Six Sigma Teaches:**\n1. **Reducing variation is often more valuable than improving the mean** because variation causes defects, delays, and customer dissatisfaction\n2. **'Good on average' isn't good enough** if variation causes problems\n3. **Process capability (Cp, Cpk) considers BOTH mean and variation** relative to specifications\n\n**What Makes a Process 'Good':**\n- Mean close to target\n- Variation low enough that nearly all output meets specifications\n- Both matter!\n\n**Improvement Strategy:**\nIdeal situation: Take Line A (faster average) and apply Six Sigma tools to reduce its variation to Line B levels (0.5 minute std dev). Result: 10-minute average with 9.5-10.5 minute consistency. Best of both worlds!\n\n**The Formula for This:**\nProcess Capability Index (Cpk) considers both mean and variation relative to specifications. It would show which line actually meets customer requirements better.\n\nWhy other options are wrong:\n\nOption B ignores variation entirely. Faster average doesn't help if inconsistency causes late deliveries or quality issues.\n\nOption C ignores average performance. You can be consistently slow and still disappoint customers.\n\nOption D misses the point completely. Making the same product doesn't mean both processes are equally good at making it. Performance and consistency differ dramatically.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.2.3",
    "estimated_time_seconds": 95
  },
  {
    "module": "2.2",
    "module_name": "Descriptive Statistics",
    "question": "A call center tracks call duration for 100 calls. They calculate: Mean = 8.5 minutes, Median = 7 minutes, Mode = 6 minutes, Range = 3 to 35 minutes, Standard Deviation = 4.2 minutes. What does this tell you about the distribution of call times?",
    "type": "multiple_choice",
    "options": [
      "The distribution is right-skewed (positively skewed) with most calls being relatively short but some very long calls pulling the mean upward; this suggests different call types or complex issues requiring extra time",
      "The distribution is perfectly normal (bell curve) since all statistics are present",
      "The data is wrong because mean should always equal median and mode",
      "The distribution tells us nothing useful; only individual call times matter"
    ],
    "correct_answer": 0,
    "explanation": "This question teaches how to interpret multiple descriptive statistics together to understand distribution shape—a critical skill for process analysis. The pattern of statistics reveals important process characteristics:\n\n**Key Observation: Mode < Median < Mean**\n- Mode: 6 minutes (most frequent value)\n- Median: 7 minutes (middle value)\n- Mean: 8.5 minutes (average)\n\nThis ascending pattern is the signature of right-skewed (positively skewed) distribution.\n\n**What Right-Skewed Means:**\nMost calls cluster at the low end (6-7 minutes), but a tail of longer calls extends to the right. The range confirms this: while the minimum is 3 minutes, the maximum is 35 minutes. Those long calls (outliers on the right) pull the mean upward but don't affect the median or mode.\n\n**Visual Interpretation:**\nIf you graphed this as a histogram:\n- Tall bars at 6-8 minutes (where most calls are)\n- Progressively shorter bars extending to the right\n- A few calls out at 20-35 minutes\n- Shape: Peak on left, tail extending right\n\n**Process Insights:**\n\n**Why is this distribution skewed? Several possibilities:**\n\n1. **Different call types mixed together:**\n   - Simple questions: 5-8 minutes (most calls)\n   - Technical support: 15-25 minutes (fewer calls)\n   - Escalated issues: 30+ minutes (rare calls)\n\n2. **Skill variation:**\n   - Experienced agents: 5-7 minutes\n   - New agents: 15-20 minutes\n   - Complex customer issues: 25+ minutes\n\n3. **Customer variation:**\n   - Standard inquiries: 6 minutes (mode)\n   - Upset customers requiring extra care: 20+ minutes\n\n**Improvement Opportunities:**\n\n**Strategy 1: Segment the data**\nSeparate call types and analyze each distribution individually. You might find:\n- 'Order status' calls: Mean = 3 minutes, very consistent\n- 'Technical support' calls: Mean = 18 minutes, high variation\n\nThis focused analysis enables targeted improvements.\n\n**Strategy 2: Address the long tail**\nThose calls taking 20-35 minutes are:\n- Pulling up your average (making performance look worse)\n- Frustrating customers (long wait, long call)\n- Occupying agents (reducing capacity)\n\nRoot cause analysis on long calls might reveal:\n- Need for better knowledge base\n- System issues causing delays\n- Training gaps\n- Certain call types that should be handled differently\n\n**Strategy 3: Understand the 'typical' call**\nMean (8.5 min) is misleading—most calls are actually 6-7 minutes. Use median or mode for capacity planning and staffing.\n\n**Standard Deviation (4.2 minutes):**\nThis is quite large relative to the mean, confirming high variability. In a consistent process, standard deviation would be much smaller.\n\nWhy other options are wrong:\n\nOption B is wrong. Normal distributions have Mode = Median = Mean. When these differ, the distribution is skewed, not normal.\n\nOption C shows misunderstanding. Mean, median, and mode are equal only in perfectly symmetric distributions. Real-world data is often skewed.\n\nOption D misses the value of summary statistics. While individual data points matter, these statistics efficiently communicate distribution patterns that guide improvement efforts.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.2.1",
    "estimated_time_seconds": 95
  },
  {
    "module": "2.2",
    "module_name": "Descriptive Statistics",
    "question": "A Six Sigma team measures a critical dimension on 50 parts. Using Excel, they calculate: Mean = 10.00mm, Standard Deviation = 0.30mm. The specification limits are 9.50mm to 10.50mm. What does the standard deviation of 0.30mm tell them?",
    "type": "multiple_choice",
    "options": [
      "Standard deviation of 0.30mm means most parts (about 68%) fall within 9.70-10.30mm, and nearly all parts (99.7%) fall within 9.10-10.90mm—this indicates the process could produce some out-of-spec parts above 10.50mm",
      "Standard deviation tells us nothing useful because only the mean matters",
      "Standard deviation of 0.30mm means all parts are exactly 0.30mm away from 10.00mm",
      "The specification limits don't matter if the mean is at the target"
    ],
    "correct_answer": 0,
    "explanation": "Standard deviation is one of Six Sigma's most important metrics because it quantifies process variation—and variation is the enemy of quality. Understanding what standard deviation means in practical terms is crucial for process capability assessment.\n\n**Understanding Standard Deviation (σ = 0.30mm):**\n\nStandard deviation measures the typical distance of data points from the mean. For approximately normal distributions, the empirical rule (68-95-99.7 rule) applies:\n\n**The 68-95-99.7 Rule:**\n\n**±1 sigma (68%):** About 68% of parts fall within mean ± 1σ\n- Range: 10.00 ± 0.30 = 9.70 to 10.30mm\n- Interpretation: Most parts are within this range\n\n**±2 sigma (95%):** About 95% of parts fall within mean ± 2σ\n- Range: 10.00 ± 0.60 = 9.40 to 10.60mm\n- Interpretation: Occasional parts outside this range\n\n**±3 sigma (99.7%):** About 99.7% of parts fall within mean ± 3σ\n- Range: 10.00 ± 0.90 = 9.10 to 10.90mm\n- Interpretation: Rare parts outside this range\n\n**Comparing to Specifications:**\n\nSpecification: 9.50 to 10.50mm\nProcess spread (±3σ): 9.10 to 10.90mm\n\n**The Problem:**\nThe process spread (9.10-10.90) is WIDER than the specification window (9.50-10.50). This means:\n\n- Lower spec (9.50mm): Process goes down to 9.10mm, so some parts will be below spec\n- Upper spec (10.50mm): Process goes up to 10.90mm, so some parts will be above spec\n\n**Estimated Defect Rate:**\nSince the mean is centered at 10.00mm (the middle of the 9.50-10.50 spec range), and the process spread exceeds the spec by about 0.40mm on each side, we'd expect defects on both ends.\n\nUsing normal distribution tables:\n- Upper spec: (10.50 - 10.00) / 0.30 = 1.67 sigma away\n- Parts above 10.50mm: approximately 4.8%\n- By symmetry, approximately 4.8% below 9.50mm\n- Total defect rate: roughly 9-10%\n\n**This is approximately 2-sigma process performance—far short of Six Sigma (3.4 defects per million).**\n\n**Improvement Strategies:**\n\n1. **Reduce variation (decrease σ):**\n   - Identify and eliminate sources of variation\n   - Target: σ < 0.17mm would achieve 6-sigma performance\n   - Tools: Cause-and-effect analysis, DOE, mistake-proofing\n\n2. **Keep mean centered:**\n   - Mean at 10.00mm is good (centered)\n   - Must maintain this while reducing variation\n   - Use control charts to monitor\n\n**Process Capability Indices:**\nThis situation would be expressed as:\n- Cp = (USL - LSL) / (6σ) = (10.50 - 9.50) / (6 × 0.30) = 0.56\n- Cp < 1.0 indicates process incapable\n- Need Cp ≥ 1.33 for acceptable process, ≥ 2.0 for Six Sigma\n\nWhy other options are wrong:\n\nOption B is dangerous. Mean without standard deviation tells you where you're centered but nothing about spread. You could be centered perfectly but producing 50% defects if variation is high.\n\nOption C misunderstands standard deviation completely. It's not a fixed distance—it's a measure of spread. Individual parts vary around the mean by different amounts.\n\nOption D is wrong. Even with mean perfectly on target, high variation causes defects. Six Sigma focuses on reducing variation to achieve nearly perfect quality.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.2.2",
    "estimated_time_seconds": 100
  },
  {
    "module": "2.2",
    "module_name": "Descriptive Statistics",
    "question": "A team collects process data and calculates statistics. They find: Dataset A has Range = 20, while Dataset B has Range = 5. Both datasets have 100 data points. What does this tell you about using Range versus Standard Deviation to measure variation?",
    "type": "multiple_choice",
    "options": [
      "Range only uses two data points (highest and lowest) and ignores all 98 other values, while standard deviation uses all 100 points, making standard deviation a much better measure of variation for large datasets",
      "Range is always better than standard deviation because it's simpler to calculate",
      "Range and standard deviation always give you the exact same information",
      "Neither range nor standard deviation is useful; you should only look at individual values"
    ],
    "correct_answer": 0,
    "explanation": "This question highlights a critical difference between two measures of variation: Range (simplest but limited) versus Standard Deviation (more complex but comprehensive). Understanding this difference is essential for proper process analysis.\n\n**Range - Simple but Limited:**\n\nRange = Maximum value - Minimum value\n\nFor Dataset A: Range = 20 means the difference between highest and lowest is 20 units.\n\n**The Problem with Range:**\n\nRange uses only 2 of the 100 data points! It tells you nothing about:\n- How the other 98 points are distributed\n- Whether most values cluster near the middle or spread evenly\n- Whether there are outliers\n- The typical deviation from the average\n\n**Example Scenarios—Same Range, Very Different Distributions:**\n\n**Scenario 1:** Dataset with Range = 20\n- 98 values clustered tightly between 10.0 and 10.2\n- 1 outlier at 0 (due to measurement error)\n- 1 outlier at 20 (due to special cause)\n- Range = 20, but the process is actually very stable (ignoring outliers)\n\n**Scenario 2:** Dataset with Range = 20\n- Values spread evenly from 0 to 20\n- No clustering, high variation throughout\n- Range = 20, and the process is genuinely unstable\n\nRange = 20 in both cases, but these are completely different process conditions!\n\n**Standard Deviation - Comprehensive and Informative:**\n\nStandard deviation uses ALL 100 data points. It calculates:\n1. How far each point deviates from the mean\n2. Squares those deviations (so direction doesn't cancel out)\n3. Averages the squared deviations\n4. Takes the square root (to get back to original units)\n\nResult: A measure that reflects the typical deviation, influenced by every single data point.\n\n**For Scenario 1 above:**\nStandard deviation would be small (maybe 1-2) because 98 points cluster tightly, and outliers have limited influence on standard deviation.\n\n**For Scenario 2 above:**\nStandard deviation would be large (maybe 6-8) because points spread widely across the entire range.\n\nSame range, very different standard deviations—revealing the true difference in process behavior.\n\n**When Range is Useful:**\n\n1. **Small sample sizes (n<10):** For very small samples, range is reasonable and simple\n2. **Quick screening:** Fast check of spread\n3. **Control charts:** R-charts (range charts) used with X-bar charts for small subgroups (n=2-9)\n4. **Communication:** Easy to explain to non-statistical audiences\n\n**When Standard Deviation is Better:**\n\n1. **Large sample sizes (n≥30):** Uses all information, not just extremes\n2. **Process capability:** Cp and Cpk require standard deviation\n3. **Comparing variation:** More reliable for comparison between processes\n4. **Statistical tests:** Most tests (t-tests, ANOVA) use standard deviation\n5. **Outlier presence:** Less affected by extreme outliers than range\n\n**Six Sigma Perspective:**\n\nSix Sigma almost always uses standard deviation (σ) because:\n- It's more accurate for large datasets\n- Process capability indices (Cp, Cpk) require it\n- The entire 'sigma level' concept is based on standard deviation\n- More mathematically rigorous\n\n**Practical Guideline:**\n- Samples of 2-9: Use range (simple, adequate)\n- Samples of 10-30: Either works, standard deviation preferred\n- Samples of 30+: Use standard deviation (much better)\n\nWhy other options are wrong:\n\nOption B prioritizes simplicity over accuracy. For large datasets, simple-but-wrong is worse than complex-but-right. Excel calculates standard deviation instantly anyway.\n\nOption C is completely wrong. Range and standard deviation often differ dramatically. Range ignores distribution, standard deviation captures it.\n\nOption D throws out valuable summary statistics. While individual values matter, summary measures help identify patterns efficiently.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.2.5",
    "estimated_time_seconds": 100
  },
  {
    "module": "2.3",
    "module_name": "Data Visualization",
    "question": "A Six Sigma team wants to communicate which defect types cause the most problems. They have data on 8 defect types with frequencies: scratches (45), dents (32), cracks (18), discoloration (12), chips (9), warping (6), contamination (4), other (2). Which chart type is most appropriate?",
    "type": "multiple_choice",
    "options": [
      "Pareto chart showing defect types in descending frequency order with a cumulative percentage line, clearly identifying the 'vital few' defects that cause most problems",
      "Pie chart showing each defect type as a slice",
      "Line graph showing defects over time",
      "Scatter plot comparing defect types"
    ],
    "correct_answer": 0,
    "explanation": "This question addresses one of Six Sigma's most powerful visualization tools: the Pareto chart. Understanding when and why to use Pareto charts is essential for prioritizing improvement efforts.\n\n**Why Pareto Chart is Best:**\n\nThe Pareto chart is specifically designed for this situation—identifying the 'vital few' causes that account for most problems. It's based on the Pareto Principle (80/20 rule): typically, 80% of problems come from 20% of causes.\n\n**Pareto Chart Components:**\n\n1. **Bars in descending order (left to right):**\n   - Scratches: 45 (35%)\n   - Dents: 32 (25%)\n   - Cracks: 18 (14%)\n   - Discoloration: 12 (9%)\n   - Chips: 9 (7%)\n   - Warping: 6 (5%)\n   - Contamination: 4 (3%)\n   - Other: 2 (2%)\n\n2. **Cumulative percentage line:**\n   - After scratches: 35%\n   - After dents: 60%\n   - After cracks: 74%\n   - After discoloration: 83%\n   - After chips: 90%\n   - (continuing to 100%)\n\n**Key Insight from Pareto:**\nThe top 3 defect types (scratches, dents, cracks) account for 74% of all defects! This is the 'vital few.'\n\nThe bottom 5 defect types collectively account for only 26% of defects. This is the 'trivial many.'\n\n**Decision Impact:**\n**Without Pareto thinking:** Team might work on all 8 defect types equally, spreading resources thin.\n\n**With Pareto analysis:** Team focuses intensively on scratches, dents, and cracks. Even if they completely eliminate these three, they address 74% of the problem. This focused approach delivers maximum impact from limited resources.\n\n**Six Sigma Application:**\n\n1. **Define Phase:** Pareto identifies which problems to tackle\n2. **Analyze Phase:** Pareto shows which causes drive the problem\n3. **Improve Phase:** Pareto prioritizes which solutions to implement\n4. **Control Phase:** Pareto monitors whether problems stay solved\n\n**Construction Rules:**\n\n1. **Sort descending:** Always arrange bars from highest to lowest (left to right)\n2. **Cumulative line:** Shows total accumulation percentage\n3. **80% line:** Optional reference line at 80% to visually mark the vital few\n4. **'Other' category:** If you have many small categories, combine them into 'Other' (but place it last, not in descending order)\n\n**Why Other Charts Don't Work as Well:**\n\n**Pie Chart (Option B):**\n- Slices aren't ordered (harder to compare)\n- Can't show cumulative percentage\n- Hard to see relative sizes when many slices\n- Doesn't emphasize the vital few\n- Research shows humans struggle to compare slice areas accurately\n\nPie charts are fine for 3-5 large categories, but for 8 categories where you want to prioritize, Pareto is superior.\n\n**Line Graph (Option C):**\nThis shows trends over time, not relative frequencies of categories. Useful if you want to track whether scratch defects are increasing, but not for comparing defect types to each other.\n\n**Scatter Plot (Option D):**\nShows relationships between two continuous variables (e.g., temperature vs. pressure). Defect types are categories, not continuous variables. Scatter plot is wrong chart type for categorical data.\n\n**Real-World Example:**\n\nAfter creating this Pareto chart, the team investigates scratches (the #1 problem):\n- Root cause: Parts rubbing against metal fixture during transport\n- Solution: Add rubber padding to fixture\n- Cost: $200\n- Benefit: Eliminate 45 of 128 defects (35% reduction)\n- ROI: Payback in 1 week\n\nThis focused effort delivers massive impact quickly—the power of Pareto thinking.\n\nWhy other options are wrong:\n\nOption B (pie chart): While technically possible, pie charts don't show cumulative impact or emphasize prioritization the way Pareto does.\n\nOption C (line graph): Wrong chart type—this is categorical data, not time series data.\n\nOption D (scatter plot): Wrong chart type—scatter plots are for continuous variables, not categories.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.3.1",
    "estimated_time_seconds": 95
  },
  {
    "module": "2.3",
    "module_name": "Data Visualization",
    "question": "A manager presents a chart showing productivity improvements. The y-axis starts at 95% instead of 0%, making a change from 96% to 98% look dramatic. What principle of data visualization is being violated?",
    "type": "multiple_choice",
    "options": [
      "Using a truncated y-axis exaggerates the visual impact of small changes, potentially misleading the audience about the magnitude of improvement; axes should start at zero (or clearly indicate truncation) for honest communication",
      "Axes should never start at zero because that wastes space",
      "The chart is perfect as-is because it emphasizes the improvement",
      "Y-axis scaling doesn't matter as long as the numbers are labeled"
    ],
    "correct_answer": 0,
    "explanation": "This question addresses one of the most common ways data visualizations mislead audiences: manipulating axis scaling to exaggerate or minimize changes. Ethical data visualization requires honest representation of magnitude.\n\n**The Problem with Truncated Axes:**\n\n**Scenario: Productivity improvement from 96% to 98%**\n\n**Chart A - Y-axis from 95% to 100%:**\nThe 2% improvement appears to take up 40% of the vertical space (2 out of 5 percentage points shown). Visually, this looks like a massive jump—the line or bar shoots upward dramatically.\n\n**Chart B - Y-axis from 0% to 100%:**\nThe 2% improvement appears as a tiny change at the top of the chart. Visually accurate—it's a 2% change out of 100%, which is modest.\n\n**Why This Matters:**\n\nHuman brains process visual magnitude quickly and intuitively. When you see a bar that's twice as tall as another, you instinctively think 'twice as much.' When the y-axis doesn't start at zero, the visual impression doesn't match the actual numbers.\n\n**Example—Productivity Chart:**\n\n**With truncated axis (95-100%):**\n- Visual impression: 'Wow, productivity nearly doubled!'\n- Actual change: 2 percentage points (2% improvement)\n- Percentage change: 2.08% increase (98/96 = 1.0208)\n\n**With full axis (0-100%):**\n- Visual impression: 'Small improvement'\n- Actual change: 2 percentage points (2% improvement)\n- Percentage change: 2.08% increase\n\nThe visual now matches reality.\n\n**When Truncated Axes Are Acceptable (with caveats):**\n\n1. **Small variations matter in context:**\n   - Blood glucose: 95-105 mg/dL (normal range is narrow)\n   - Stock prices: Small percentage moves mean billions of dollars\n   - Manufacturing: When operating at 96-99% yield, small changes are significant\n\n2. **IF you truncate, you MUST:**\n   - Clearly mark the axis break (zigzag line at bottom)\n   - Explicitly state in the title: 'Y-axis starts at 95%'\n   - Use annotation: 'Note: Axis does not start at zero'\n   - Consider if you're emphasizing the right thing\n\n**Best Practices for Honest Visualization:**\n\n**For Bar Charts:**\n- ALWAYS start at zero (bars represent magnitude visually)\n- Truncating bar charts is almost never acceptable\n- Exception: Very rare situations with narrow ranges where context makes truncation obvious\n\n**For Line Graphs:**\n- Usually start at zero, but truncation is more acceptable if:\n  - You show axis break\n  - The focus is on trend/change rather than absolute magnitude\n  - Context makes the scale clear\n\n**For Time Series:**\n- Sometimes truncation helps see patterns in noisy data\n- But clearly indicate it's truncated\n\n**The Manager's Intent:**\n\nIn this scenario, the manager likely wants to make the improvement look impressive (perhaps for a presentation to executives, to justify their department, or to celebrate team success). While the intention might not be malicious, the effect is misleading.\n\n**Ethical Consideration:**\nAsk: 'Am I choosing this visualization to help my audience understand reality, or to manipulate their perception?' If the latter, you're crossing into chart junk or data manipulation territory.\n\n**Better Approach:**\nIf a 2% improvement truly matters in context, explain WHY:\n- 'In our industry, 96% is the standard. Reaching 98% puts us in the top quartile of performers.'\n- 'This 2% improvement translates to $500K annual savings.'\n- 'We've been stuck at 96% for 3 years; reaching 98% breaks through a plateau.'\n\nContext and explanation are more honest than visual manipulation.\n\n**Six Sigma Perspective:**\n\nSix Sigma emphasizes data-driven decision-making. That includes honest data presentation. When you manipulate visualizations to exaggerate results, you:\n- Undermine trust in your data\n- May lead to incorrect decisions\n- Violate the spirit of continuous improvement (honest assessment required)\n\nWhy other options are wrong:\n\nOption B is completely wrong. While zooming in can show detail, you must do so honestly with clear labeling.\n\nOption C endorses deception. Even if unintentional, exaggerating improvement through axis manipulation is misleading.\n\nOption D ignores human psychology. People see the visual first, numbers second. Visual representation must be honest.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.3.3",
    "estimated_time_seconds": 100
  },
  {
    "module": "2.3",
    "module_name": "Data Visualization",
    "question": "A team wants to show how cycle time relates to temperature in their process. They have 50 data points, each with a temperature reading and corresponding cycle time. What chart type is most appropriate?",
    "type": "multiple_choice",
    "options": [
      "Scatter plot with temperature on the x-axis and cycle time on the y-axis, which reveals whether a relationship exists and whether it's linear or non-linear",
      "Bar chart showing temperatures as categories",
      "Pie chart dividing cycle times into slices",
      "Two separate histograms, one for temperature and one for cycle time"
    ],
    "correct_answer": 0,
    "explanation": "This question introduces the scatter plot—a fundamental tool for discovering relationships between two continuous variables. Understanding when and how to use scatter plots is essential for root cause analysis and process optimization.\n\n**Why Scatter Plot is Correct:**\n\n**Purpose: Reveal relationships between two continuous variables**\n\nIn this case:\n- X-axis (independent variable): Temperature (continuous—could be 75.2°F, 75.3°F, etc.)\n- Y-axis (dependent variable): Cycle time (continuous—could be 12.5 minutes, 12.7 minutes, etc.)\n- Each of 50 data points appears as a dot at coordinates (temperature, cycle time)\n\n**What the Scatter Plot Reveals:**\n\n**Pattern 1: Positive Correlation**\nDots trend upward from left to right:\n- As temperature increases, cycle time increases\n- Interpretation: Higher temperature slows the process\n- Action: Control temperature to reduce cycle time\n\n**Pattern 2: Negative Correlation**\nDots trend downward from left to right:\n- As temperature increases, cycle time decreases\n- Interpretation: Higher temperature speeds the process (up to a point)\n- Action: Optimize temperature for faster cycle time (but watch for other issues)\n\n**Pattern 3: No Correlation**\nDots scattered randomly with no pattern:\n- Temperature doesn't affect cycle time\n- Interpretation: Look elsewhere for cycle time drivers\n- Action: Investigate other variables\n\n**Pattern 4: Non-Linear Relationship**\nDots form a curve:\n- Example: Cycle time decreases as temperature rises from 60-80°F, then increases again above 80°F\n- Interpretation: There's an optimal temperature range\n- Action: Maintain temperature in the optimal zone\n\n**Pattern 5: Clusters**\nDots form distinct groups:\n- Example: One cluster at low temp/fast cycle, another at high temp/slow cycle\n- Interpretation: Might represent different operators, shifts, or product types\n- Action: Investigate why clusters exist\n\n**Six Sigma Application:**\n\n**Analyze Phase—Root Cause Investigation:**\n\nScatter plots help test hypotheses:\n- Hypothesis: 'Temperature affects cycle time'\n- Method: Create scatter plot\n- Result: If dots show pattern, hypothesis supported; if random, hypothesis rejected\n\nYou might create multiple scatter plots:\n- Cycle time vs. Temperature\n- Cycle time vs. Humidity\n- Cycle time vs. Material age\n- Cycle time vs. Operator experience\n\nThose showing strongest patterns point to likely root causes.\n\n**Advanced Analysis:**\n\nOnce you see a relationship in a scatter plot, you can:\n1. **Add trend line:** Shows strength and direction of relationship\n2. **Calculate correlation coefficient (r):** Quantifies relationship strength (-1 to +1)\n3. **Perform regression analysis:** Creates equation predicting cycle time from temperature\n4. **Add control limits:** Identify outliers that don't fit the pattern\n\n**Why Other Charts Don't Work:**\n\n**Bar Chart (Option B):**\nBar charts compare categories (North region vs. South region, Product A vs. Product B). Temperature is continuous, not categorical. While you COULD artificially create categories ('60-70°F' vs. '70-80°F'), you'd lose information about the exact relationship and might miss non-linear patterns.\n\n**Pie Chart (Option C):**\nPie charts show parts of a whole (how does each piece contribute to 100%). You're not showing parts of a whole—you're showing relationship between two variables. Completely wrong chart type.\n\n**Two Separate Histograms (Option D):**\nHistograms show the distribution of a single variable:\n- Temperature histogram: Shows how often each temperature range occurred\n- Cycle time histogram: Shows how often each cycle time range occurred\n\nBut separate histograms don't show RELATIONSHIP between variables. You can't tell if high temperature corresponds to high cycle time.\n\n**Example—Two histograms:**\n- Histogram 1: Temperatures mostly between 70-80°F\n- Histogram 2: Cycle times mostly between 10-15 minutes\n\nSo what? You don't know if the 15-minute cycle times happened at 70°F or 80°F.\n\n**Scatter plot:**\nDots at (70°F, 10min), (75°F, 12min), (80°F, 15min) reveal: As temp increases, cycle time increases!\n\n**Real Example:**\n\nA plastic molding process shows cycle time varying from 8-16 minutes. Engineers suspect temperature affects cycle time.\n\n**Without scatter plot:**\nEndless speculation: 'I think it's temperature.' 'No, I think it's pressure.' 'Maybe humidity?'\n\n**With scatter plot:**\nClear pattern: cycle time increases linearly with temperature from 70-90°F. Root cause identified in minutes, not weeks.\n\n**Solution:** Install better temperature control. Cycle time variation drops from 8 minutes to 2 minutes.\n\nWhy other options are wrong:\n\nOption B (bar chart): Wrong tool—temperature is continuous, not categorical.\n\nOption C (pie chart): Completely inappropriate—not showing parts of a whole.\n\nOption D (separate histograms): Shows distributions but not relationships—misses the point entirely.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.3.1",
    "estimated_time_seconds": 100
  },
  {
    "module": "2.3",
    "module_name": "Data Visualization",
    "question": "When creating a histogram to show the distribution of process cycle times, what is the primary challenge and why does it matter?",
    "type": "multiple_choice",
    "options": [
      "Choosing the right number and width of bins (bars) is critical—too few bins hide important patterns in the distribution, while too many bins create noise that obscures the overall shape",
      "Histograms are always easy to create and interpret with no decisions required",
      "The number of bins doesn't affect the histogram's usefulness at all",
      "Histograms should always have exactly 10 bins regardless of the data"
    ],
    "correct_answer": 0,
    "explanation": "This question addresses a subtle but important aspect of creating histograms: bin selection significantly affects how clearly you can see distribution patterns. Understanding this helps you create more effective visualizations.\n\n**What is a Histogram?**\n\nA histogram shows the distribution of continuous data by grouping values into ranges (bins) and displaying frequency counts as bars. It answers: 'How are my values distributed?'\n\n**The Bin Selection Challenge:**\n\n**Example Data:** 100 cycle time measurements ranging from 10 to 20 minutes\n\n**Scenario A: Too Few Bins (only 2 bins)**\n\nBin 1: 10-15 minutes (60 counts)\nBin 2: 15-20 minutes (40 counts)\n\nResult: You see a basic split, but miss important details:\n- Is there a spike at 12 minutes?\n- Is the distribution normal or bimodal?\n- Are there outliers?\n- Where's the central tendency?\n\nToo few bins create a crude, low-resolution view that hides patterns.\n\n**Scenario B: Too Many Bins (50 bins)**\n\nBin 1: 10.0-10.2 minutes (3 counts)\nBin 2: 10.2-10.4 minutes (1 count)\nBin 3: 10.4-10.6 minutes (0 counts)\n...(continuing)\n\nResult: Jagged, spiky histogram full of noise. Hard to see overall shape because random variation dominates. You can't distinguish meaningful patterns from sampling noise.\n\n**Scenario C: Right Number of Bins (about 8-12 bins)**\n\nBins at reasonable intervals (e.g., 1-minute width):\n- 10-11 minutes (5 counts)\n- 11-12 minutes (15 counts)\n- 12-13 minutes (25 counts)\n- 13-14 minutes (30 counts)\n- 14-15 minutes (15 counts)\n- 15-16 minutes (7 counts)\n- 16-17 minutes (2 counts)\n- 17-18 minutes (1 count)\n\nResult: Clear pattern emerges:\n- Approximately normal distribution\n- Peak around 13-14 minutes\n- Slight right skew (tail extending toward higher values)\n- Most values between 11-16 minutes\n- A couple potential outliers at 17-18 minutes\n\nThis is the 'Goldilocks' number of bins—not too few, not too many, just right.\n\n**Guidelines for Choosing Bin Count:**\n\n**Sturges' Rule (simple):**\nNumber of bins ≈ 1 + 3.322 × log₁₀(n)\nWhere n = sample size\n\nFor 100 data points: 1 + 3.322 × log₁₀(100) = 1 + 6.64 ≈ 7-8 bins\n\n**Square Root Rule (simpler):**\nNumber of bins ≈ √n\n\nFor 100 data points: √100 = 10 bins\n\n**Practical Approach:**\n1. Start with 10-12 bins\n2. Look at the histogram\n3. If too jagged, reduce bins\n4. If too smooth (hiding detail), increase bins\n5. Try 2-3 different bin counts to see what reveals patterns best\n\n**Most software (Excel, Minitab) provides options:**\n- Automatic binning (software chooses)\n- Manual bin count specification\n- Custom bin width definition\n\nExperiment to find what communicates best.\n\n**Why Bin Choice Matters for Process Improvement:**\n\n**Case 1: Bimodal Distribution Hidden by Too Few Bins**\n\nWith 5 bins: Looks like normal distribution\nWith 10 bins: Reveals two peaks (bimodal)\n\n**Interpretation:** Process has two operating modes\n- Investigation reveals: Day shift (fast) vs. night shift (slow), or Product A vs. Product B mixed together\n- Action: Separate the data by shift/product and analyze each distribution\n- Result: Targeted improvements for each mode\n\nToo few bins would have missed this critical insight.\n\n**Case 2: Outliers Masked by Too Many Bins**\n\nWith 50 bins: Noisy, hard to see anything\nWith 10 bins: Clear normal shape with one bin way out at the right (outlier)\n\n**Interpretation:** Process is stable except for occasional special causes\n- Investigation: That outlier bin represents machine jams\n- Action: Fix jam prevention, not the entire process\n- Result: Remove special causes, process becomes capable\n\nToo many bins would have obscured the outlier in noise.\n\n**Six Sigma Application:**\n\nHistograms are essential in:\n\n**Measure Phase:** Visualize current process distribution\n**Analyze Phase:** Compare distributions before/after changes\n**Control Phase:** Verify process remains stable\n\n**Best Practice:**\n- Create histogram\n- Try different bin counts (7, 10, 15)\n- Choose the one that most clearly shows the distribution shape\n- Label axes clearly\n- Add specification limits if relevant\n- Use for communication and analysis\n\nWhy other options are wrong:\n\nOption B is dangerously wrong. Histogram creation involves decisions (bin count, width, range) that significantly affect interpretation.\n\nOption C ignores that bin count dramatically changes what patterns you can see.\n\nOption D is overly rigid. While 10 bins is a reasonable starting point, it's not always optimal. Sample size and data range matter.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.3.2",
    "estimated_time_seconds": 100
  },
  {
    "module": "2.3",
    "module_name": "Data Visualization",
    "question": "A process improvement team creates a chart showing production volume over 12 months. They make the chart 3D with tilted bars, gradient colors, and a textured background. What principle of effective data visualization are they likely violating?",
    "type": "multiple_choice",
    "options": [
      "Less is more—unnecessary decoration (chart junk) distracts from the data and can make the chart harder to read; clean, simple designs communicate more effectively",
      "Charts should always be as decorative and colorful as possible to be interesting",
      "3D effects always make charts better and easier to understand",
      "The more complex a chart looks, the more professional it appears"
    ],
    "correct_answer": 0,
    "explanation": "This question addresses 'chart junk'—unnecessary decoration that clutters visualizations and impedes understanding. Data visualization expert Edward Tufte championed the principle of maximizing the data-to-ink ratio: every visual element should serve a purpose.\n\n**The Problem with Over-Decorated Charts:**\n\n**Chart Junk Examples:**\n\n**1. 3D Effects:**\n- Tilted 3D bars create perspective distortion\n- Bars in the back appear smaller than bars in front, even if values are similar\n- Hard to read exact values from distorted axes\n- Viewer must mentally compensate for perspective—adding cognitive load\n\n**Example:** Two bars with values 100 and 105 (5% difference). In 3D perspective:\n- Back bar appears 30% smaller due to perspective\n- Front bar appears 10% larger due to perspective\n- Visual difference suggests 40% when actual difference is 5%\n\nResult: Misleading visual that requires extra work to interpret correctly.\n\n**2. Gradient Colors:**\n- Gradients within bars add visual noise\n- Serve no data purpose—color doesn't encode any information\n- Can make bars harder to see clearly\n- Distract eye from the data\n\n**3. Textured Backgrounds:**\n- Patterns behind data compete for attention\n- Reduce contrast, making text and data harder to read\n- Serve only decoration—no information value\n\n**The 'Data-Ink Ratio' Principle:**\n\nData-Ink Ratio = Ink used to display data / Total ink used\n\nGoal: Maximize this ratio by:\n1. **Keeping essential elements:**\n   - Data (bars, lines, points)\n   - Axes with clear labels\n   - Axis tick marks and numbers\n   - Title\n   - Legend (if needed)\n\n2. **Removing non-essential elements:**\n   - Unnecessary gridlines\n   - Decorative colors\n   - 3D effects\n   - Shadows\n   - Gradients\n   - Background patterns\n   - Excessive labels\n   - Chartjunk\n\n**Before (With Chart Junk):**\n- 3D tilted bars (distortion)\n- Gradient fill (noise)\n- Textured background (distraction)\n- Shadow effects (clutter)\n- Multiple font styles (inconsistent)\n- Decorative borders (wasted space)\n\nUser experience: 'This chart is pretty, but what's it saying? Let me squint to figure it out...'\n\n**After (Clean Design):**\n- Simple 2D bars (clear height comparison)\n- Solid colors (one color for all bars, or colors encoding meaningful categories)\n- White or light gray background (maximum contrast)\n- Clear axis labels\n- Simple title\n- Minimal gridlines (or none)\n\nUser experience: 'I immediately see production peaked in June, dropped in August, and recovered in October.'\n\n**Effective Design Principles:**\n\n**1. Clarity Over Complexity:**\nComplex ≠ Professional. Clear = Professional.\nThe goal is communication, not decoration.\n\n**2. Direct Labeling:**\nLabel data points directly rather than forcing readers to match colors to a legend.\n\n**Bad:** Blue bars with legend saying 'Blue = Product A'\n**Good:** Label directly: 'Product A' right on or next to the bar\n\n**3. Minimize Cognitive Load:**\nEvery unnecessary element requires mental processing:\n- 'What does this gradient mean?' (Nothing—it's decorative)\n- 'Why is this bar smaller?' (Perspective distortion, not actual data)\n- 'What's that pattern in the background?' (Decoration—ignore it)\n\nEach distraction reduces understanding.\n\n**4. Accessibility:**\nClean designs work better for:\n- Color-blind viewers (don't rely solely on color)\n- Printed documents (gradients and 3D effects print poorly)\n- Projectors (extra details wash out)\n- Quick comprehension (busy executives)\n\n**Six Sigma Application:**\n\nSix Sigma visualizations should prioritize:\n- **Clarity:** Stakeholders understand data instantly\n- **Accuracy:** Visual representation matches numerical reality\n- **Actionability:** Patterns and insights are obvious\n- **Professionalism:** Clean, simple, focused\n\n**When Decoration is Acceptable:**\n\n1. **Marketing materials:** Where aesthetics matter for brand\n2. **Public presentations:** Where engagement is key\n3. **Infographics:** Where design supports storytelling\n\nBut even then, decoration should enhance understanding, not impede it.\n\n**For Six Sigma Project Reports:**\nAlways prioritize clarity. Your audience (executives, engineers, operators) needs to understand data quickly and act on it. Chart junk wastes their time and risks miscommunication.\n\n**The Tufte Test:**\nFor every element in your chart, ask: 'Does this help communicate the data, or is it just decoration?'\n- If it helps: Keep it\n- If it's decoration: Delete it\n\nResult: Cleaner, more effective visualizations.\n\n**Tools That Encourage Chart Junk:**\nExcel's default chart options often include unnecessary 3D effects, gradients, and heavy gridlines. Actively simplify:\n- Change to 2D chart type\n- Remove background fill\n- Lighten or remove gridlines\n- Use solid colors\n- Remove shadows and effects\n\nResult: Charts that look more professional and communicate better.\n\nWhy other options are wrong:\n\nOption B is opposite of best practice. Decoration distracts from data.\n\nOption C is wrong—3D effects create distortion and confusion, making charts harder to read.\n\nOption D confuses complexity with professionalism. Simple, clear designs are more professional.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.3.4",
    "estimated_time_seconds": 100
  },
  {
    "module": "2.4",
    "module_name": "Normal Distribution & Probability",
    "question": "A manufacturing process produces parts with a mean diameter of 50mm and standard deviation of 2mm. Assuming a normal distribution, approximately what percentage of parts will have diameters between 48mm and 52mm?",
    "type": "multiple_choice",
    "options": [
      "Approximately 68% of parts will fall between 48-52mm, because this range represents mean ± 1 standard deviation, which captures about 68% of data in a normal distribution (the 68-95-99.7 rule)",
      "Exactly 50% because 48-52mm is half the specification",
      "Approximately 95% because that's always the right answer for normal distributions",
      "100% because all parts must be within this range"
    ],
    "correct_answer": 0,
    "explanation": "This question tests understanding of the empirical rule (68-95-99.7 rule), one of the most practical tools for working with normal distributions in Six Sigma. This rule provides quick probability estimates without complex calculations.\n\n**The 68-95-99.7 Rule (Empirical Rule):**\n\nFor any normal distribution:\n- **±1σ (one standard deviation):** Approximately 68% of data falls within mean ± 1σ\n- **±2σ (two standard deviations):** Approximately 95% of data falls within mean ± 2σ\n- **±3σ (three standard deviations):** Approximately 99.7% of data falls within mean ± 3σ\n\n**Applying to This Problem:**\n\nGiven:\n- Mean (μ) = 50mm\n- Standard Deviation (σ) = 2mm\n- Range of interest: 48-52mm\n\n**Step 1: Calculate how many standard deviations from the mean:**\n- Lower bound: 48mm = 50mm - 2mm = μ - 1σ\n- Upper bound: 52mm = 50mm + 2mm = μ + 1σ\n\n**Step 2: Apply the empirical rule:**\nThe range 48-52mm represents μ ± 1σ, which captures approximately **68%** of the data.\n\n**Visual Understanding:**\n\nImagine the bell curve:\n- Peak at 50mm (the mean)\n- 68% of the area under the curve falls between 48mm and 52mm\n- 16% of area falls below 48mm (left tail)\n- 16% of area falls above 52mm (right tail)\n- Total: 16% + 68% + 16% = 100%\n\n**Expanding the Analysis:**\n\nIf the question asked about different ranges:\n\n**Range 46-54mm (μ ± 2σ):**\n- 46mm = 50 - (2 × 2) = 50 - 4mm\n- 54mm = 50 + (2 × 2) = 50 + 4mm\n- Answer: Approximately 95%\n\n**Range 44-56mm (μ ± 3σ):**\n- 44mm = 50 - (3 × 2) = 50 - 6mm\n- 56mm = 50 + (3 × 2) = 50 + 6mm\n- Answer: Approximately 99.7%\n\n**Practical Implications for Six Sigma:**\n\n**Scenario: Specification Limits are 45-55mm**\n\nThe specification range is 45-55mm (10mm total width).\nThe process spread (±3σ) is 44-56mm (12mm total width).\n\n**Problem:** Process spread exceeds specification width!\n- Parts below 45mm: Roughly 16% (everything below μ - 2.5σ)\n- Parts above 55mm: Roughly 16% (everything above μ + 2.5σ)\n- Total defect rate: approximately 32% (!)\n\nThis process is incapable and will produce many defects.\n\n**Improvement Target:**\nReduce σ from 2mm to 1mm:\n- New spread (±3σ): 47-53mm (6mm total width)\n- Specification: 45-55mm (10mm width)\n- Now process fits comfortably within specs\n- Defect rate drops to near zero\n\n**Why the 68-95-99.7 Rule is So Useful:**\n\n**1. Quick Mental Math:**\nNo calculator needed. Just count standard deviations:\n- Within ±1σ: About 2/3 (68%)\n- Within ±2σ: About 19/20 (95%)\n- Within ±3σ: Nearly all (99.7%)\n\n**2. Defect Rate Estimation:**\nIf specs are at ±3σ:\n- 99.7% good\n- 0.3% defects\n- 3,000 defects per million\n\nThis is 3-sigma performance (roughly 66,807 DPMO accounting for process shift, but the quick estimate gives you the magnitude).\n\n**3. Process Capability:**\nQuickly assess if process spread (±3σ) fits within specifications.\n\n**4. Sample Size Decisions:**\nIf you want to capture typical process variation, sample size to cover ±2σ or ±3σ.\n\n**Important Assumptions:**\n\n**This rule applies ONLY to normal (Gaussian) distributions:**\n- Bell curve shape\n- Symmetric around the mean\n- Mean = Median = Mode\n\nIf your data is skewed, bimodal, or otherwise non-normal, the 68-95-99.7 rule does NOT apply. Always check distribution shape with a histogram first.\n\n**More Precise Calculations:**\n\nThe empirical rule gives approximations. For exact probabilities:\n- Use Z-score formula: Z = (X - μ) / σ\n- Look up Z in standard normal table\n- Or use Excel: NORM.DIST function\n\nBut for quick estimates in Six Sigma projects, the empirical rule is incredibly useful.\n\nWhy other options are wrong:\n\nOption B (50%) misunderstands probability. The 48-52mm range isn't 'half' of anything relevant. It's ±1σ, which by the empirical rule contains 68%.\n\nOption C (95%) confuses ±1σ with ±2σ. 95% is correct for ±2σ (46-54mm), not ±1σ (48-52mm).\n\nOption D (100%) is wrong. Normal distributions have infinite tails—there's always some probability (however small) of values outside any finite range. About 32% of parts will fall outside 48-52mm (16% below, 16% above).",
    "difficulty": "beginner",
    "learning_objective": "LO-2.4.2",
    "estimated_time_seconds": 100
  },
  {
    "module": "2.4",
    "module_name": "Normal Distribution & Probability",
    "question": "You create a histogram of process data and notice it has two distinct peaks instead of one central peak. What does this tell you about using normal distribution assumptions for this data?",
    "type": "multiple_choice",
    "options": [
      "The data is bimodal (two peaks), not normal—normal distribution assumptions and tools (like the 68-95-99.7 rule) don't apply; investigate why two distinct populations exist in your data",
      "This is perfectly normal distribution—all normal curves have two peaks",
      "The data is fine; use normal distribution calculations anyway",
      "Two peaks mean the data is twice as normal as usual"
    ],
    "correct_answer": 0,
    "explanation": "This question addresses a critical Six Sigma skill: recognizing when data doesn't fit normal distribution assumptions. Using normal distribution tools on non-normal data leads to incorrect conclusions and poor decisions.\n\n**What is Bimodal Distribution?**\n\nA bimodal distribution has two distinct peaks (modes) rather than the single central peak characteristic of normal distributions.\n\n**Normal Distribution (One Peak):**\n```\n        *\n       ***\n      *****\n     *******\n    *********\n   ***********\n  *************\n```\nSingle peak at the center (the mean). Symmetric bell curve.\n\n**Bimodal Distribution (Two Peaks):**\n```\n    **        **\n   ****      ****\n  ******    ******\n ******** ********\n******************\n```\nTwo peaks with a valley between. NOT a normal distribution.\n\n**Why Bimodal Data Occurs:**\n\nBimodal distributions typically indicate two distinct populations mixed together:\n\n**Example 1: Manufacturing Process**\nCycle time data shows two peaks:\n- Peak 1 at 10 minutes\n- Peak 2 at 15 minutes\n\n**Investigation reveals:**\n- Operator A (experienced): Average 10 minutes\n- Operator B (new): Average 15 minutes\n\n**Root cause:** Two different operating modes mixed in one dataset.\n\n**Example 2: Customer Order Size**\nOrder size shows two peaks:\n- Peak 1: $50-100 (retail customers)\n- Peak 2: $5,000-10,000 (wholesale customers)\n\n**Root cause:** Two different customer types.\n\n**Example 3: Product Dimensions**\nPart weight shows two peaks:\n- Peak 1 at 2.0 kg\n- Peak 2 at 2.5 kg\n\n**Investigation reveals:**\n- Two different products (Model A and Model B) measured together\n- Or: Machine setpoint changed during production, creating two populations\n\n**Root cause:** Mixed product types or process shift.\n\n**Why This Matters for Six Sigma:**\n\n**If you treat bimodal data as normal:**\n\n**1. Wrong Mean:**\nMean of combined data (12.5 minutes) represents neither operator accurately:\n- Operator A: 10 min average\n- Operator B: 15 min average\n- Combined mean: 12.5 min\n- But NO operator actually operates at 12.5 min!\n\n**2. Inflated Standard Deviation:**\nThe variation between the two groups inflates standard deviation artificially:\n- Operator A: σ = 1 min (consistent)\n- Operator B: σ = 1 min (consistent)\n- Combined: σ = 3 min (appears highly variable)\n\nThe large σ reflects the difference between groups, not variation within groups.\n\n**3. Wrong Process Capability:**\nCp and Cpk calculated on bimodal data are meaningless:\n- You're treating two different processes as one\n- Capability indices will be artificially low\n- You might declare process incapable when each sub-process is actually capable\n\n**4. Wrong Probability Estimates:**\nThe 68-95-99.7 rule doesn't apply:\n- Assumes symmetric, single-peaked distribution\n- With two peaks, the probability distribution is completely different\n- Predictions will be wrong\n\n**Correct Approach When You See Bimodal Data:**\n\n**Step 1: Recognize it's not normal**\nHistogram shows two peaks → Don't use normal distribution tools\n\n**Step 2: Investigate root causes**\nAsk: 'Why are there two distinct populations?'\n- Different operators, shifts, machines?\n- Different product types, customer types?\n- Process changed midway through data collection?\n- Different measurement methods?\n\n**Step 3: Separate the populations**\nSplit data into subgroups:\n- Analyze Operator A data separately (likely normal)\n- Analyze Operator B data separately (likely normal)\n- Now each dataset is normal and you can use normal distribution tools\n\n**Step 4: Address the root cause**\n- Train Operator B to match Operator A's performance\n- Standardize the process so both operators achieve similar results\n- Or: Recognize different products need different specifications\n\n**Step 5: Re-analyze**\nOnce populations are separated or root cause addressed, re-create histogram:\n- Should now show single peak (normal)\n- Can now correctly calculate mean, σ, capability\n- Improvement: Reduced variation, better capability\n\n**Statistical Tests for Normality:**\n\nBefore using normal distribution tools, verify normality:\n\n**1. Visual Tests:**\n- **Histogram:** Should show single, symmetric peak\n- **Normal Probability Plot:** Points should follow straight line\n\n**2. Statistical Tests:**\n- **Anderson-Darling test:** Tests goodness of fit\n- **Shapiro-Wilk test:** Tests normality hypothesis\n- **Kolmogorov-Smirnov test:** Compares to normal distribution\n\nIf tests fail (p-value < 0.05), data is not normal—investigate why.\n\n**What to Do with Non-Normal Data:**\n\n**Option 1: Transform the data**\nLog transformation, Box-Cox transformation might make data more normal\n\n**Option 2: Use non-parametric methods**\nMedian instead of mean, non-parametric control charts\n\n**Option 3: Separate the populations** (best for bimodal)\nAnalyze each group separately\n\n**Six Sigma Principle:**\n'Trust but verify.' Don't assume normality—test it. When data doesn't fit the model, understand why before applying statistical tools.\n\nWhy other options are wrong:\n\nOption B is factually wrong. Normal distributions have ONE peak at the center, not two. This is a defining characteristic.\n\nOption C is dangerous. Using normal distribution calculations on non-normal data produces incorrect results that lead to poor decisions.\n\nOption D makes no sense. 'Twice as normal' isn't a thing. Bimodal is non-normal, period.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.4.3",
    "estimated_time_seconds": 100
  },
  {
    "module": "2.4",
    "module_name": "Normal Distribution & Probability",
    "question": "In Six Sigma, why is understanding normal distribution so fundamental to measuring process capability?",
    "type": "multiple_choice",
    "options": [
      "Process capability indices (Cp and Cpk) assume normal distribution to calculate how many standard deviations fit between the process mean and specification limits; without normal distribution, we can't accurately predict defect rates or sigma levels",
      "Normal distribution is just a theoretical concept with no practical application",
      "Process capability doesn't actually require any understanding of statistics",
      "The shape of the distribution doesn't matter for calculating capability"
    ],
    "correct_answer": 0,
    "explanation": "This question connects normal distribution theory to practical Six Sigma application: calculating process capability. Understanding this link is essential for interpreting Cp, Cpk, and sigma levels.\n\n**What is Process Capability?**\n\nProcess capability measures whether your process can consistently produce output within customer specifications. It answers: 'Can this process meet requirements?'\n\n**Key Metrics:**\n- **Cp (Potential Capability):** Compares specification width to process width\n- **Cpk (Actual Capability):** Like Cp but accounts for process centering\n- **Sigma Level:** How many standard deviations fit between mean and nearest specification\n\n**Why Normal Distribution is Essential:**\n\n**The Foundation:**\nAll these metrics assume your process data follows a normal distribution. Here's why that matters:\n\n**1. Predictable Probability:**\n\nNormal distributions have known probability properties:\n- We know exactly what % of data falls within ±1σ, ±2σ, ±3σ\n- This lets us predict defect rates from simple calculations\n- Without normal distribution, these predictions fail\n\n**Example:**\n\nProcess: Mean = 50mm, σ = 1mm, Specifications = 47-53mm\n\n**Distance to specs:**\n- Lower spec (47mm) is 3σ below mean (50 - 3×1 = 47)\n- Upper spec (53mm) is 3σ above mean (50 + 3×1 = 53)\n\n**If normal distribution:**\n- 99.7% of parts fall within ±3σ (47-53mm)\n- 0.3% fall outside (defects)\n- Defect rate: 3,000 per million opportunities (3-sigma process)\n\n**If NOT normal distribution (e.g., skewed):**\n- Same calculation gives you 0.3% defects\n- But actual defect rate might be 5% or 15%!\n- Your prediction is wildly wrong\n- You think the process is capable when it's not\n\n**2. Process Capability Index (Cp):**\n\n**Formula:**\nCp = (USL - LSL) / (6σ)\n\nWhere:\n- USL = Upper Specification Limit\n- LSL = Lower Specification Limit\n- 6σ = Total process spread (±3σ captures 99.7% if normal)\n\n**Interpretation:**\n- Cp = 1.0: Process spread exactly matches spec width (marginal)\n- Cp = 1.33: Process is capable (commonly required minimum)\n- Cp = 2.0: Process is excellent (Six Sigma level)\n\n**This ONLY works if:**\n- Data is normally distributed\n- The ±3σ spread actually captures 99.7% of data\n\nIf data is skewed or bimodal, the 6σ in the denominator doesn't represent your true process spread.\n\n**3. Process Capability Index (Cpk):**\n\n**Formula:**\nCpk = min[(USL - μ) / 3σ, (μ - LSL) / 3σ]\n\nThis measures:\n- How many standard deviations fit between mean and nearest specification\n- Accounts for process centering (Cp does not)\n\n**Example:**\nμ = 50mm, σ = 1mm, LSL = 47mm, USL = 53mm\n\nCpk = min[(53-50)/3×1, (50-47)/3×1]\n    = min[3/3, 3/3]\n    = min[1.0, 1.0]\n    = 1.0\n\n**Interpretation:**\nCpk = 1.0 means the nearest specification is exactly 3σ away.\n- If normal: 99.7% of parts are within specs\n- If NOT normal: This prediction is wrong\n\n**4. Sigma Level Calculation:**\n\n**What '6 Sigma' Means:**\nThe nearest specification limit is 6 standard deviations away from the process mean.\n\n**Using normal distribution tables:**\n- 3σ = 99.7% within specs (3,000 DPMO)\n- 4σ = 99.994% within specs (62 DPMO)\n- 5σ = 99.99994% within specs (0.6 DPMO)\n- 6σ = 99.9999998% within specs (0.002 DPMO, or 3.4 DPMO with 1.5σ shift)\n\n**These calculations assume normal distribution!**\n\nIf data is not normal:\n- The table values don't apply\n- Your sigma level calculation is meaningless\n- You might claim '4 sigma performance' when actual defect rate is much higher\n\n**Real-World Implications:**\n\n**Scenario: Pharmaceutical Manufacturing**\n\nTablet weight specification: 498-502mg\nProcess: Mean = 500mg, σ = 0.5mg\n\n**If you assume normal without checking:**\n\nCpk = (502-500) / (3×0.5) = 2/1.5 = 1.33\nConclusion: Process is capable\nPredicted defects: ~63 per million\n\n**But actual histogram reveals:**\n- Data is right-skewed (not normal)\n- Tail extends toward high weights\n- Actual defects: 5,000 per million (10x higher than predicted!)\n\n**Consequence:**\n- Regulatory violation\n- Product recalls\n- Patient safety risk\n- All because you assumed normality without verifying\n\n**Best Practice:**\n\n**Before calculating Cp or Cpk:**\n1. Create histogram—check for normal shape\n2. Create normal probability plot—check for linearity\n3. Run normality test (Anderson-Darling, Shapiro-Wilk)\n4. If data is NOT normal:\n   - Investigate why (bimodal? skewed? outliers?)\n   - Transform data, or\n   - Use non-parametric capability analysis, or\n   - Fix the root cause making data non-normal\n5. Only then calculate capability indices\n\n**The Bottom Line:**\n\nNormal distribution is the foundation of Six Sigma capability analysis because:\n- It enables probability predictions\n- It defines standard deviation relationships\n- It allows calculation of Cp, Cpk, and sigma levels\n- It connects process performance to defect rates\n\nWithout normal distribution, these calculations are unreliable or meaningless.\n\nWhy other options are wrong:\n\nOption B is completely wrong. Normal distribution is THE theoretical foundation for practical Six Sigma calculations. It's applied millions of times daily in quality control worldwide.\n\nOption C is dangerous. Process capability is BUILT on statistics—specifically normal distribution statistics. Without this understanding, capability metrics are just meaningless numbers.\n\nOption D is wrong. Distribution shape is CRITICAL. Cp and Cpk calculations explicitly assume normal distribution. Using them on non-normal data produces incorrect capability assessments.",
    "difficulty": "beginner",
    "learning_objective": "LO-2.4.4",
    "estimated_time_seconds": 105
  }
]
