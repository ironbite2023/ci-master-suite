# Course 4: Six Sigma DMAIC Overview - Video Scripts

**Created:** October 3, 2025  
**Status:** Ready for integration into main video scripts file  
**Total Word Count:** ~5,600 words

---

## Module 4.1: Define Phase—Setting Up for Success

**Duration:** 11 minutes | **Word Count:** ~1,420 words

### Script

[VISUAL: Title slide - "Define Phase—Setting Up for Success" with CI Master Academy logo]

**[00:00 - 00:45] HOOK & INTRODUCTION**

Let me tell you about two Six Sigma projects.

**Project A:** Team spent 6 months analyzing production defects, implemented multiple solutions, spent $250,000. Result? 12% defect reduction—better, but not the breakthrough they expected.

**Project B:** Team spent 2 weeks clearly defining the problem, validating it with data, and aligning stakeholders. Then spent 4 months solving it. Result? 85% defect reduction, $1.2 million in annual savings.

[VISUAL: Comparison graphic showing both projects]

What made the difference? Project B invested in the **Define phase**—setting up the project for success before diving into solutions.

Today, we're exploring the Define phase of DMAIC—the foundation that determines whether your Six Sigma project succeeds or struggles.

As the saying goes: "A problem well-defined is half-solved."

**[00:45 - 02:15] THE PURPOSE OF THE DEFINE PHASE**

The Define phase is the first step in DMAIC, and its purpose is simple yet critical:

[VISUAL: Define phase highlighted in DMAIC cycle]

**DEFINE PHASE PURPOSE:**
- Clearly articulate the problem
- Establish project goals and scope
- Identify stakeholders and customers
- Document baseline performance
- Secure resources and commitment

**Why Define matters:**

[ANIMATION: Foundation visual]

Think of Define as laying the foundation for a building. If the foundation is crooked or weak, everything built on it will be unstable—no matter how good the construction.

Similarly, if your problem definition is vague, your scope is unclear, or your stakeholders aren't aligned, your project will struggle throughout.

**COMMON MISTAKES (skipping Define):**

[VISUAL: Warning signs appearing]

- Solving the wrong problem (symptoms vs root causes)
- Scope creep (project grows unmanageably)
- Stakeholder resistance (people weren't involved early)
- Unclear success criteria (no one knows what "done" looks like)
- Resource conflicts (no formal commitment)

**Time investment in Define:** Typically 10-15% of total project time, but it prevents wasting 50% of project effort later!

**[02:15 - 05:00] THE PROJECT CHARTER: YOUR ROADMAP**

The primary deliverable of Define phase is the **Project Charter**—a one-page document that answers critical questions about your project.

[VISUAL: Project Charter template appearing]

**PROJECT CHARTER COMPONENTS:**

**1. PROBLEM STATEMENT**

Describes the problem in specific, measurable terms.

**Bad problem statement:**
"Customer satisfaction is low."

**Good problem statement:**
"Customer satisfaction scores for Product Line X have declined from 85% (baseline 2023) to 68% (current), resulting in 15% increase in returns and estimated $2M annual revenue loss."

**Formula:** What + Where + When + Impact

**2. GOAL STATEMENT (SMART)**

Defines what success looks like.

**Bad goal:**
"Improve customer satisfaction."

**Good goal:**
"Increase customer satisfaction scores for Product Line X from 68% to 82% by Q4 2025, reducing returns by 50% and recovering $1M in annual revenue."

**SMART criteria:**
- **S**pecific: Exactly what will improve
- **M**easurable: Numeric target (82%)
- **A**chievable: Realistic given resources
- **R**elevant: Aligns with business goals
- **T**ime-bound: Clear deadline (Q4 2025)

**3. SCOPE (IN/OUT)**

Defines boundaries—what's included and excluded.

[VISUAL: Scope boundary diagram]

**In Scope:**
- Product Line X only
- Current manufacturing process
- Existing customer base
- North American operations

**Out of Scope:**
- Other product lines
- New product development
- Marketing/pricing changes
- International operations

**Why scope matters:** Prevents project from expanding uncontrollably. Every time someone says "While we're at it, let's also...", you point to scope!

**4. BUSINESS CASE**

Justifies why this project matters.

**Questions answered:**
- What's the cost of the current problem?
- What's the expected benefit of solving it?
- What's the ROI?
- Why now?

[EXAMPLE]

**Business case:**
- Current cost of problem: $2M/year (lost revenue + returns processing)
- Expected benefit: $1.5M/year savings (75% of problem cost)
- Project investment: $150K (staff time + resources)
- ROI: 10:1 return, payback in 2 months
- Strategic importance: Product Line X is key growth area

**5. TEAM & ROLES**

Identifies who's involved and their responsibilities.

[VISUAL: Team structure appearing]

**Typical roles:**

- **Sponsor:** Executive who owns the problem, removes barriers, provides resources
- **Champion:** Senior leader who guides and supports project
- **Black Belt/Project Leader:** Drives day-to-day project execution
- **Green Belts/Team Members:** Execute analysis and improvements
- **Process Owner:** Owns the process being improved
- **Subject Matter Experts (SMEs):** Provide technical expertise

**Why roles matter:** Clarity prevents "I thought you were doing that" confusion!

**6. TIMELINE & MILESTONES**

High-level project schedule.

[VISUAL: Timeline graphic]

**Example:**
- Define: Weeks 1-2
- Measure: Weeks 3-6
- Analyze: Weeks 7-10
- Improve: Weeks 11-18
- Control: Weeks 19-20
- Total: 20 weeks (5 months)

**[05:00 - 07:00] SIPOC: HIGH-LEVEL PROCESS MAPPING**

Another key Define phase tool is SIPOC—a high-level process map.

[VISUAL: SIPOC diagram template]

**SIPOC = Suppliers - Inputs - Process - Outputs - Customers**

**Purpose:** Create shared understanding of the process at a glance, before diving into detailed analysis.

**HOW TO CREATE A SIPOC:**

**STEP 1: Start with the Process (middle column)**

List 5-8 high-level steps (not detailed!).

**Example - Order Fulfillment:**
1. Receive order
2. Check inventory
3. Pick items
4. Pack order
5. Ship to customer

**STEP 2: Identify Outputs**

What does the process produce?

- Completed order
- Packing slip
- Shipping notification
- Invoice

**STEP 3: Identify Customers**

Who receives the outputs?

- End customer (primary)
- Accounts receivable (invoice)
- Shipping department (internal)

**STEP 4: Identify Inputs**

What's needed for the process to work?

- Customer order
- Inventory data
- Packing materials
- Shipping labels

**STEP 5: Identify Suppliers**

Who provides the inputs?

- Customer (order)
- Inventory system (data)
- Warehouse (materials)
- IT system (labels)

[VISUAL: Completed SIPOC example]

**Why SIPOC matters:**

- Creates common language for team
- Identifies process boundaries
- Reveals key inputs/outputs to measure
- Shows connections to suppliers/customers
- Takes only 30-60 minutes to create!

**[07:00 - 09:00] VOICE OF THE CUSTOMER (VOC)**

Understanding what customers actually want is critical—that's Voice of the Customer (VOC).

[VISUAL: VOC gathering methods]

**VOC METHODS:**

**1. Direct Feedback:**
- Surveys
- Interviews
- Focus groups
- Customer complaints/returns data

**2. Observation:**
- Watch customers use product/service
- Note pain points and frustrations

**3. Data Analysis:**
- Customer satisfaction scores
- Net Promoter Score (NPS)
- Return/defect data by issue type

**TRANSLATING VOC TO REQUIREMENTS:**

Customers often speak in vague terms. Your job: translate to specific, measurable requirements.

[EXAMPLE]

**Customer says:** "Delivery is too slow."

**VOC translation questions:**
- How slow is "too slow"? (Current: 7 days)
- What would be acceptable? (Target: 3 days)
- Why does speed matter? (Need product for time-sensitive use)
- What percentage of customers feel this way? (Survey: 45%)

**Result:** Measurable requirement: "Reduce delivery time from 7 days to 3 days to satisfy 45% of customers who cite speed as concern."

**CTQ (Critical To Quality) TREE:**

Tool for breaking down customer needs into specific requirements.

[VISUAL: CTQ tree example]

**Example:**

**Customer Need:** "Reliable product"

**CTQ Drivers:**
- Product doesn't fail during use
- Product lasts expected lifetime
- Product performs consistently

**Measurable Requirements:**
- <0.5% field failure rate
- >3 year average lifetime
- <2% performance variation

Now you have actionable, measurable requirements!

**[09:00 - 10:30] STAKEHOLDER ANALYSIS & ENGAGEMENT**

Projects don't fail for technical reasons—they fail because people resist change. Stakeholder engagement in Define phase prevents this.

[VISUAL: Stakeholder matrix]

**IDENTIFY STAKEHOLDERS:**

Who's affected by the problem or solution?
- Process owners
- Department managers
- Employees who do the work
- Customers (internal and external)
- Suppliers
- Senior leadership
- Other departments (IT, HR, Finance)

**STAKEHOLDER ANALYSIS:**

For each stakeholder, assess:
- **Power/Influence:** Can they make/break the project?
- **Interest/Impact:** How much do they care?
- **Support Level:** Champion, supporter, neutral, skeptic, blocker?

[VISUAL: 2x2 matrix - Power vs Interest]

**HIGH POWER + HIGH INTEREST:** Manage closely (key players)
**HIGH POWER + LOW INTEREST:** Keep satisfied (don't surprise them)
**LOW POWER + HIGH INTEREST:** Keep informed (they care!)
**LOW POWER + LOW INTEREST:** Monitor (minimal effort)

**ENGAGEMENT STRATEGIES:**

**Champions/Supporters:**
- Involve actively in project
- Ask for advocacy with others
- Recognize their support

**Neutral:**
- Educate on project benefits
- Address their concerns
- Provide regular updates

**Skeptics/Blockers:**
- Understand their concerns (often legitimate!)
- Involve early to give voice
- Find common ground
- Address objections with data

[EXAMPLE]

**Skeptical Production Manager:**
- Concern: "This will slow down production!"
- Engagement: Invite to charter meeting, show data on quality cost, pilot solution on one line
- Result: Converts to supporter when sees benefits

**[10:30 - 10:45] COMMON DEFINE PHASE MISTAKES**

Avoid these pitfalls:

**MISTAKE 1: Vague Problem Statement**
- Bad: "Quality is poor"
- Good: "Defect rate is 8%, costing $500K annually"

**MISTAKE 2: Solution in Disguise**
- Bad: "We need new equipment" (that's a solution!)
- Good: "Throughput is 30% below target" (that's a problem)

**MISTAKE 3: Scope Too Broad**
- Bad: "Improve entire supply chain"
- Good: "Reduce raw material receiving time for Plant 3"

**MISTAKE 4: No Sponsor Commitment**
- Charter signed without real executive commitment
- Project later starved of resources

**MISTAKE 5: Ignoring Stakeholders**
- Team works in isolation
- Presents solution
- Faces resistance and rejection

**[10:45 - 11:00] WRAP-UP & ASSIGNMENT**

Let's recap the Define phase:

[VISUAL: Define phase summary]

✓ **Purpose:** Set up project for success with clear problem, goals, scope  
✓ **Project Charter:** One-page roadmap (problem, goal, scope, business case, team, timeline)  
✓ **SIPOC:** High-level process understanding  
✓ **VOC:** Translate customer needs to measurable requirements  
✓ **Stakeholder Engagement:** Build support early  
✓ **Output:** Approved charter, aligned team, clear path forward

**Your assignment:**

**Create a project charter for a real problem:**

1. Identify a problem at work (use DOWNTIME to spot waste!)
2. Write a problem statement (What + Where + When + Impact)
3. Write a SMART goal statement
4. Define scope (in/out)
5. Create simple business case (cost of problem, expected benefit)
6. Identify key stakeholders
7. Create a basic SIPOC diagram

**Don't overthink it—practice the structure!**

In Module 4.2, we'll move to the Measure phase—establishing baseline performance and validating your measurement system.

[VISUAL: End screen - "Module 4.1 Complete" with transition to Module 4.2]

---

## Module 4.2: Measure Phase—Establishing Baseline

**Duration:** 11 minutes | **Word Count:** ~1,400 words

### Script

[VISUAL: Title slide - "Measure Phase—Establishing Baseline" with CI Master Academy logo]

**[00:00 - 00:45] HOOK & INTRODUCTION**

"You can't improve what you don't measure."

This famous management principle is true—but incomplete. The full truth is:

"You can't improve what you don't measure **accurately**."

[VISUAL: Two measurement scenarios side by side]

A manufacturing company measured their defect rate at 3%. They launched a Six Sigma project to reduce it to 1%. After six months of hard work, they remeasured: still 3%.

Frustrated, they dug deeper and discovered their measurement system was broken. Inspectors used different standards, measuring tools were uncalibrated, and data recording was inconsistent.

The real defect rate? Actually 8%. But they couldn't improve what they couldn't measure correctly!

Today, we're exploring the Measure phase of DMAIC—where we establish baseline performance and ensure our measurements are trustworthy.

**[00:45 - 02:00] THE PURPOSE OF THE MEASURE PHASE**

The Measure phase has two critical objectives:

[VISUAL: Two objectives appearing]

**OBJECTIVE 1: Establish Baseline Performance**

Quantify current performance with data:
- How bad is the problem really?
- What's the starting point for improvement?
- How will we know if improvement happens?

**OBJECTIVE 2: Validate the Measurement System**

Ensure measurements are reliable:
- Is our measurement system accurate?
- Do different people get consistent results?
- Can we trust the data for decision-making?

**Why baseline matters:**

[ANIMATION: Before/after comparison]

Without baseline, you can't prove improvement!

- Claim: "We improved throughput!"
- Response: "Compared to what? How much? Show me the data."

Baseline = your proof of starting point and proof of improvement.

**Why measurement system matters:**

[VISUAL: Garbage in, garbage out]

If your measurement system has high error, your data is unreliable. You'll make wrong decisions based on bad data.

**Rule:** If measurement variation is larger than process variation, you're measuring the measurement system, not the process!

**[02:00 - 04:30] OPERATIONAL DEFINITIONS**

Before measuring anything, you need **operational definitions**—crystal-clear definitions of what you're measuring.

[VISUAL: Operational definition template]

**OPERATIONAL DEFINITION:**

A precise description of:
- What you're measuring
- How to measure it
- Under what conditions
- What equipment/tools to use
- Acceptance criteria

**Why operational definitions matter:**

[EXAMPLE - Ambiguous vs Clear]

**Ambiguous:** "Measure defects"

Questions this raises:
- What's a defect? (Scratch? How big? How deep?)
- Where on the product? (All surfaces or just visible ones?)
- How to detect? (Visual? Magnification? Automated?)
- Who decides? (Each inspector's judgment?)

**With operational definition:**

**"Defect = Surface scratch on Product X:**
- **Location:** Top face only (visible to customer)
- **Size:** Length >5mm
- **Depth:** Visible under normal lighting (500 lux) from 12 inches
- **Detection method:** Visual inspection with standardized lighting
- **Measurement tool:** Depth gauge (Model ABC-123)
- **Inspector:** Certified inspectors only
- **Decision rule:** If ANY scratch meets criteria, unit is defective"

**Result:** 10 different inspectors will make the same judgment on the same part!

**COMMON THINGS NEEDING OPERATIONAL DEFINITIONS:**

- Cycle time (start point? End point? Include delays?)
- Customer satisfaction (measure how? Scale? Which questions?)
- "Complete" (what criteria define completeness?)
- "On-time" (to what standard? Customer requested or promised date?)
- Defect types (each type precisely defined)

**Without operational definitions, data is unreliable!**

**[04:30 - 07:00] MEASUREMENT SYSTEM ANALYSIS (MSA)**

Now let's ensure your measurement system is trustworthy. This is called Measurement System Analysis (MSA), and the most common method is **Gage R&R** (Gage Repeatability & Reproducibility).

[VISUAL: MSA concept diagram]

**TWO SOURCES OF MEASUREMENT VARIATION:**

**1. REPEATABILITY ("Equipment Variation")**
- Same person measures same item multiple times
- Do they get the same result?
- Variation due to the measurement equipment/process

**2. REPRODUCIBILITY ("Appraiser Variation")**
- Different people measure same item
- Do they get the same result?
- Variation due to different operators

[VISUAL: Repeatability vs Reproducibility illustrated]

**GAGE R&R STUDY:**

**Basic procedure:**
1. Select 10 representative parts (range of variation)
2. Select 3 appraisers (typical operators)
3. Each appraiser measures each part 3 times (in random order, blind)
4. Total: 90 measurements (10 parts × 3 appraisers × 3 trials)
5. Analyze data to calculate variation components

**WHAT YOU CALCULATE:**

**Gage R&R % = (Measurement Variation ÷ Total Variation) × 100%**

**Interpretation:**

- **<10%:** Excellent measurement system
- **10-30%:** Acceptable for most applications
- **>30%:** Unacceptable—fix before proceeding!

[EXAMPLE]

**Measuring part diameter:**

**Study results:**
- Total variation: 0.12 mm
- Measurement system variation: 0.08 mm
- Gage R&R: (0.08 ÷ 0.12) × 100% = 67%

**Interpretation:** Measurement system accounts for 67% of observed variation! Most "variation" we're seeing is measurement error, not actual part variation.

**Actions:**
- Calibrate equipment
- Retrain operators
- Improve measurement procedure
- Re-run Gage R&R study until <30%

**Why MSA matters:**

If you skip MSA and your measurement system is poor:
- You'll "improve" the measurement system, not the process
- You'll make wrong decisions based on bad data
- You'll waste months chasing phantom problems

**[07:00 - 09:00] DATA COLLECTION PLANNING**

Now that you have operational definitions and a validated measurement system, plan your data collection.

[VISUAL: Data collection plan template]

**DATA COLLECTION PLAN ELEMENTS:**

**1. WHAT TO MEASURE**

Identify key metrics:
- **Output (Y):** The problem metric (defects, cycle time, cost)
- **Inputs (X's):** Potential causes to track

**2. HOW TO MEASURE**

- Measurement method (automated? Manual? Sampling?)
- Tools required
- Operational definitions

**3. WHERE TO COLLECT**

- Physical location
- Process step
- Which product line/department

**4. WHEN TO COLLECT**

- Start date, end date
- Frequency (continuous, hourly, daily, weekly)
- Sample size needed

**5. WHO COLLECTS**

- Responsible person(s)
- Training requirements
- Backup collectors

**6. HOW TO RECORD**

- Data collection form/template
- Database or spreadsheet
- Data validation checks

[EXAMPLE - Call Center Data Collection Plan]

**Metric:** Average Handle Time (AHT)

| Element | Specification |
|---------|---------------|
| **What** | Time from call answer to call close |
| **How** | Automated phone system tracking |
| **Where** | All customer service lines |
| **When** | Continuous, 4 weeks, all shifts |
| **Who** | IT system (automated), verified by supervisor |
| **Sample** | All calls (population, not sample) |
| **Recording** | Database export to Excel daily |

**SAMPLING CONSIDERATIONS:**

Sometimes you can't measure everything—use sampling.

**Sample size rules of thumb:**
- Minimum 30 data points for statistical analysis
- Preferably 100+ for stable estimates
- Stratify if subgroups exist (different shifts, machines, operators)

**Stratification:** Breaking data into meaningful subgroups to see patterns.

[EXAMPLE]

Measuring defects without stratification: Overall defect rate = 5%

With stratification by shift:
- Shift A: 2% defects
- Shift B: 3% defects
- Shift C: 10% defects (aha! Problem is shift-specific!)

Stratification reveals root causes hidden in averages!

**[09:00 - 10:30] CALCULATING BASELINE AND PROCESS CAPABILITY**

Once you've collected data, calculate baseline performance.

**BASELINE METRICS:**

[VISUAL: Baseline dashboard]

**For continuous data (time, size, weight):**
- Mean (average)
- Standard deviation (variation)
- Range (min to max)
- Distribution shape (histogram)

**For discrete data (counts, defects):**
- Defect rate or proportion
- DPMO (Defects Per Million Opportunities)
- Frequency by type (Pareto chart)

**PROCESS CAPABILITY (BASELINE):**

Calculate current capability using Cp and Cpk (from Module 2.4!):

[EXAMPLE]

**Specification:** Delivery time must be 3-7 days

**Baseline data:**
- Mean: 6.2 days
- Standard deviation: 1.8 days

**Capability:**
- Cp = (7 - 3) ÷ (6 × 1.8) = 0.37 (poor!)
- Cpk = min[(7-6.2)÷(3×1.8), (6.2-3)÷(3×1.8)] = 0.15 (terrible!)

**Interpretation:** Process is not capable of meeting specifications consistently. About 35% of deliveries fall outside 3-7 day window.

**This baseline:**
- Quantifies the problem (35% failures)
- Sets improvement target (need Cpk > 1.33)
- Provides proof for business case
- Establishes starting point for before/after comparison

**PRESENT BASELINE TO STAKEHOLDERS:**

[VISUAL: Baseline presentation slide]

Create simple, clear presentation:
- Current performance: 6.2 days average, Cpk = 0.15
- Problem size: 35% of deliveries outside spec
- Business impact: Customer complaints, lost sales
- Improvement target: 5 days average, Cpk > 1.33
- Expected benefit: $500K annual revenue recovery

Baseline makes the problem real and urgent!

**[10:30 - 10:45] COMMON MEASURE PHASE MISTAKES**

Avoid these pitfalls:

**MISTAKE 1: Skipping Operational Definitions**
- Result: Inconsistent, unreliable data

**MISTAKE 2: Skipping MSA**
- Result: Don't know if measurements are trustworthy

**MISTAKE 3: Too Little Data**
- Collecting only 10-15 data points
- Result: Can't detect patterns or make reliable conclusions

**MISTAKE 4: Not Stratifying**
- Missing important subgroup patterns

**MISTAKE 5: Measuring Wrong Metric**
- Measuring what's easy instead of what matters
- Example: Measuring # of calls instead of problem resolution rate

**[10:45 - 11:00] WRAP-UP & ASSIGNMENT**

Let's recap the Measure phase:

[VISUAL: Measure phase summary]

✓ **Purpose:** Establish baseline, validate measurements  
✓ **Operational Definitions:** Precise definitions of what/how to measure  
✓ **MSA (Gage R&R):** Ensure measurement system is reliable (<30%)  
✓ **Data Collection Plan:** Who, what, where, when, how  
✓ **Baseline:** Quantify current performance with statistics  
✓ **Process Capability:** Calculate Cp/Cpk  
✓ **Output:** Reliable data, quantified baseline, clear improvement opportunity

**Your assignment:**

**For your Define phase project:**

1. Write operational definition for your key metric
2. Assess your measurement system (formal Gage R&R or simple check: do different people measuring the same thing get same result?)
3. Create a data collection plan
4. Collect baseline data (at least 30 data points)
5. Calculate baseline statistics (mean, standard deviation, range)
6. Create histogram of your data
7. If you have specifications, calculate Cp/Cpk

**You now have your starting point—proven with data!**

In Module 4.3, we'll move to Analyze phase—identifying and verifying the root causes of your problem.

[VISUAL: End screen - "Module 4.2 Complete" with transition to Module 4.3]

---

## Module 4.3: Analyze Phase—Finding Root Causes

**Duration:** 11 minutes | **Word Count:** ~1,390 words

### Script

[VISUAL: Title slide - "Analyze Phase—Finding Root Causes" with CI Master Academy logo]

**[00:00 - 00:45] HOOK & INTRODUCTION**

A hospital emergency department was struggling with long patient wait times—averaging 4 hours from arrival to treatment.

They brainstormed solutions: hire more nurses, expand waiting area, implement triage app. They piloted several solutions. Wait times barely budged.

Finally, they decided to actually analyze the root causes before jumping to more solutions.

[VISUAL: Analysis revealing insights]

What they discovered: 65% of wait time was after triage, waiting for lab results. The bottleneck wasn't staffing—it was the lab workflow and communication between lab and ED.

They redesigned lab prioritization and communication. Wait times dropped to 90 minutes.

**The lesson:** Don't guess at solutions. Analyze to find root causes. Then solutions become obvious.

Today, we're exploring the Analyze phase of DMAIC—where we use data and analysis tools to identify what's actually causing the problem.

**[00:45 - 02:00] THE PURPOSE OF THE ANALYZE PHASE**

The Analyze phase has a clear mission:

[VISUAL: Analyze phase in DMAIC cycle]

**ANALYZE PHASE PURPOSE:**
- Identify potential root causes
- Use data to verify actual root causes
- Quantify cause-and-effect relationships
- Prioritize which causes to address

**Why Analyze matters:**

Most people jump from problem to solution without this step:
- Problem: "Defects are high!"
- Solution: "We need better training!" (guess)

But what if training isn't the root cause? You've wasted time and money.

Analyze phase prevents this by:
1. Generating many potential causes (brainstorming)
2. Testing which causes are actual (data analysis)
3. Focusing improvement efforts on verified root causes

[VISUAL: Symptoms vs Root Causes]

**Symptoms vs Root Causes:**

**Symptom:** The visible problem (headache)
**Root Cause:** The underlying issue (dehydration)

**Treating symptoms:** Temporary relief (pain reliever)
**Treating root causes:** Permanent fix (drink water!)

In processes, treating symptoms means problems return. Treating root causes means problems don't come back.

**[02:00 - 04:30] BRAINSTORMING POTENTIAL CAUSES**

Analysis starts with generating many potential causes—don't filter yet!

**BRAINSTORMING TOOLS:**

**1. FISHBONE DIAGRAM (ISHIKAWA DIAGRAM)**

Organizes potential causes into categories.

[VISUAL: Fishbone diagram template]

**Traditional categories (6 M's):**
- **Methods:** Procedures, processes, work instructions
- **Machines:** Equipment, technology, tools
- **Materials:** Raw materials, supplies, inputs
- **Measurements:** How we measure, data accuracy
- **Mother Nature (Environment):** Temperature, humidity, lighting, noise
- **Manpower (People):** Training, skills, experience, fatigue

**How to create:**

1. Draw problem as "fish head" (e.g., "High Defect Rate")
2. Draw main bones (6 categories)
3. Team brainstorms causes for each category
4. Ask "Why?" repeatedly to get to deeper causes

[EXAMPLE - Manufacturing Defects]

**Methods:**
- Unclear work instructions
- Process steps not standardized
- No quality checks during process

**Machines:**
- Equipment not calibrated
- Tools worn out
- Machine settings drift over time

**Materials:**
- Raw material variability
- Supplier quality issues
- Material stored improperly

**(Continue for all 6 categories...)**

**Result:** 30-50 potential causes identified!

**2. BRAINSTORMING RULES:**

- **Quantity over quality** (generate many ideas)
- **No criticism** (all ideas welcome initially)
- **Build on others' ideas**
- **Wild ideas encouraged** (can be refined later)
- **Involve people who do the work** (they see things managers miss!)

**3. AFFINITY DIAGRAM**

For complex problems with many causes, group similar causes together.

1. Write each cause on sticky note
2. Group related causes
3. Name each group
4. Identify themes and relationships

**[04:30 - 07:00] DATA ANALYSIS TO VERIFY CAUSES**

Now you have many potential causes. Which are actual causes? Use data to find out!

**ANALYSIS TECHNIQUE 1: STRATIFICATION**

Break data down by suspected cause to see if pattern emerges.

[EXAMPLE - Defect Analysis]

**Stratify defects by:**
- **Shift:** Shift C has 3x defect rate of Shifts A/B → Shift is a cause!
- **Operator:** All operators similar → Operator skill not a cause
- **Machine:** Machine 3 has 5x defects → Machine 3 is a cause!
- **Material lot:** Lot #4567 has high defects → Material variability is a cause!

**Result:** Verified 3 causes (Shift C, Machine 3, Material Lot variability). Now focus improvement efforts on these!

**ANALYSIS TECHNIQUE 2: PARETO ANALYSIS**

Prioritize causes by impact (remember 80/20 rule from Module 3.1?).

[VISUAL: Pareto chart]

**Example - Defect Types:**
- Scratches: 45% of defects
- Misalignment: 28%
- Wrong color: 15%
- Other: 12%

**Conclusion:** Fix scratches and misalignment → solve 73% of problem!

Pareto focuses effort on vital few causes, not trivial many.

**ANALYSIS TECHNIQUE 3: SCATTER PLOTS (CORRELATION)**

Test if two variables are related.

[VISUAL: Scatter plot examples]

**Example - Temperature vs Defects:**

Plot shows: As temperature increases, defects increase (positive correlation).

**Conclusion:** Temperature is a cause! Control temperature to control defects.

**Caution:** Correlation doesn't prove causation, but it's strong evidence!

**ANALYSIS TECHNIQUE 4: HYPOTHESIS TESTING**

Use statistics to verify if differences are real or due to random chance.

[EXAMPLE]

**Hypothesis:** Shift C has higher defect rate than Shifts A/B

**Data:**
- Shift A/B average: 3% defects
- Shift C average: 9% defects

**Question:** Is this real difference, or could it be random variation?

**Statistical test:** Use 2-sample t-test (software does calculation)

**Result:** p-value = 0.001 (< 0.05 significance level)

**Conclusion:** Difference is statistically significant—not random! Shift C really does have higher defects.

**Common statistical tests:**
- **t-test:** Compare means of two groups
- **ANOVA:** Compare means of 3+ groups
- **Chi-square:** Compare proportions/frequencies
- **Regression:** Model relationship between variables

**Don't worry—statistical software does the math! You just need to:**
1. Know which test to use (ask your Black Belt mentor!)
2. Interpret the results (p-value < 0.05 = significant difference)

**[07:00 - 09:00] ROOT CAUSE VERIFICATION: 5 WHYS**

You've identified potential causes with data. Now dig deeper to find root causes.

**THE 5 WHYS TECHNIQUE:**

Ask "Why?" repeatedly until you reach root cause.

[VISUAL: 5 Whys chain]

**Example - Machine Breakdown:**

**Problem:** Machine stopped (symptom)

**Why?** Circuit breaker tripped.

**Why?** Electrical overload.

**Why?** Bearings seized, motor drew excessive current.

**Why?** Bearings not lubricated.

**Why?** Lubrication schedule not followed.

**Why? (Root cause)** No formal preventive maintenance system exists.

**Solution:** Implement preventive maintenance system (not just "replace bearings"!).

**KEY INSIGHTS:**

- Stop at 5 Whys (or when you reach actionable root cause)
- Root cause is usually a system/process issue, not a person
- You know you've reached root cause when fixing it prevents recurrence

**CAUTION WITH 5 WHYS:**

5 Whys is powerful but can be misleading if:
- You guess instead of verify with data
- You stop too early (surface cause, not root)
- You go down wrong path

**Better:** Use 5 Whys AFTER data analysis narrows causes!

**[09:00 - 10:30] ROOT CAUSE VERIFICATION: FAILURE MODE ANALYSIS**

For complex problems, consider **Failure Mode and Effects Analysis (FMEA)**.

[VISUAL: FMEA matrix]

**FMEA systematically evaluates:**
- **What can go wrong?** (Failure modes)
- **What causes it?** (Potential causes)
- **What's the effect?** (Impact on customer/process)
- **How likely?** (Probability)
- **How severe?** (Impact if it happens)
- **How detectable?** (Can we catch it before customer?)

**Risk Priority Number (RPN):**

RPN = Severity × Occurrence × Detection

**Higher RPN = Higher priority to address**

[EXAMPLE]

**Failure Mode:** Incorrect dosage on medication label

- **Severity:** 10 (could harm patient)
- **Occurrence:** 3 (happens occasionally)
- **Detection:** 4 (sometimes caught, sometimes not)
- **RPN:** 10 × 3 × 4 = 120 (HIGH PRIORITY!)

FMEA helps prioritize which root causes to tackle first based on risk, not just frequency.

**[10:30 - 10:45] ANALYZE PHASE DELIVERABLES**

At end of Analyze phase, you should have:

[VISUAL: Deliverables checklist]

✓ **Comprehensive list of potential causes** (fishbone, brainstorming)
✓ **Data analysis verifying actual causes** (stratification, Pareto, correlation)
✓ **Statistical verification** (hypothesis tests showing causes are significant)
✓ **Root cause identification** (5 Whys, FMEA)
✓ **Prioritized list of causes to address** (focus on vital few)
✓ **Documented analysis** (so stakeholders understand your conclusions)

**PRESENT ANALYSIS TO STAKEHOLDERS:**

[VISUAL: Analysis presentation]

Tell the data story:
1. We brainstormed 40 potential causes
2. Data analysis narrowed to 5 significant causes
3. Pareto shows 3 causes drive 80% of problem
4. Root cause analysis shows these 3 stem from 2 system issues
5. Recommendation: Address these 2 root causes in Improve phase

**Data + analysis = credibility!**

**[10:45 - 11:00] WRAP-UP & ASSIGNMENT**

Let's recap the Analyze phase:

[VISUAL: Analyze phase summary]

✓ **Purpose:** Find and verify root causes with data  
✓ **Brainstorm:** Generate many potential causes (fishbone, affinity)  
✓ **Analyze:** Use data to verify causes (stratification, Pareto, scatter plots)  
✓ **Verify:** Statistical tests prove causes are significant  
✓ **Root Cause:** 5 Whys/FMEA identify underlying system issues  
✓ **Prioritize:** Focus on vital few causes (80/20 rule)  
✓ **Output:** Verified root causes, ready for Improve phase

**Your assignment:**

**For your project:**

1. Create fishbone diagram (brainstorm potential causes)
2. Stratify your baseline data by suspected causes (shift, operator, machine, etc.)
3. Create Pareto chart of causes
4. For top cause(s), use 5 Whys to find root cause
5. Document your analysis: What are the 1-3 verified root causes you'll address?

**Now you know what to fix—not just guessing!**

In Module 4.4, we'll cover Improve and Control phases—implementing solutions and sustaining improvements.

[VISUAL: End screen - "Module 4.3 Complete" with transition to Module 4.4]

---

## Module 4.4: Improve & Control Phases

**Duration:** 11 minutes | **Word Count:** ~1,380 words

### Script

[VISUAL: Title slide - "Improve & Control Phases" with CI Master Academy logo]

**[00:00 - 00:45] HOOK & INTRODUCTION**

Two teams completed Six Sigma projects to reduce defects.

**Team A:** Identified root causes, implemented solutions, reduced defects 70%, celebrated success. One year later? Defects back to original levels.

**Team B:** Identified root causes, implemented solutions, reduced defects 70%, created control plans, trained staff, monitored performance. One year later? Defects still at improved level—and continuing to improve!

[VISUAL: Two trend charts showing Team A backsliding vs Team B sustaining]

What's the difference? Team B completed the DMAIC cycle—not just Improve, but also Control.

Today, we're exploring the final two phases: Improve (making changes) and Control (sustaining results).

These phases transform analysis into lasting results!

**[00:45 - 02:30] IMPROVE PHASE: GENERATING SOLUTIONS**

Now you know the root causes. Time to develop solutions!

[VISUAL: Improve phase in DMAIC cycle]

**IMPROVE PHASE PURPOSE:**
- Generate potential solutions
- Select best solutions
- Pilot solutions (test before full implementation)
- Implement improvements

**GENERATING SOLUTIONS:**

**1. BRAINSTORM SOLUTIONS** (like you brainstormed causes!)

For each root cause, ask: "How could we eliminate or reduce this cause?"

[EXAMPLE - Root Cause: "Operators not following standard procedure"]

**Potential solutions:**
- Retrain operators
- Simplify procedure
- Post visual work instructions at workstation
- Implement mistake-proofing (poka-yoke)
- Add checklist for critical steps
- Redesign process to eliminate confusing step
- Automate error-prone step

**Generate many options before evaluating!**

**2. THINK CREATIVELY**

[VISUAL: SCAMPER technique]

**SCAMPER brainstorming:**
- **S**ubstitute: Replace something
- **C**ombine: Merge steps or resources
- **A**dapt: Use something from another context
- **M**odify: Change size, shape, sequence
- **P**ut to other use: Repurpose something
- **E**liminate: Remove unnecessary steps
- **R**everse: Do things in opposite order

**Example:**
- **Eliminate:** Can we remove the step that causes errors?
- **Reverse:** What if we checked quality before processing instead of after?

**3. INVOLVE THE PEOPLE WHO DO THE WORK**

Frontline employees often have the best solution ideas—they live with the problem daily!

[ANIMATION: Lightbulb moment]

**[02:30 - 04:30] SELECTING SOLUTIONS**

You have many potential solutions. Which to implement?

**SELECTION CRITERIA:**

[VISUAL: Solution selection matrix]

**Evaluate each solution on:**
- **Impact:** How much will this improve the problem?
- **Cost:** Money, time, resources required
- **Feasibility:** Can we actually do this?
- **Time to implement:** Quick win or long project?
- **Risk:** What could go wrong?

**IMPACT VS EFFORT MATRIX:**

Plot solutions on 2×2 grid:

[VISUAL: 2×2 matrix]

**High Impact + Low Effort = Quick Wins (do first!)**
**High Impact + High Effort = Major projects (plan carefully)**
**Low Impact + Low Effort = Maybe do (if easy)**
**Low Impact + High Effort = Avoid (not worth it)**

**Prioritize:** Implement quick wins first (build momentum!), then tackle major projects.

[EXAMPLE]

**Root Cause:** Machine temperature varies, causing defects

**Solution options:**

| Solution | Impact | Effort | Priority |
|----------|--------|--------|----------|
| Add temperature monitoring alarm | High | Low | Quick Win! |
| Replace old heating element | High | Medium | Do 2nd |
| Install advanced climate control | High | High | Consider later |
| Train operators to check temp manually | Low | Low | Low priority |

**Decision:** Start with monitoring alarm (quick win), then replace heating element.

**PILOT TESTING (CRITICAL!):**

Never implement solutions organization-wide immediately—test first!

[VISUAL: Pilot process flowchart]

**Pilot = Small-scale test of solution**

**Why pilot?**
- Verify solution actually works
- Identify unintended consequences
- Refine solution based on learning
- Build evidence for wider rollout
- Reduce risk

**Pilot approach:**
1. Select limited scope (one line, one shift, one product)
2. Implement solution
3. Collect data (before/after comparison)
4. Evaluate results
5. Refine solution if needed
6. If successful, roll out broadly
7. If unsuccessful, try different solution!

[EXAMPLE]

**Solution:** New work instruction format

**Pilot:**
- Test with one team (10 people) for 2 weeks
- Collect data: error rate, time per task, employee feedback
- Results: Errors down 60%, time unchanged, feedback positive
- Refinement: Add visual aids based on suggestions
- Decision: Roll out to all teams!

**[04:30 - 06:30] IMPLEMENTING SOLUTIONS**

Pilot succeeded—now implement broadly!

**IMPLEMENTATION PLAN:**

[VISUAL: Implementation plan template]

**Plan should specify:**

**1. WHAT** (Exactly what changes)
- New procedure
- Equipment modifications
- Training requirements
- Updated forms/systems

**2. WHO** (Roles and responsibilities)
- Project leader
- Implementation team
- People affected
- Support resources

**3. WHEN** (Timeline)
- Start date
- Milestones
- Completion date
- Phased rollout schedule?

**4. WHERE** (Scope)
- Which locations, lines, departments
- Phased approach or all at once?

**5. HOW** (Implementation steps)
- Detailed action plan
- Dependencies
- Communication plan
- Training plan

**6. HOW MUCH** (Resources needed)
- Budget
- People hours
- Materials/equipment
- Support services

**CHANGE MANAGEMENT:**

Remember: People resist change! Address this proactively.

[VISUAL: Change management strategies]

**Strategies:**
- **Communicate WHY:** People support what they understand
- **Involve early:** People own what they help create
- **Train thoroughly:** People resist what they can't do
- **Support transition:** Provide help during adjustment
- **Celebrate wins:** Build momentum with early successes

**COMMON IMPLEMENTATION MISTAKES:**

❌ Implementing without pilot
❌ Poor communication (surprise changes)
❌ Insufficient training
❌ Removing old process before new one proven
❌ Declaring victory too early

**[06:30 - 09:00] CONTROL PHASE: SUSTAINING IMPROVEMENTS**

Congratulations—your solution works! Defects reduced 70%!

But... how do you ensure it stays that way?

[VISUAL: Control phase in DMAIC cycle]

**CONTROL PHASE PURPOSE:**
- Document new process
- Train all stakeholders
- Monitor performance
- Respond quickly if performance slips
- Hand off to process owner

**CONTROL PLAN:**

The primary Control phase deliverable is a **Control Plan**—a document specifying how to maintain improvements.

[VISUAL: Control plan template]

**Control Plan elements:**

**1. WHAT TO MONITOR**

Key metrics (Y's and critical X's):
- Output metric (defect rate, cycle time)
- Critical input variables (temperature, material spec)

**2. HOW TO MEASURE**

- Measurement method
- Operational definition
- Measurement frequency (continuous, hourly, daily)

**3. CONTROL LIMITS**

- Acceptable range (from process capability)
- When to investigate (approaching limits)
- When to stop process (outside limits)

**4. WHO'S RESPONSIBLE**

- Who measures?
- Who reviews data?
- Who takes action if out of control?

**5. WHAT ACTIONS TO TAKE**

If out of control:
- Immediate corrective action
- Root cause investigation
- Escalation procedure

[EXAMPLE - Temperature Control Plan]

| What | How | Frequency | Limits | Responsible | Action If Out |
|------|-----|-----------|--------|-------------|---------------|
| Machine temp | Digital sensor | Continuous | 220-230°F | Automated system | Alarm triggers, operator adjusts |
| Visual check | Operator observation | Hourly | No smoke/odor | Operator | Stop machine, call maintenance |

**STATISTICAL PROCESS CONTROL (SPC) CHARTS:**

The best tool for monitoring is the **Control Chart**.

[VISUAL: Control chart example]

**Control chart shows:**
- Data points over time
- Center line (process average)
- Upper Control Limit (UCL = mean + 3σ)
- Lower Control Limit (LCL = mean - 3σ)

**Interpretation:**

**Process in control:** Points randomly scattered between control limits
**Process out of control:** Points outside limits, or non-random patterns

**Rules indicating special cause (investigate!):**
- Point outside control limits
- 7+ consecutive points on one side of center line
- Trend (7+ consecutive points moving up or down)

[ANIMATION: Control chart showing out-of-control signals]

Control charts give early warning—catch problems before they become disasters!

**STANDARDIZED WORK:**

Document the new improved process as standard.

[VISUAL: Standard work document]

**Standard work includes:**
- Step-by-step procedure
- Critical quality points
- Cycle time for each step
- Visual aids (photos, diagrams)
- Safety requirements

**Purpose:** Everyone does work the same way (the best way discovered), reducing variation.

**TRAINING:**

Train everyone who touches the process:
- New procedure
- Why it changed
- How to follow it
- How to identify problems

Training = insurance policy for sustained improvement!

**[09:00 - 10:30] HANDOFF & PROJECT CLOSURE**

Final Control phase activities:

**PROJECT HANDOFF:**

[VISUAL: Handoff checklist]

Transfer ownership from project team to process owner:
- Deliver control plan
- Complete training
- Demonstrate monitoring system
- Confirm process owner accepts responsibility

**RESULTS VALIDATION:**

Prove improvement is real and sustained:
- Compare before/after data
- Calculate improvement (% reduction, cost savings)
- Update process capability (new Cp/Cpk)

[EXAMPLE]

**Before (baseline):**
- Defect rate: 8.5%
- Process capability: Cpk = 0.45

**After (sustained for 3 months):**
- Defect rate: 2.1%
- Process capability: Cpk = 1.52

**Improvement: 75% defect reduction, process now capable!**

**PROJECT DOCUMENTATION:**

[VISUAL: Final report cover]

Create final report:
- Problem and goal (from Define)
- Baseline performance (from Measure)
- Root causes identified (from Analyze)
- Solutions implemented (from Improve)
- Results achieved (from Control)
- Lessons learned
- Recommendations

**Benefits:**
- Proves ROI to leadership
- Shares lessons with organization
- Documents methodology for future projects
- Celebrates team success!

**CELEBRATE SUCCESS:**

[ANIMATION: Celebration graphics]

Recognize the team:
- Present results to leadership
- Celebrate with team
- Share success story widely
- Thank stakeholders

Recognition reinforces CI culture—people volunteer for future projects!

**[10:30 - 10:45] LESSONS LEARNED**

Reflect on what worked and what to improve:

**Questions:**
- What went well?
- What was challenging?
- What would you do differently?
- What did you learn about DMAIC?
- How can the organization support future projects better?

Document lessons—improve the improvement process!

**[10:45 - 11:00] WRAP-UP & ASSIGNMENT**

Let's recap Improve and Control phases:

[VISUAL: Improve & Control summary]

**IMPROVE:**
✓ Generate many solution ideas  
✓ Select solutions (impact vs effort)  
✓ Pilot test before full implementation  
✓ Implement with change management  
✓ Verify improvement

**CONTROL:**
✓ Create control plan  
✓ Implement SPC charts  
✓ Standardize new process  
✓ Train all stakeholders  
✓ Hand off to process owner  
✓ Document and celebrate!

**Your assignment:**

**Complete your DMAIC project:**

1. For your identified root cause(s), brainstorm 5-10 solutions
2. Select top 2-3 solutions using impact/effort matrix
3. Design a pilot test plan for one solution
4. Create a simple control plan: what will you monitor, how often, who's responsible?
5. If possible, implement pilot and collect before/after data!

**Bonus:** Present your project (even informally) to your manager or team using DMAIC structure.

Congratulations on completing Course 4: Six Sigma DMAIC Overview! You now have a structured, data-driven methodology for solving complex problems.

In Course 5, our final course, we'll explore essential problem-solving tools and techniques you can use within DMAIC or standalone.

[VISUAL: Course 4 completion badge, transition to Course 5]

---

**End of Course 4 Scripts**
**Total Word Count: ~5,590 words**
